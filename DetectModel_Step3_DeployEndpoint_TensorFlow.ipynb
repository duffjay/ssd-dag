{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Model\n",
    "## Step 3 - Deploy Endpoint as TensorFlow Model\n",
    "\n",
    "ref:  https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, re, os, sys\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed since we cloned tensorflow/models under code.\n",
    "cwd = os.getcwd()\n",
    "models = os.path.join(cwd, 'code/models/research/')\n",
    "slim = os.path.join(cwd, 'code/models/research/slim')\n",
    "sys.path.append(models)\n",
    "sys.path.append(slim)\n",
    "\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-03 20:59:28  127054873 model.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "S3_SAGEMAKER = \"s3://sagemaker-us-east-1-586454201570/\"\n",
    "SAGEMAKER_JOB = \"cfa-products-mobilenet-v1-SSD-2019-08-03-20-36-17-231/\"\n",
    "MODEL_OUTPUT = \"output/model.tar.gz\"\n",
    "IMAGE_SIZE = (300,300)\n",
    "\n",
    "FULL_MODEL_PATH = S3_SAGEMAKER + SAGEMAKER_JOB + MODEL_OUTPUT\n",
    "! aws s3 ls {FULL_MODEL_PATH}\n",
    "\n",
    "SAMPLE_IMAGE = \"/home/ec2-user/SageMaker/ssd-dag/data/new_jpeg_images/20190710_variety_1562781002.jpg\"\n",
    "\n",
    "# NAME - get this from the console\n",
    "ENDPOINT_NAME = \"ep-mobilenet-ssd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoint\n",
    "\n",
    "The SageMaker console appears to be the best place to create your SageMaker endpoint.    If you did NOT train your model on SageMaker (shame on you), you'll have to jump through some hoops.    I'm not covering that here because training on SageMaker seems to have many advantages but -- it seems you have to bundle up your model artifacts in a tarball that mimics the output of a SageMaker HOSTED training session.\n",
    "\n",
    "### Endpoint  Configuration\n",
    "Use the console to create an endpoint configuration.   This includes specifying the EC2 instance type.  Name your instance with a ep_* prefix to make it recognizable.   Include model and instance type.   For example:  \n",
    "\n",
    "ep-config-mobilenet-ssd-p2xlarge-gpu\n",
    "\n",
    "It appears you can install multiple models (?) - you can put more details in the model config line.\n",
    "\n",
    "### Create Endpoint from a Model\n",
    "\n",
    "Not sure about specifying a model in the configuration and the endpoint - need to read more documentation.   Name your endpoint:  \n",
    "\n",
    "ep-mobilenet-ssd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "predictor=sagemaker.tensorflow.model.TensorFlowPredictor(ENDPOINT_NAME, sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Endpoint with code\n",
    "\n",
    "Below is an example of creating an endpoint with code - referencing an S3 bucket where the model artifacts are stored.   This is way harder - use SageMaker to training    your model and avoid this hassle.    PLUS - it failed after 20 minutes when I tried it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Python 2 tensorflow images will be soon deprecated and may not be supported for newer upcoming versions of the tensorflow images.\n",
      "Please set the argument \"py_version='py3'\" to use the Python 3 tensorflow image.\n"
     ]
    }
   ],
   "source": [
    "# 20190804\n",
    "# DON'T EVEN THINK ABOUT THE OBVIOUS!\n",
    "#  py_version='py3'\n",
    "#  framework_version='1.13'\n",
    "\n",
    "sagemaker_model = TensorFlowModel(model_data = FULL_MODEL_PATH,\n",
    "                    role = role,\n",
    "                    framework_version='1.12',\n",
    "                    entry_point = 'code/train.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type='ml.p2.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "# load an image and resturn a numpy array\n",
    "\n",
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = IMAGE_SIZE\n",
    "  image = image.resize((im_width, im_height), Image.ANTIALIAS)\n",
    "  image = np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 300, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(SAMPLE_IMAGE)\n",
    "image_np = load_image_into_numpy_array(image)\n",
    "# Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "print (image_np_expanded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"image_tensor:0\", shape=(1, 300, 300, 3), dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "tensor_image = tf.constant(image_np_expanded, shape=(1,300,300,3), dtype=tf.uint8, name=\"image_tensor\")\n",
    "print (tensor_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_detections': <tf.Tensor 'num_detections:0' shape=<unknown> dtype=float32>, 'detection_boxes': <tf.Tensor 'detection_boxes:0' shape=<unknown> dtype=float32>, 'detection_scores': <tf.Tensor 'detection_scores:0' shape=<unknown> dtype=float32>, 'detection_classes': <tf.Tensor 'detection_classes:0' shape=<unknown> dtype=float32>}\n"
     ]
    }
   ],
   "source": [
    "tensor_num_detections = tf.compat.v1.placeholder(tf.float32, shape=None, name=\"num_detections\")\n",
    "tensor_detection_boxes = tf.compat.v1.placeholder(tf.float32, shape=None, name=\"detection_boxes\")\n",
    "tensor_detection_scores = tf.compat.v1.placeholder(tf.float32, shape=None, name=\"detection_scores\")\n",
    "tensor_detection_classes = tf.compat.v1.placeholder(tf.float32, shape=None, name=\"detection_classes\")\n",
    "\n",
    "tensor_dict = {'num_detections': tensor_num_detections, \n",
    "              'detection_boxes': tensor_detection_boxes, \n",
    "              'detection_scores': tensor_detection_scores, \n",
    "              'detection_classes': tensor_detection_classes}\n",
    "print (tensor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dict: {'num_detections': <tf.Tensor 'num_detections:0' shape=<unknown> dtype=float32>, \n",
    "              'detection_boxes': <tf.Tensor 'detection_boxes:0' shape=<unknown> dtype=float32>, \n",
    "              'detection_scores': <tf.Tensor 'detection_scores:0' shape=<unknown> dtype=float32>, \n",
    "              'detection_classes': <tf.Tensor 'detection_classes:0' shape=<unknown> dtype=float32>}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected uint8, got 'image_tensor' of type 'str' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3c3b400afd07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"image_tensor\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_np_expanded\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant_v1\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    177\u001b[0m   \"\"\"\n\u001b[1;32m    178\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=verify_shape,\n\u001b[0;32m--> 179\u001b[0;31m                         allow_broadcast=False)\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    281\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m    282\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[1;32m    284\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    464\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m       \u001b[0m_AssertCompatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m       \u001b[0;31m# check to them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n\u001b[0;32m--> 371\u001b[0;31m                       (dtype.name, repr(mismatch), type(mismatch).__name__))\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected uint8, got 'image_tensor' of type 'str' instead."
     ]
    }
   ],
   "source": [
    "image_tensor = tf.constant(\"image_tensor\", shape=(1, 300, 300, 3), dtype=tf.uint8)\n",
    "feed_dict={image_tensor: image_np_expanded}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "output_dict = predictor.predict(tensor_dict,feed_dict={image_tensor: image})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
