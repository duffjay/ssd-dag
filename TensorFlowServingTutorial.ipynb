{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Serve TF Model with TensorFlow Serving\n",
    "\n",
    "### Combining Tutorials\n",
    "\n",
    "https://www.tensorflow.org/tfx/tutorials/serving/rest_simple  \n",
    "https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/\n",
    "\n",
    "The Tensorflow tutorial builds a simple MNIST model and shows you how to set up a server (using Docker).  \n",
    "The second tutorial, SageMaker, shows you how to deploy a model you trained outside of SageMaker.  \n",
    "\n",
    "This hybrid will deploy the MNIST model to a SageMaker endpoint (not use a Docker to serve the model.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import subprocess\n",
    "import tarfile\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "print(tf.__version__)\n",
    "\n",
    "# AWS SageMaker\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd\n",
    "PROJECT_DIR = os.getcwd()\n",
    "BUCKET = 'cfa-eadatasciencesb-sagemaker'\n",
    "\n",
    "TRAINED_MODEL_PATH = os.path.join(PROJECT_DIR, \"trained_model\")\n",
    "EXPORT_MODEL_PATH = os.path.join(TRAINED_MODEL_PATH, \"export/Servo\")\n",
    "\n",
    "S3_MNIST_MODEL_PATH = 's3://{}/trained-models/mnist_fashion/20190814_100/output/'.format(BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST \n",
    "this bypasses a lot of model stuff - that's not really the point\n",
    "\n",
    "#### Note:\n",
    "- the images are normalized\n",
    "- note the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# scale the values to 0.0 to 1.0\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# reshape for feeding into the model\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print('\\ntrain_images.shape: {}, of {}'.format(train_images.shape, train_images.dtype))\n",
    "print('test_images.shape: {}, of {}'.format(test_images.shape, test_images.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "  keras.layers.Conv2D(input_shape=(28,28,1), filters=8, kernel_size=3, \n",
    "                      strides=2, activation='relu', name='Conv1'),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10, activation=tf.nn.softmax, name='Softmax')\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "testing = False\n",
    "epochs = 5\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=epochs)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('\\nTest accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have a trained model - now create a saved_graph.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the Keras session and save the model\n",
    "# The signature definition is defined by the input and output tensors,\n",
    "# and stored with the default serving key\n",
    "\n",
    "\n",
    "print (\"export model path:\", EXPORT_MODEL_PATH)\n",
    "version = 100       # can be any integer, just changing the default here\n",
    "export_path = os.path.join(EXPORT_MODEL_PATH, str(version))\n",
    "print('export model path = {}\\n'.format(export_path))\n",
    "if os.path.isdir(export_path):\n",
    "  print('\\nAlready saved a model, cleaning up\\n')\n",
    "  !rm -r {export_path}\n",
    "\n",
    "tf.saved_model.simple_save(\n",
    "    keras.backend.get_session(),\n",
    "    export_path,\n",
    "    inputs={'input_image': model.input},\n",
    "    outputs={t.name:t for t in model.outputs})\n",
    "\n",
    "print('\\nSaved model:')\n",
    "!ls -l {export_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the Signatures of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir {export_path} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving Your Model\n",
    "\n",
    "https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/\n",
    "\n",
    "This blog shows you that you can easily push this to S3 in the expected format (as if you trained it on SageMaker - which is really just straight TensorFlow) - then it will deploy easily to SageMaker.    Other tutorials suggest you need to make a Docker image.   This appears easier.\n",
    "\n",
    "#### This demonstrates you have the right artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple saved_grap.py is where it belongs\n",
    "\n",
    "os.chdir(TRAINED_MODEL_PATH)\n",
    "! ls\n",
    "! ls export/Servo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n",
    "    archive.add('export', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "print (sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model to S3\n",
    "SageMaker tutorial suggests using SageMaker Session as a unique folder name - but that provides no insight into the model.  So, I'm using a different bucket/folder scheme:  \n",
    "\n",
    "Sorry for the mixed - and _  -- bad habits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is slightly different than the tutorial\n",
    "#   tutorial says /model but SageMaker when it saves a model put the model output in /output\n",
    "#   so, I went with /output\n",
    "\n",
    "# you MANUALLY need to make sure this bucket/folder path is there\n",
    "# verify S3 path is good\n",
    "! aws s3 ls {S3_MNIST_MODEL_PATH}\n",
    "\n",
    "# verify your tarball is ready to go\n",
    "! ls model.tar.gz -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp model.tar.gz {S3_MNIST_MODEL_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go back to the project directory\n",
    "os.chdir(PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model to SageMaker Hosted Endpoint\n",
    "you are doing this with a weird combination of local and S3 artifacts:  \n",
    "- model is on S3 as a tarball (model.tar.gz)\n",
    "- train.py - which is not used, is a local asset\n",
    "- framework is TF 1.12, but you are on TF 1.14\n",
    "- And, there is a hidden issue of python version - that you probably won't see\n",
    "\n",
    "This is all telling you - that this will probably change with a near future release!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_model = TensorFlowModel(model_data = S3_MNIST_MODEL_PATH + 'model.tar.gz',\n",
    "                                  role = role,\n",
    "                                  framework_version = '1.12',\n",
    "                                  entry_point = 'code/train.py',\n",
    "                                  name='model-mnist-v100-20190814')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes 10 min\n",
    "# - this creates the model on SageMaker\n",
    "#   i.e. looks like you created it when you instantiated sagemaker_model - but you won't see it on console\n",
    "#        NOW you'll see it on the console\n",
    "# - this always fails\n",
    "#   UnexpectedStatusException: Error hosting endpoint ep-mnist-v100: Failed. Reason:  The primary container for production variant \n",
    "#      AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint\n",
    "\n",
    "# hack - once you've done this once, you'll have the artifacts you need\n",
    "\n",
    "# predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge', endpoint_name='ep-mnist-v100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After the .deploy fails\n",
    "1. go to console\n",
    "2. create an endpoint configuration - if none exists\n",
    "3. your model will exist\n",
    "4. create the endpoint (from console)\n",
    "   - reference the endpoint config\n",
    "   - and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
