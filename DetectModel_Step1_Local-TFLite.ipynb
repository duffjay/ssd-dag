{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect (with the) Model\n",
    "## Step 1 - run the TFLite model locally\n",
    "\n",
    "In this notebook, we are replicating  some of the original MobileNet TensorFlow Lite functionality.  That means taking the TensorFlow model (checkpoint) and converting it to a TensorFlow Lite model.   This is not quite what the SageMaker tutorial does.   The tutorial sticks with TensorFlow.   Confirm that the Lite model works first - because that is a known path then:\n",
    "- Detect Step 2 - run endpoint as a TensorFlow model\n",
    "- Detect Step 3 - run endpoint as a TensorFlow Lite Model -- the advantage of a Lite model is less latency, faster, less resources -- with slightly less accuracy.    Lite models are the path towards IoT deployments and TPU compatible models\n",
    "\n",
    "We are testing the model before we deploy it as an endpoint.   This is optional but definitely makes sense as we learn this process\n",
    "\n",
    "Your Python environment will require:\n",
    "- tensorflow\n",
    "- opencv\n",
    "- if you are displaying (on a local machine - not SageMaker) - you need gtk2*x86_64\n",
    "\n",
    "####  Installing OpenCV 4.1.2\n",
    "`conda activate <your environment>`  \n",
    "`pip install opencv-python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Environment Variables\n",
    "\n",
    "We are using objects and scripts in the project as much as possible.   They require environment variables to pass along where stuff is stored\n",
    "\n",
    "### Note \n",
    "This notebook assumes output_tflite_graph.tflite is already on the SageMaker server (it will be if you ran the Train Step 3 notebook.)  If not, you can get it from a SageMaker Training Job /output/tflite_model on S3\n",
    "\n",
    "### Execute the Common Globals + 1 other cell that corresponds to the model you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMON\n",
    "PROJECT_DIR = os.getcwd()\n",
    "IMAGE_DIR = os.path.join(PROJECT_DIR, \"data/new_jpeg_images\")\n",
    "ANNOTATION_DIR = os.path.join(PROJECT_DIR, \"data/unverified_annotations\")\n",
    "\n",
    "# if you are running this on a remote server witout GTK, use 'None'\n",
    "# if local with GTK, use 'gtk'\n",
    "# DISPLAY = \"None\"\n",
    "DISPLAY = 'gtk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute 1 of the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNet - V1 - SSD - cfa_prod (Chick-fil-A products)\n",
    "MODEL_NAME = 'tf_lite'   # not using the edge_tpu \n",
    "MODEL_PATH = os.path.join(PROJECT_DIR, \"tflite_model/output_tflite_graph.tflite\")\n",
    "LABEL_MAP = os.path.join(PROJECT_DIR, \"code/cfa_prod_label_map.pbtxt\")\n",
    "\n",
    "# SOURCE images \n",
    "S3_TEST_IMAGES = \"s3://cfa-eadatasciencesb-sagemaker/datasets/cfa_products/test_images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNet - V1 - SSD - coco (original)\n",
    "# - this is the original model, it was downloaded to code/ckpt in TrainModel_Step1_Local.ipynb\n",
    "# - convert using UnderstandingTendorRT_ConvertGraph - so now the name is the same\n",
    "MODEL_NAME = 'tf_lite'   # not using the edge_tpu \n",
    "MODEL_PATH = os.path.join(PROJECT_DIR, \"tflite_model/output_tflite_graph.tflite\")\n",
    "LABEL_MAP = os.path.join(PROJECT_DIR, \"code/ckpt/mscoco_label_map.pbtxt\")\n",
    "\n",
    "# SOURCE images \n",
    "S3_TEST_IMAGES = \"s3://cfa-eadatasciencesb-sagemaker/datasets/coco/test_images/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Copy the test images locally\n",
    "\n",
    "### aws2\n",
    "- if local, remember to update our credentials\n",
    "- if using AWS CLI Version2,   \n",
    "https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux-mac.html#cliv2-linux-mac-install  \n",
    "- remember, it's:  \n",
    "`aws2 s3 cp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the existing images\n",
    "# - obviously, you can skip this cell if you don't want to delete\n",
    "! rm -rf {IMAGE_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://cfa-eadatasciencesb-sagemaker/datasets/coco/test_images/Sailing_02.jpg to data/new_jpeg_images/Sailing_02.jpg\n",
      "download: s3://cfa-eadatasciencesb-sagemaker/datasets/coco/test_images/111-1152_IMG.JPG to data/new_jpeg_images/111-1152_IMG.JPG\n",
      "download: s3://cfa-eadatasciencesb-sagemaker/datasets/coco/test_images/111-1122_IMG.JPG to data/new_jpeg_images/111-1122_IMG.JPG\n",
      "download: s3://cfa-eadatasciencesb-sagemaker/datasets/coco/test_images/WV 200727.jpg to data/new_jpeg_images/WV 200727.jpg\n",
      "download: s3://cfa-eadatasciencesb-sagemaker/datasets/coco/test_images/WV 200713.jpg to data/new_jpeg_images/WV 200713.jpg\n"
     ]
    }
   ],
   "source": [
    "# version 2\n",
    "! aws2 s3 cp {S3_TEST_IMAGES} {IMAGE_DIR} --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect\n",
    "The detect.py script will read a directory of images, infer and generate XML (unverified) Annotations (VOC PASCAL schema).    Unverified means if you run them through a labeling program like labelImg, they do not have the verified attribute.    (Downstream, they will be ignored if you try to fold them into training data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/tensorRT/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jay/anaconda3/envs/tensorRT/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jay/anaconda3/envs/tensorRT/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jay/anaconda3/envs/tensorRT/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jay/anaconda3/envs/tensorRT/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jay/anaconda3/envs/tensorRT/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jay/anaconda3/envs/tensorRT/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jay/anaconda3/envs/tensorRT/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jay/anaconda3/envs/tensorRT/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jay/anaconda3/envs/tensorRT/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jay/anaconda3/envs/tensorRT/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jay/anaconda3/envs/tensorRT/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      " 03:59:17  - detector - INFO - Arguments. image_dir: /home/jay/projects/ssd-dag/data/new_jpeg_images\n",
      " 03:59:17  - detector - INFO - Arguments. model_name: tf_lite\n",
      " 03:59:17  - detector - INFO - Arguments. model_path: /home/jay/projects/ssd-dag/tflite_model/output_tflite_graph.tflite\n",
      " 03:59:17  - detector - INFO - Arguments. label_map_path: /home/jay/projects/ssd-dag/code/ckpt/mscoco_label_map.pbtxt\n",
      " 03:59:17  - detector - INFO - Arguments. display: gtk\n",
      " 03:59:17  - detector - INFO - Arguments. annotation_dir: /home/jay/projects/ssd-dag/data/unverified_annotations\n",
      "WARNING:tensorflow:From /home/jay/projects/ssd-dag/code/utils/label_map_util.py:119: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      " 03:59:17  - tensorflow - WARNING - From /home/jay/projects/ssd-dag/code/utils/label_map_util.py:119: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      " 03:59:17  - detector - INFO - label dict: {1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane', 6: 'bus', 7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light', 11: 'fire hydrant', 13: 'stop sign', 14: 'parking meter', 15: 'bench', 16: 'bird', 17: 'cat', 18: 'dog', 19: 'horse', 20: 'sheep', 21: 'cow', 22: 'elephant', 23: 'bear', 24: 'zebra', 25: 'giraffe', 27: 'backpack', 28: 'umbrella', 31: 'handbag', 32: 'tie', 33: 'suitcase', 34: 'frisbee', 35: 'skis', 36: 'snowboard', 37: 'sports ball', 38: 'kite', 39: 'baseball bat', 40: 'baseball glove', 41: 'skateboard', 42: 'surfboard', 43: 'tennis racket', 44: 'bottle', 46: 'wine glass', 47: 'cup', 48: 'fork', 49: 'knife', 50: 'spoon', 51: 'bowl', 52: 'banana', 53: 'apple', 54: 'sandwich', 55: 'orange', 56: 'broccoli', 57: 'carrot', 58: 'hot dog', 59: 'pizza', 60: 'donut', 61: 'cake', 62: 'chair', 63: 'couch', 64: 'potted plant', 65: 'bed', 67: 'dining table', 70: 'toilet', 72: 'tv', 73: 'laptop', 74: 'mouse', 75: 'remote', 76: 'keyboard', 77: 'cell phone', 78: 'microwave', 79: 'oven', 80: 'toaster', 81: 'sink', 82: 'refrigerator', 84: 'book', 85: 'clock', 86: 'vase', 87: 'scissors', 88: 'teddy bear', 89: 'hair drier', 90: 'toothbrush'}\n",
      " 03:59:17  - detector - INFO - TF Lite Model loading...\n",
      " 03:59:17  - detector - INFO - Loading Model: /home/jay/projects/ssd-dag/tflite_model/output_tflite_graph.tflite\n",
      "INFO: Initialized TensorFlow Lite runtime.\n",
      " 03:59:17  - detector - INFO - Loaded Model\n",
      " 03:59:17  - detector - INFO - Input Details: [{'name': 'normalized_input_image_tensor', 'index': 175, 'shape': array([  1, 300, 300,   3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.0078125, 128)}]\n",
      " 03:59:17  - detector - INFO - Output Details: [{'name': 'TFLite_Detection_PostProcess', 'index': 167, 'shape': array([ 1, 10,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'TFLite_Detection_PostProcess:1', 'index': 168, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'TFLite_Detection_PostProcess:2', 'index': 169, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'TFLite_Detection_PostProcess:3', 'index': 170, 'shape': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\n",
      " 03:59:17  - detector - INFO - Model Input Dimension: (1, 300, 300, 3)\n",
      " 03:59:17  - detector - INFO - image directory: 5\n",
      " 03:59:17  - detector - INFO - Image: /home/jay/projects/ssd-dag/data/new_jpeg_images/Sailing_02.jpg\n",
      " 03:59:17  - detector - INFO - time spent: 0.1406\n",
      " 03:59:17  - detector - INFO -       class: 1-person prob: 0.65625  bbox: (521,332), (721,571)\n",
      " 03:59:17  - detector - INFO -       class: 1-person prob: 0.62890625  bbox: (602,327), (741,482)\n",
      " 03:59:17  - detector - INFO -     obj detected: 2, obj ignored: 8\n",
      "detected objects: [(1, 'person', 0.65625, 521, 332, 721, 571), (1, 'person', 0.62890625, 602, 327, 741, 482)]\n",
      "/home/jay/projects/ssd-dag/data/new_jpeg_images Sailing_02.jpg\n",
      "[(1, 'person', 0.65625, 521, 332, 721, 571), (1, 'person', 0.62890625, 602, 327, 741, 482)]\n",
      "/home/jay/projects/ssd-dag/data/unverified_annotations\n",
      " 03:59:20  - detector - INFO - Image: /home/jay/projects/ssd-dag/data/new_jpeg_images/111-1152_IMG.JPG\n",
      " 03:59:20  - detector - INFO - time spent: 0.1338\n",
      " 03:59:20  - detector - INFO -       class: 1-person prob: 0.7734375  bbox: (877,388), (1071,886)\n",
      " 03:59:20  - detector - INFO -       class: 1-person prob: 0.71875  bbox: (1343,686), (1468,868)\n",
      " 03:59:20  - detector - INFO -     obj detected: 2, obj ignored: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected objects: [(1, 'person', 0.7734375, 877, 388, 1071, 886), (1, 'person', 0.71875, 1343, 686, 1468, 868)]\n",
      "/home/jay/projects/ssd-dag/data/new_jpeg_images 111-1152_IMG.JPG\n",
      "[(1, 'person', 0.7734375, 877, 388, 1071, 886), (1, 'person', 0.71875, 1343, 686, 1468, 868)]\n",
      "/home/jay/projects/ssd-dag/data/unverified_annotations\n",
      " 03:59:20  - detector - INFO - Image: /home/jay/projects/ssd-dag/data/new_jpeg_images/111-1122_IMG.JPG\n",
      " 03:59:21  - detector - INFO - time spent: 0.1342\n",
      " 03:59:21  - detector - INFO -       class: 4-motorcycle prob: 0.80078125  bbox: (198,548), (1376,1206)\n",
      " 03:59:21  - detector - INFO -       class: 1-person prob: 0.70703125  bbox: (382,218), (981,1180)\n",
      " 03:59:21  - detector - INFO -     obj detected: 2, obj ignored: 8\n",
      "detected objects: [(4, 'motorcycle', 0.80078125, 198, 548, 1376, 1206), (1, 'person', 0.70703125, 382, 218, 981, 1180)]\n",
      "/home/jay/projects/ssd-dag/data/new_jpeg_images 111-1122_IMG.JPG\n",
      "[(4, 'motorcycle', 0.80078125, 198, 548, 1376, 1206), (1, 'person', 0.70703125, 382, 218, 981, 1180)]\n",
      "/home/jay/projects/ssd-dag/data/unverified_annotations\n",
      " 03:59:24  - detector - INFO - Image: /home/jay/projects/ssd-dag/data/new_jpeg_images/WV 200713.jpg\n",
      " 03:59:24  - detector - INFO - time spent: 0.1324\n",
      " 03:59:24  - detector - INFO -       class: 1-person prob: 0.65625  bbox: (638,1056), (1156,1744)\n",
      " 03:59:24  - detector - INFO -       class: 4-motorcycle prob: 0.6015625  bbox: (546,1004), (1213,1902)\n",
      " 03:59:24  - detector - INFO -     obj detected: 2, obj ignored: 8\n",
      "detected objects: [(1, 'person', 0.65625, 638, 1056, 1156, 1744), (4, 'motorcycle', 0.6015625, 546, 1004, 1213, 1902)]\n",
      "/home/jay/projects/ssd-dag/data/new_jpeg_images WV 200713.jpg\n",
      "[(1, 'person', 0.65625, 638, 1056, 1156, 1744), (4, 'motorcycle', 0.6015625, 546, 1004, 1213, 1902)]\n",
      "/home/jay/projects/ssd-dag/data/unverified_annotations\n",
      " 03:59:25  - detector - INFO - Image: /home/jay/projects/ssd-dag/data/new_jpeg_images/WV 200727.jpg\n",
      " 03:59:25  - detector - INFO - time spent: 0.1324\n",
      " 03:59:26  - detector - INFO -       class: 1-person prob: 0.7734375  bbox: (1397,890), (1811,1567)\n",
      " 03:59:26  - detector - INFO -     obj detected: 1, obj ignored: 9\n",
      "detected objects: [(1, 'person', 0.7734375, 1397, 890, 1811, 1567)]\n",
      "/home/jay/projects/ssd-dag/data/new_jpeg_images WV 200727.jpg\n",
      "[(1, 'person', 0.7734375, 1397, 890, 1811, 1567)]\n",
      "/home/jay/projects/ssd-dag/data/unverified_annotations\n"
     ]
    }
   ],
   "source": [
    "! python code/detect.py --image_dir {IMAGE_DIR} --model_name {MODEL_NAME} --model_path {MODEL_PATH} --label_map_path {LABEL_MAP} --display {DISPLAY} --annotation_dir {ANNOTATION_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tarball the annotations\n",
    "os.chdir(\"data\")\n",
    "! tar -czvf  unverif_annotations.tar.gz unverified_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "If you want the annotations - you'll find the annotations tarball in data/ directory\n",
    "use the Notebook browser to download it\n",
    "\n",
    "The fastest, easiest way to review (and correct / verify) is to use labelImg program which will merge the image and annotation\n",
    "\n",
    "The main conclusion here is our model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorRT)",
   "language": "python",
   "name": "tensorrt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
