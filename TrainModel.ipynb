{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "## SageMaker Script Mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-TensorFlow 2.0\n",
    "# set up eager execution\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(0)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_TFRECORDS_PATH = \"s3://cfaanalyticsresearch-sagemaker/datasets/cfa_products/tfrecords/\"\n",
    "TFRECORDS_TARBALL = \"20190718_tfrecords.tar.gz\"\n",
    "\n",
    "S3_MODEL_PATH = \"s3://cfaanalyticsresearch-sagemaker/trained-models/tensorflow_mobilenet/\"\n",
    "MODEL_FOLDER = \"20190718_cfa_prod_mobilenet_v1_ssd300/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Get the data from s3.  you'll need to pass a directory into the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://cfaanalyticsresearch-sagemaker/datasets/cfa_products/tfrecords/20190718_tfrecords.tar.gz\n",
      "download: s3://cfaanalyticsresearch-sagemaker/datasets/cfa_products/tfrecords/20190718_tfrecords.tar.gz to data/20190718_tfrecords.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_tfrecords = os.path.join(S3_TFRECORDS_PATH, TFRECORDS_TARBALL)\n",
    "print (s3_tfrecords)\n",
    "! aws s3 cp $s3_tfrecords data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190718_tfrecords/test.tfrecord\n",
      "20190718_tfrecords/train.tfrecord\n",
      "20190718_tfrecords/val.tfrecord\n"
     ]
    }
   ],
   "source": [
    "! tar -xvf data/$TFRECORDS_TARBALL --strip=1 -C data/tfrecords\n",
    "! rm data/$TFRECORDS_TARBALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model\n",
    "this exercise will RETRAIN an existing model.  So, you need the starting point.\n",
    "\n",
    "Copy the model from S3.    The files will include a label map.   This will be put into the training directory (the directory that is passed into the training job so everything is together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://cfaanalyticsresearch-sagemaker/trained-models/tensorflow_mobilenet/20190718_cfa_prod_mobilenet_v1_ssd300/cfa_prod_label_map.pbtxt to model/cfa_prod_label_map.pbtxt\n",
      "download: s3://cfaanalyticsresearch-sagemaker/trained-models/tensorflow_mobilenet/20190718_cfa_prod_mobilenet_v1_ssd300/labels.txt to model/labels.txt\n",
      "download: s3://cfaanalyticsresearch-sagemaker/trained-models/tensorflow_mobilenet/20190718_cfa_prod_mobilenet_v1_ssd300/sagemaker_mobilenet_v1_ssd_retrain.config to model/sagemaker_mobilenet_v1_ssd_retrain.config\n",
      "download: s3://cfaanalyticsresearch-sagemaker/trained-models/tensorflow_mobilenet/20190718_cfa_prod_mobilenet_v1_ssd300/output_tflite_graph.tflite to model/output_tflite_graph.tflite\n",
      "download: s3://cfaanalyticsresearch-sagemaker/trained-models/tensorflow_mobilenet/20190718_cfa_prod_mobilenet_v1_ssd300/tflite_graph.pb to model/tflite_graph.pb\n",
      "download: s3://cfaanalyticsresearch-sagemaker/trained-models/tensorflow_mobilenet/20190718_cfa_prod_mobilenet_v1_ssd300/tflite_graph.pbtxt to model/tflite_graph.pbtxt\n"
     ]
    }
   ],
   "source": [
    "s3_model_folder = os.path.join(S3_MODEL_PATH, MODEL_FOLDER)\n",
    "! aws s3 cp $s3_model_folder model --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/ssd-dag/code\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local (Script) Mode Training\n",
    "\n",
    "see the AWS SageMaker tutorials notably:  \n",
    "https://github.com/aws-samples/amazon-sagemaker-script-mode/blob/master/tf-eager-script-mode/tf-eager-sm-scriptmode.ipynb\n",
    "\n",
    "The point here is, you can develop a training script locally, then know (have a high degree of confidence) it will run as a SageMaker training job.   (This is relatively new, the old way was more difficult and cumbersome.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.  Do you have a training script that will run locally - without Docker?\n",
    "\n",
    "considering what is coming up, you want all code needed to train in one directory.  That directory will be included in the Docker image.    \n",
    "\n",
    "This is going to get cumbersome because we took a bunch of stuff from the (real) tensorflow/models project.    To make sure we keep up to date, build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### github tensorflow/models\n",
    "manually git clone the FIRST TIME.   The official TensorFlow github repo has a related repo with a bunch of models, tutorials, utilities etc.   We are using them.  So clone them to this machine.   In a subsequent step, we'll get the files we need from this local copy.\n",
    "\n",
    "git clone https://github.com/tensorflow/models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the latest software\n",
    "TF_MODEL = \"/home/ec2-user/SageMaker/models\"                # this is the TensorFlow repo for momdels\n",
    "OUR_CODE = \"/home/ec2-user/SageMaker/ssd-dag/code\"          # this is OUR directory for code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up-to-date.\r\n"
     ]
    }
   ],
   "source": [
    "os.chdir(TF_MODEL)\n",
    "! git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TF / Model ---\n",
      "anchor_generators\t\t     matchers\n",
      "box_coders\t\t\t     meta_architectures\n",
      "builders\t\t\t     metrics\n",
      "CONTRIBUTING.md\t\t\t     model_hparams.py\n",
      "core\t\t\t\t     model_lib.py\n",
      "data\t\t\t\t     model_lib_test.py\n",
      "data_decoders\t\t\t     model_lib_v2.py\n",
      "dataset_tools\t\t\t     model_lib_v2_test.py\n",
      "dockerfiles\t\t\t     model_main.py\n",
      "eval_util.py\t\t\t     models\n",
      "eval_util_test.py\t\t     model_tpu_main.py\n",
      "exporter.py\t\t\t     object_detection_tutorial.ipynb\n",
      "exporter_test.py\t\t     predictors\n",
      "export_inference_graph.py\t     protos\n",
      "export_tflite_ssd_graph_lib.py\t     __pycache__\n",
      "export_tflite_ssd_graph_lib_test.py  README.md\n",
      "export_tflite_ssd_graph.py\t     samples\n",
      "g3doc\t\t\t\t     test_ckpt\n",
      "inference\t\t\t     test_data\n",
      "__init__.py\t\t\t     test_images\n",
      "inputs.py\t\t\t     tpu_exporters\n",
      "inputs_test.py\t\t\t     utils\n",
      "legacy\n",
      "--- our code ---\n",
      "annotation.py  missing_pb2.tar.gz   requirements.txt\t   utils\n",
      "detect.py      move_missing_pb2.sh  tflite_interpreter.py\n",
      "display.py     object_detection     train.py\n"
     ]
    }
   ],
   "source": [
    "# copy necessary programs/scripts to OUR train_model directory\n",
    "! echo \"--- TF / Model ---\"\n",
    "! ls $TF_MODEL/research/object_detection \n",
    "\n",
    "\n",
    "# copy from the tensorflow repo to our repo\n",
    "! cp $TF_MODEL/research/object_detection $OUR_CODE -r\n",
    "\n",
    "! echo \"--- our code ---\"\n",
    "! ls $OUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing *.pb2.py scripts\n",
    "This is evidently something related to different versions of protobufs.   I never fully figured this out.\n",
    "- these were in the original Coral Tutorial & model training\n",
    "- but NOT in the tensorflow/models repo\n",
    "\n",
    "I pulled them from the original Coral TPU tutorial and put them in a tarball.  This script will move them to the correct place and you can forget about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing_pb2/anchor_generator_pb2.py\r\n",
      "missing_pb2/argmax_matcher_pb2.py\r\n",
      "missing_pb2/bipartite_matcher_pb2.py\r\n",
      "missing_pb2/box_coder_pb2.py\r\n",
      "missing_pb2/box_predictor_pb2.py\r\n",
      "missing_pb2/calibration_pb2.py\r\n",
      "missing_pb2/eval_pb2.py\r\n",
      "missing_pb2/faster_rcnn_box_coder_pb2.py\r\n",
      "missing_pb2/faster_rcnn_pb2.py\r\n",
      "missing_pb2/graph_rewriter_pb2.py\r\n",
      "missing_pb2/grid_anchor_generator_pb2.py\r\n",
      "missing_pb2/hyperparams_pb2.py\r\n",
      "missing_pb2/image_resizer_pb2.py\r\n",
      "missing_pb2/input_reader_pb2.py\r\n",
      "missing_pb2/keypoint_box_coder_pb2.py\r\n",
      "missing_pb2/losses_pb2.py\r\n",
      "missing_pb2/matcher_pb2.py\r\n",
      "missing_pb2/mean_stddev_box_coder_pb2.py\r\n",
      "missing_pb2/model_pb2.py\r\n",
      "missing_pb2/multiscale_anchor_generator_pb2.py\r\n",
      "missing_pb2/optimizer_pb2.py\r\n",
      "missing_pb2/pipeline_pb2.py\r\n",
      "missing_pb2/post_processing_pb2.py\r\n",
      "missing_pb2/preprocessor_pb2.py\r\n",
      "missing_pb2/region_similarity_calculator_pb2.py\r\n",
      "missing_pb2/square_box_coder_pb2.py\r\n",
      "missing_pb2/ssd_anchor_generator_pb2.py\r\n",
      "missing_pb2/ssd_pb2.py\r\n",
      "missing_pb2/string_int_label_map_pb2.py\r\n",
      "missing_pb2/train_pb2.py\r\n"
     ]
    }
   ],
   "source": [
    "# so we are copying them \n",
    "os.chdir(CODE)\n",
    "!/bin/bash ./move_missing_pb2.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Packages not in the tensorflow_p36 conda environment\n",
    "so add them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python packages that are required\n",
    "! pip install pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Configuration\n",
    "See the config file for all parameters. the .config file is in the /model direcory But you DEFINITELY need to look at these!\n",
    "- num_classes = should be consistent with labels.txt & label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(CODE)   # this will be the training directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\n",
      "For more information, please see:\r\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n",
      "  * https://github.com/tensorflow/addons\r\n",
      "If you depend on functionality not listed there, please file an issue.\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"train.py\", line 26, in <module>\r\n",
      "    from object_detection import model_lib\r\n",
      "  File \"/home/ec2-user/SageMaker/ssd-dag/code/object_detection/model_lib.py\", line 28, in <module>\r\n",
      "    from object_detection import exporter as exporter_lib\r\n",
      "  File \"/home/ec2-user/SageMaker/ssd-dag/code/object_detection/exporter.py\", line 24, in <module>\r\n",
      "    from object_detection.builders import model_builder\r\n",
      "  File \"/home/ec2-user/SageMaker/ssd-dag/code/object_detection/builders/model_builder.py\", line 20, in <module>\r\n",
      "    from object_detection.builders import anchor_generator_builder\r\n",
      "  File \"/home/ec2-user/SageMaker/ssd-dag/code/object_detection/builders/anchor_generator_builder.py\", line 22, in <module>\r\n",
      "    from object_detection.protos import anchor_generator_pb2\r\n",
      "ValueError: source code string cannot contain null bytes\r\n"
     ]
    }
   ],
   "source": [
    "! python train.py \\\n",
    "  --pipeline_config_path=\"sagemaker_mobilenet_v1_ssd_retrain.config\" \\\n",
    "  --model_dir=\"${TRAIN_DIR}\" \\\n",
    "  --num_train_steps=\"${num_training_steps}\" \\\n",
    "  --num_eval_steps=\"${num_eval_steps}\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.  Now try training locally - but your training goes into a Docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/bin/bash ./sagemaker_docker_setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a local SageMaker estimator\n",
    "\n",
    "code/train_model - this entire directory goes to the Docker image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an Estimator is a SageMaker class\n",
    "# and, you're using the tensorflow flavor\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "model_dir = '/opt/ml/model'     # this is related to how it gets deployed in the Docker\n",
    "train_instance_type = 'local'   # local vs another server\n",
    "hyperparameters = {'epochs': 5, 'batch_size': 128, 'learning_rate': 0.01}\n",
    "local_estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir='train_model',\n",
    "                       model_dir=model_dir,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       base_job_name='tf-eager-scriptmode-bostonhousing',\n",
    "                       framework_version='1.13',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
