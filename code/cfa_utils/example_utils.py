import json
import os
import sys

from os import listdir
from os.path import isfile, join
from bs4 import BeautifulSoup

import numpy as np
import tensorflow as tf



from object_detection.utils.label_map_util import get_label_map_dict

from .gen_imagesets import gen_imageset_list

# taken from: https://github.com/tensorflow/models/blob/master/research/object_detection/utils/dataset_util.py

def int64_feature(value):
  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

def int64_list_feature(value):
  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))

def bytes_feature(value):
  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

def bytes_list_feature(value):
  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))

def float_list_feature(value):
  return tf.train.Feature(float_list=tf.train.FloatList(value=value))

# use this as the standard feature definition for MobileNet SSD Example
# this comes from the models/mobilenet code - it's not CFA specific
# but it's specific to the model - must be consistent through the pipeline

feature_obj_detect = {
        'image/encoded': tf.io.FixedLenFeature((), tf.string, default_value=''),
        'image/format': tf.io.FixedLenFeature((), tf.string, default_value='jpeg'),
        'image/filename': tf.io.FixedLenFeature((), tf.string, default_value=''),
        'image/key/sha256': tf.io.FixedLenFeature((), tf.string, default_value=''),
        'image/source_id': tf.io.FixedLenFeature((), tf.string, default_value=''),
        'image/height': tf.io.FixedLenFeature((), tf.int64, default_value=1),
        'image/width': tf.io.FixedLenFeature((), tf.int64, default_value=1),
        'image/class/text': tf.io.VarLenFeature(tf.string),
        'image/class/label': tf.io.VarLenFeature(tf.int64),
        'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),
        'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),
        'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),
        'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),
        'image/object/class/label': tf.io.VarLenFeature(tf.int64),
        'image/object/class/text': tf.io.VarLenFeature(tf.string),
        'image/object/area': tf.io.VarLenFeature(tf.float32),
        'image/object/is_crowd': tf.io.VarLenFeature(tf.int64),
        'image/object/difficult': tf.io.VarLenFeature(tf.int64),
        'image/object/group_of': tf.io.VarLenFeature(tf.int64),
        'image/object/weight': tf.io.VarLenFeature(tf.float32)
    }

# Default values for the annotations
# - seems you must supply a value for every attr in the  dict
#   or you'll get a MergeFrom error
IMG_FORMAT = b'jpg'
IMG_SHA256 = b''
IMG_CLASS_NAMES = [b'cfa_prod']
IMG_CLASS_IDS = [1]

OBJ_AREA = 1.0
OBJ_IS_CROWD = 0
OBJ_DIFFICULT = 0  # the DIFFICULT in XML is per image - not per object
OBJ_GROUP_OF = 1
OBJ_WEIGHT = 1


# return a list of imagesets in given directory
def get_imagesets(imageset_dir):
    file_list = []
    for f in listdir(imageset_dir):
        file_part = os.path.splitext(f)
        if (isfile(join(imageset_dir, f)) and (file_part[1] == ".txt") ):
            file_list.append(f)
    return file_list

def incr_class_count (class_dict, class_id):
    try:
        class_dict[class_id] = class_dict[class_id] + 1
    except:
        class_dict.update({ class_id : 1})

    return class_dict

def invert_dict(orig_dict):
    inverted_dict = dict([[v,k] for k,v in orig_dict.items()])
    return inverted_dict

# from VOC compliant images/annotations/sets
# convert to tfrecords
def voc_to_tfrecord_file(image_dir,
                    annotation_dir,
                    label_map_file,
                    tfrecord_dir,
                    training_split_tuple,
                    include_classes = "all",
                    exclude_truncated=False,
                    exclude_difficult=False):
    # this uses only TensorFlow libraries
    # - no P Ferrari classes

    label_map = get_label_map_dict(label_map_file)
    # label_map_dict = invert_dict(origin_label_map_dict)    # we need the id, not the name as the key

    train_list, val_list, test_list = gen_imageset_list(annotation_dir, training_split_tuple)

    print (label_map)

    # iterate through each image_id (file name w/o extension) in the image list
    # this list will give you the variables needed to iterate through train/val/test
    imageset_list = [(train_list, 'train'), (val_list, 'val'), (test_list, 'test')]
    j = 0
    for (image_list, imageset_name) in imageset_list:

        # you can create/open the tfrecord writer
        output_path = os.path.join(tfrecord_dir, imageset_name, imageset_name + ".tfrecord")
        tf_writer = tf.python_io.TFRecordWriter(output_path)
        print (" -- images", len(image_list), " writing to:", output_path)

        image_count = 0   # simple image cuonter
        class_dict = {}   # dict to keep class count

        # loop through each image in the image list

        for image_id in image_list:
            if image_id.startswith('.'):
                continue
            # get annotation information
            annotation_path = os.path.join(annotation_dir, image_id + '.xml')
            with open(annotation_path) as f:
                    soup = BeautifulSoup(f, 'xml')

                    folder = soup.folder.text
                    filename = soup.filename.text
                    # size = soup.size.text
                    sizeWidth = float(soup.size.width.text)     # you need everything as floats
                    sizeHeight = float(soup.size.height.text)
                    sizeDepth = float(soup.size.depth.text)

                    boxes = [] # We'll store all boxes for this image here
                    objects = soup.find_all('object') # Get a list of all objects in this image

                    # Parse the data for each object
                    for obj in objects:
                        class_name = obj.find('name').text
                        try:
                            class_id = label_map[class_name]
                            class_dict = incr_class_count(class_dict, class_id)
                        except:
                            print ("!!! label map error:", image_id, class_name, " skipped")
                            continue
                        # Check if this class is supposed to be included in the dataset
                        if (not include_classes == 'all') and (not class_id in include_classes): continue
                        pose = obj.pose.text
                        truncated = int(obj.truncated.text)
                        if exclude_truncated and (truncated == 1): continue
                        difficult = int(obj.difficult.text)
                        if exclude_difficult and (difficult == 1): continue
                        # print (image_id, image_count, "xmin:", obj.bndbox.xmin.text)
                        xmin = int(obj.bndbox.xmin.text.split('.')[0])  # encountered a few decimals - that will throw an error
                        ymin = int(obj.bndbox.ymin.text.split('.')[0])
                        xmax = int(obj.bndbox.xmax.text.split('.')[0])
                        ymax = int(obj.bndbox.ymax.text.split('.')[0])
                        item_dict = {'class_name': class_name,
                                    'class_id': class_id,
                                    'pose': pose,
                                    'truncated': truncated,
                                    'difficult': difficult,
                                    'xmin': xmin,
                                    'ymin': ymin,
                                    'xmax': xmax,
                                    'ymax': ymax}
                        boxes.append(item_dict)
    
            # get the encoded image
            img_path = os.path.join(image_dir, image_id + ".jpg")
            with tf.io.gfile.GFile(img_path, 'rb') as fid:
                encoded_jpg = fid.read()

            # now you have everything necessary to create a tf.example
            # tf.Example proto 
            # print ("    ", filename)
            # print ("     ", class_name, class_id)
            # print ("     ", sizeHeight, sizeWidth, sizeDepth)
            # print ("     ", len(boxes))

            xmins = []
            xmaxs = []
            ymins = []
            ymaxs = []
            class_names = []
            class_ids = []
            obj_areas = []
            obj_is_crowds = []
            obj_difficults = []
            obj_group_ofs = []
            obj_weights = []

            # loop through each bbox to make a list of each
            for box in boxes:
                # print ("       ", box)
                # create lists of bbox dimensions
                xmins.append(box['xmin'] / sizeWidth)
                xmaxs.append(box['xmax'] / sizeWidth)

                ymins.append(box['ymin'] / sizeHeight)
                ymaxs.append(box['ymax'] / sizeHeight)

                class_names.append(str.encode(box['class_name']))
                class_ids.append(int(box['class_id']))

                obj_areas.append(OBJ_AREA)
                obj_is_crowds.append(OBJ_IS_CROWD)
                obj_difficults.append(OBJ_DIFFICULT)
                obj_group_ofs.append(OBJ_GROUP_OF)
                obj_weights.append(OBJ_WEIGHT)

            # use the commonly defined feature dictionary
            feature = feature_obj_detect.copy()
            # thus you have a common structure for writing & reading
            # these image features

            # per image attributes
            feature['image/encoded'] = bytes_feature(encoded_jpg)
            feature['image/format'] = bytes_feature(IMG_FORMAT)
            feature['image/filename'] = bytes_feature(str.encode(filename))
            feature['image/key/sha256'] = bytes_feature(IMG_SHA256)
            feature['image/source_id'] = bytes_feature(str.encode(image_id))

            feature['image/height'] = int64_feature(int(sizeHeight))
            feature['image/width'] = int64_feature(int(sizeWidth))

            feature['image/class/text'] = bytes_list_feature(IMG_CLASS_NAMES)
            feature['image/class/label'] = int64_list_feature(IMG_CLASS_IDS)


            # per image/object attributes
            feature['image/object/bbox/xmin'] = float_list_feature(xmins)
            feature['image/object/bbox/xmax'] = float_list_feature(xmaxs)
            feature['image/object/bbox/ymin'] = float_list_feature(ymins)
            feature['image/object/bbox/ymax'] = float_list_feature(ymaxs)
            feature['image/object/class/text'] = bytes_list_feature(class_names)
            feature['image/object/class/label'] = int64_list_feature(class_ids)

            # these are all taken from default values
            feature['image/object/area'] = float_list_feature(obj_areas)
            feature['image/object/is_crowd'] = int64_list_feature(obj_is_crowds)
            feature['image/object/difficult'] = int64_list_feature(obj_difficults)
            feature['image/object/group_of'] = int64_list_feature(obj_group_ofs)
            feature['image/object/weight'] = float_list_feature(obj_weights)


            features = tf.train.Features(feature=feature)

            tf_example = tf.train.Example(features=features)
            # write to the tfrecords writer
            tf_writer.write(tf_example.SerializeToString())
            image_count = image_count + 1

        # end of loop
        # TODO - shard on larger sets
        #        https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md
        tf_writer.close()                   # close the writer
        print ('     image count:', image_count, "  class_count:", class_dict)
    return 1

def _parse_function(example_proto):
    # Parse the input using the standard dictionary
    feature = tf.io.parse_single_example(example_proto, feature_obj_detect)
    return feature

def get_class_names(label_map):
    """
    get the class names - as a dict of byte arrays
    input:  full absolute path o the label map protobuf
    output: dict of byte arrays {class id:  b'name'}
    """
    label_map = get_label_map_dict(label_map)
    label_map_reverse = {}
    for (k,v) in label_map.items():
        label_map_reverse[v] = k
    for (k,v) in label_map_reverse.items():
        label_map_reverse[k] = str.encode(v)
    return label_map_reverse

def get_dataset_length(ds):
    """
    get dataset size
      brute force iteration to get record count
    """
    num_elements = 0
    for element in ds:
        num_elements += 1
    return num_elements

def display_detection(image_tensor, class_names, threshold, prediction):
    """
    utilility to display image & prediction
    input:  image - EagerTensor
            class names - dict {class id: b'name'}
            threshold - 0.0 - 1.0  (keep if score > threshold)
            prediction - model response for this instance
    """
    # post processed prediction
    # - without the threshold applied
    detect_scores = np.asarray(prediction['detection_scores'], dtype=np.float32)
    detect_classes = np.asarray(prediction['detection_classes'], dtype=np.int8)
    detect_boxes = np.asarray(prediction['detection_boxes'], dtype=np.float32)

    # keep == w/ threshold applied
    keep_idx = np.argwhere(detect_scores > threshold)
    class_ids = detect_classes[keep_idx]
    (obj_count, discard) = class_ids.shape
    # get the class names you need to keep
    obj_class_ids = class_ids.reshape(-1)
    obj_class_names = []
    for i in obj_class_ids:
        obj_class_names.append(class_names[i])
    obj_class_names = np.asarray(obj_class_names)
    # get the bounding boxes you need to keep
    bboxes = detect_boxes[keep_idx]

    # just keep data from here on
    image_decoded_tensor = tf.image.decode_jpeg(image_tensor)
    image = image_decoded_tensor.numpy()
    pil_image = Image.fromarray(image)
    bb = bboxes.reshape(-1,4)
    ymins = bb[:,0]
    xmins = bb[:,1]
    ymaxs = bb[:,2]
    xmaxs = bb[:,3]

    print ("detected:", obj_count, obj_class_names)

    for idx in range(obj_count):
        draw_bounding_box_on_image(pil_image,ymins[idx],xmins[idx], ymaxs[idx], xmaxs[idx],
                color=STANDARD_COLORS[obj_class_ids[idx]], 
                thickness=4, display_str_list=[obj_class_names[idx]],
                use_normalized_coordinates=True)
    display.display(pil_image)
