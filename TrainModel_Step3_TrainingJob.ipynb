{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "#### tensorflow_p36 environment\n",
    "\n",
    "ref:  https://github.com/aws-samples/amazon-sagemaker-script-mode/blob/master/tf-eager-script-mode/tf-eager-sm-scriptmode.ipynb\n",
    "\n",
    "Note:  AWS tutorials tend to name the post-training data = 'test'.   Most books call this 'val' for validation or 'eval' for model evaluation.   I named it 'val'.   So if you follow the example, AWS calls it 'test', I call it 'val'\n",
    "\n",
    "## Step 3 - SageMaker HOSTED Training\n",
    "At this point, you know you have a working training script (train.py).  So, you can have SageMaker deploy it to outside (not local) resources.  \n",
    "\n",
    "### Output\n",
    "After training in the HOSTED SageMaker environment, the model is pushed to S3.  This notebook pulls that newly trained model checkpoint to this (SageMaker) computer.   This notebook will then convert that checkpoint to a tflite model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "SageMaker will pull the data from S3.    This is much faster than putting it in your Docker.   However, this is somewhat confusing because the MobileNet software (and utilities) were looking for data path in the config file.  We need to merge this approach:\n",
    "- allow SageMaker to pull from S3\n",
    "- AND, we want to continue leveraging the config design pattern\n",
    "\n",
    "The other challenge is working with tarballs versus tfrecord files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefix = 'cfaanalyticsresearch-sagemaker'\n",
    "\n",
    "traindata_s3_prefix = '{}/datasets/cfa_products/train'.format(s3_prefix)\n",
    "valdata_s3_prefix = '{}/datasets/cfa_products/val'.format(s3_prefix)\n",
    "print (traindata_s3_prefix)\n",
    "print (valdata_s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIP\n",
    "you would be wise to test and make sure you path is good before continuing!!  \n",
    "cut/paste the printed value and put it into the following form.   You can run this AWS CLI command in a new cell.  \n",
    "\n",
    "! aws s3 ls s3://cfaanalyticsresearch-sagemaker/datasets/cfa_products/train/  \n",
    "! aws s3 ls s3://cfaanalyticsresearch-sagemaker/datasets/cfa_products/val/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy data from local (SageMaker instance) to S3\n",
    "If you ran the TrainModel_Step1 notebook, the data was moved to:\n",
    "- code/tfrecords/train \n",
    "- code/tfrecords/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd\n",
    "# train_s3 == a full s3 URL, note that it is a folder, not a file\n",
    "# this operation may take a few seconds (depending on data size) - it is silently copying\n",
    "#     data from local drive on SageMaker to s3\n",
    "train_s3 = sagemaker.Session().upload_data(path='./code/tfrecords/train/', key_prefix=traindata_s3_prefix)\n",
    "val_s3 = sagemaker.Session().upload_data(path='./code/tfrecords/val/', key_prefix=valdata_s3_prefix)\n",
    "\n",
    "inputs = {'train':train_s3, 'val': val_s3}\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/opt/ml/model'     # this is related to how it gets deployed in the Docker\n",
    "                                # this is a SAGEMAKER thing - don't confuse with the model_dir \n",
    "                                # that we have inside our code\n",
    "# p2.xlarge == $1/hr\n",
    "# p3.2xlarge = $3/hr\n",
    "# this is a very controlled train & quick so the better server makes sense\n",
    "# if you are developing - use the p2\n",
    "train_instance_type = 'ml.p3.2xlarge'   \n",
    "\n",
    "hyperparameters = {'pipeline_config_path' : 'sagemaker_mobilenet_v1_ssd_retrain.config',\n",
    "                   'num_train_steps' : '502',\n",
    "                   'num_eval_steps' : '10'\n",
    "                  }\n",
    "\n",
    "# SageMaker Execution Role\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir='code',\n",
    "                       model_dir=model_dir,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=role,\n",
    "                       base_job_name='cfa-products-mobilenet-v1-SSD',\n",
    "                       framework_version='1.13',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreiving the Trained Model\n",
    "SageMaker created a Docker job to train our model and sent it off to external resources (external meaning - not this computer.)   Now we need to get the result - it's not on this computer.\n",
    "\n",
    "- ./trained_model:  this local directory is (should be) empty\n",
    "- the trained model is on s3 - in the next step we are copying the result to code/model\n",
    "- now you'll see the tarball\n",
    "\n",
    "/trained_model is NOT under the code/ directory.  Primarily because there is no reason to include it inside the Docker training job (in the event you re-run.)  That would just carry extra baggage around for no reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current directory is still the top project directory - NOT the code directory\n",
    "!aws s3 cp {estimator.model_data} ./trained_model/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!tar -xvzf ./trained_model/model.tar.gz -C ./trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert (Trained) Model Checkpoint to a tflite Model\n",
    "\n",
    "WARNING: labels.txt - not included, don't think we need this though it was in the Coral project.  We think it's getting the label name from the label map (*.pbtxt) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./convert_checkpoint_to_edgetpu_tflite.sh --checkpoint_num 502 --pipeline_config ../code/sagemaker_mobilenet_v1_ssd_retrain.config\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
