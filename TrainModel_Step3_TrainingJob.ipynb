{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "#### tensorflow_p36 environment\n",
    "\n",
    "ref:  https://github.com/aws-samples/amazon-sagemaker-script-mode/blob/master/tf-eager-script-mode/tf-eager-sm-scriptmode.ipynb\n",
    "\n",
    "Note:  AWS tutorials tend to name the post-training data = 'test'.   Most books call this 'val' for validation or 'eval' for model evaluation.   I named it 'val'.   So if you follow the example, AWS calls it 'test', I call it 'val'\n",
    "\n",
    "## Step 3 - SageMaker HOSTED Training\n",
    "At this point, you know you have a working training script (train.py).  So, you can have SageMaker deploy it to outside (not local) resources.  \n",
    "\n",
    "### Output\n",
    "After training in the HOSTED SageMaker environment, the model is pushed to S3.  This notebook pulls that newly trained model checkpoint to this (SageMaker) computer.   This notebook will then convert that checkpoint to a tflite model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals\n",
    "It is helpful to increment the version number, you'll see this in the model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = os.getcwd()\n",
    "TRAINED_MODEL_DIR = os.path.join(PROJECT_DIR, 'trained_model')\n",
    "\n",
    "NUM_TRAINING_STEPS = 5000\n",
    "VERSION = 104\n",
    "PIPELINE_CONFIG = 'sagemaker_mobilenet_v1_ssd_retrain.config'\n",
    "S3_PREFIX = 'cfa-datasciencesb-sagemaker'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "SageMaker will pull the data from S3.    This is much faster than putting it in your Docker.   However, this is somewhat confusing because the MobileNet software (and utilities) were looking for data path in the config file.  We need to merge this approach:\n",
    "- allow SageMaker to pull from S3\n",
    "- AND, we want to continue leveraging the config design pattern\n",
    "\n",
    "The other challenge is working with tarballs versus tfrecord files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_s3_prefix = '{}/datasets/cfa_products/train'.format(S3_PREFIX)\n",
    "valdata_s3_prefix = '{}/datasets/cfa_products/val'.format(S3_PREFIX)\n",
    "print (traindata_s3_prefix)\n",
    "print (valdata_s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIP\n",
    "you would be wise to test and make sure you path is good before continuing!!  \n",
    "cut/paste the printed value and put it into the following form.   You can run this AWS CLI command in a new cell.  \n",
    "\n",
    "! aws s3 ls s3://cfaanalyticsresearch-sagemaker/datasets/cfa_products/train/  \n",
    "! aws s3 ls s3://cfaanalyticsresearch-sagemaker/datasets/cfa_products/val/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy data from local (SageMaker instance) to S3\n",
    "If you ran the TrainModel_Step1 notebook, the data was moved to:\n",
    "- code/tfrecords/train \n",
    "- code/tfrecords/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd\n",
    "# train_s3 == a full s3 URL, note that it is a folder, not a file\n",
    "# this operation may take a few seconds (depending on data size) - it is silently copying\n",
    "#     data from local drive on SageMaker to s3\n",
    "train_s3 = sagemaker.Session().upload_data(path='./code/tfrecords/train/', key_prefix=traindata_s3_prefix)\n",
    "val_s3 = sagemaker.Session().upload_data(path='./code/tfrecords/val/', key_prefix=valdata_s3_prefix)\n",
    "\n",
    "inputs = {'train':train_s3, 'val': val_s3}\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/opt/ml/model'     # this is related to how it gets deployed in the Docker\n",
    "                                # this is a SAGEMAKER thing - don't confuse with the model_dir \n",
    "                                # that we have inside our code\n",
    "# p2.xlarge == $1/hr\n",
    "# p3.2xlarge = $3/hr\n",
    "# this is a very controlled train & quick so the better server makes sense\n",
    "# if you are developing - use the p2\n",
    "train_instance_type = 'ml.p3.2xlarge'   \n",
    "# TODO\n",
    "#  o  try a different config for p3.2xlarge that has more images in the batch size to \n",
    "#     take advantage of the GPU memory\n",
    "#  o  still have to figure out the data\n",
    "#     - including the data under code/ directory means it is in the tarball\n",
    "#     - but that means the inputs is a wasted step (and it takes longer to create the Docker image)\n",
    "hyperparameters = {'pipeline_config_path' : 'sagemaker_mobilenet_v1_ssd_retrain.config',\n",
    "                   'num_train_steps' : NUM_TRAINING_STEPS,\n",
    "                   'num_eval_steps' : '1000'\n",
    "                  }\n",
    "\n",
    "# SageMaker Execution Role\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Version\n",
    "aka here it is frame_version -- be sure to bump it up as you upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(entry_point='train115.py',\n",
    "                       source_dir='code',\n",
    "                       model_dir=model_dir,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=role,\n",
    "                       base_job_name='cfa-products-mobilenet-v1-ssd',\n",
    "                       framework_version='1.14',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring the Progress\n",
    "You can watch the utilization of resources on the console.\n",
    "- Training Jobs\n",
    "- expand the active job\n",
    "\n",
    "you'll see GPU utilization, memory, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print (\"start time: {:.4f}\".format(start_time))\n",
    "\n",
    "# this will create the training job\n",
    "# you can also see this on the SageMaker Training Job console\n",
    "# 3 minute overhead prepping the servers, downloading data\n",
    "# 5000 steps on a p3.2xlarge == 18 min (training time)\n",
    "# overall ~ 20 minutes\n",
    "estimator.fit(inputs)\n",
    "\n",
    "# show the time\n",
    "finish_time = time.time()\n",
    "minutes = (finish_time - start_time) / 60\n",
    "print(\"time spent: {:.4f}\".format(finish_time - start_time))\n",
    "print(\"in minutes: {:.4f}\".format(minutes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Model\n",
    "\n",
    "https://sagemaker.readthedocs.io/en/stable/estimators.html\n",
    "\n",
    "if you job completed successfully, you should see a message (above).   Something like:\n",
    "Uploading generated training model\n",
    "Training job completed.\n",
    "\n",
    "If you don't export your model correctly - using the Exporter and the correct enironment variable values, you might not have anything to upload.   Your training script has to generate the correct artifacts for this proccess to work.\n",
    "\n",
    "You can see output on S3 in the sagemaker bucket.  /output/model.tar.gz    In this tarball, you'll see /export/Servo/<version numer - which is a timestamp>/saved_graph.pb & variables\n",
    "    \n",
    "At this point you have a completed training job.   You do NOT have a SageMaker model.  i.e. look on the console:  Models - it's not there.   You have to create it next.\n",
    "\n",
    "### Using the Console to go from Training Job -> Model\n",
    "I'm sure there is a way to do this - but I don't understand it yet.\n",
    "    \n",
    "### Training Job Artifacts\n",
    "#### Training Image  \n",
    "You need the INFERENCE image\n",
    " \n",
    "#### Output - S3 model artifact\n",
    "this is what you need below\n",
    "  \n",
    "### Create Model\n",
    "Models / Create  \n",
    "name (example): model-mobilenet-ssdv1-cfa-products  \n",
    "IAM Role:  you can use the SageMaker execution role  \n",
    "(select) Profide model artificats and inference image location \n",
    "- location of inference code:  !!! you need this - I don't know what it is yet !!!\n",
    "- location of model artifacts: s3://sagemaker-us-east-1-586454201570.s3.amazonaws.com/cfa-products-mobilenet-v1-ssd-2019-09-24-19-47-57-172/output/model.tar.gz\n",
    "- container host name: blank\n",
    "\n",
    "No VPC\n",
    "\n",
    "## Conclusion\n",
    "So, you can't complete this step.   The easiest workaround is, deploy the model using the code below.  When you deploy, it's automating all of this stuff anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Retreiving the Trained Model\n",
    "SageMaker created a Docker job to train our model and sent it off to external resources (external meaning - not this computer.)   Now we need to get the result - it's not on this computer.\n",
    "\n",
    "- ./trained_model:  this local directory is (should be) empty\n",
    "- the trained model is on s3 - in the next step we are copying the result to code/model\n",
    "- now you'll see the tarball\n",
    "\n",
    "/trained_model is NOT under the code/ directory.  Primarily because there is no reason to include it inside the Docker training job (in the event you re-run.)  That would just carry extra baggage around for no reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (estimator.output_path)\n",
    "print (estimator.base_job_name)\n",
    "print (estimator.model_uri)\n",
    "print (estimator.checkpoint_s3_uri)\n",
    "print (estimator.tags)\n",
    "print (estimator.model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current directory is still the top project directory - NOT the code directory\n",
    "# this will get the current training artifacts\n",
    "# !aws s3 cp {estimator.model_data} ./trained_model/model.tar.gz\n",
    "\n",
    "# this will get the artifacts from a prevous job\n",
    "model_data = estimator.model_data\n",
    "print (\"retrieve trained model:\", model_data)\n",
    "print (\"copy to:\", TRAINED_MODEL_DIR)\n",
    "! aws s3 cp {model_data} {TRAINED_MODEL_DIR}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.ckpt-XXXX\n",
    "Make note of the checkpoint files.   For example, if you  said run 5000 steps, there should be a checkpoint file:  \n",
    "model.ckpt-5000*\n",
    "\n",
    "This is the file that will be converted \n",
    "- frozen graph\n",
    "- tflite model\n",
    "\n",
    "#### CONFUSION WARNING - there is a difference between training locally and training with SageMaker\n",
    "- Local, the checkpoint file is in code/model\n",
    "- SageMaker job, you are about to extract the output to /trained_model  \n",
    "\n",
    "The conversion script below is expecting the checkpoint to be in /trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf ./trained_model/model.tar.gz -C ./trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert (Trained) Model Checkpoint to a tflite Model\n",
    "\n",
    "WARNING: labels.txt - not included, don't think we need this though it was in the Coral project.  We think it's getting the label name from the label map (*.pbtxt) file\n",
    "\n",
    "ALSO NOTE:  the script is named convert_checkpoint_to_edge_tflite.sh  \n",
    "Well... the name is no longer totally accurate\n",
    "- I took this from the original Coral TPU tutorial\n",
    "- Another step is required for compiling for the EdgeTPU (not really relevant here since we are confined to AWS where there is no TPU -- so we skip that stuff)\n",
    "- And, I added a step that converts the checkpoint to a TENSORFLOW frozen graph \n",
    "  - note that this generates a frozen_inference_graph.pb\n",
    "  - it ALSO generates a saved model graph.pb  \n",
    "  THESE (frozen graph & saved model) are NOT the SAME!!  \n",
    "  https://stackoverflow.com/questions/46547319/error-when-parsing-graph-def-from-string\n",
    "\n",
    "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/exporting_models.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAKE UP - make sure the checkpoint num == hyperparameters/num_train_steps\n",
    "# convert checkpoint is a task script - located in the tasks/ directory\n",
    "os.chdir(\"tasks\")  \n",
    "! ./convert_checkpoint_to_edgetpu_tflite.sh --checkpoint_num {NUM_TRAINING_STEPS} --pipeline_config {PIPELINE_CONFIG}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory back to the project directory\n",
    "# see the tflite model artifacts\n",
    "os.chdir(PROJECT_DIR)\n",
    "! ls tflite_model -l\n",
    "! ls tensorflow_model -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the converted model artifacts to S3 (SageMaker Training Job Data)\n",
    "Add these artifacts:\n",
    "- frozen model graph\n",
    "- tflite model\n",
    "\n",
    "To the SageMaker folder that has all of the training job artifacts - so everything is together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator.model_data == the s3 url & file for the model output tarball\n",
    "# we need the s3 url only; so you have to extract it; and, you need a / at the end\n",
    "s3_model_artifacts = os.path.dirname(estimator.model_data) + '/'\n",
    "\n",
    "# now you can copy these converted files up to s3\n",
    "!aws s3 cp  ./tensorflow_model {s3_model_artifacts}tensorflow_model --recursive\n",
    "!aws s3 cp  ./tflite_model {s3_model_artifacts}tflite --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy\n",
    "\n",
    "If you use this deploy method, it will build the model, endpoint config and the endpoint for you -- and it works.  I have had problems doing this manually.    So, until you figure out how to do it reliably as sub-steps.  It's easiest to just deploy, then delete the active endpoint.  \n",
    "\n",
    "If you do that, then you can reliably recreate an endpoint from the endpoint config.\n",
    "\n",
    "## Optional\n",
    "This is not required.  You can take your model artifacts:\n",
    "- tensorflow frozen graph\n",
    "- tensorflow Lite frozen graph\n",
    "- tensorflow Lite model file\n",
    "They are completely useable.  Note that TensorFlow is quite different than TensorFlow Lite (which is much different than the EdgeTPU model.)  Missing is a TensorRT variant by the way.   You can test these models locally or on a different machine.   You don't have to deploy (but if you don't you'll lose the estimator object)\n",
    "\n",
    "At this moment, you have an SageMaker Estimator.  This is a bunch of information about a model.   At this time, you can't recreate an Estimator from a file (i.e. restore/create from file).   So if you're going to deploy it - now is the time!   \n",
    "\n",
    "### Version\n",
    "see version in the GLOBALS  \n",
    "if you use the same version number, it seems to reuse some stuff\n",
    "\n",
    "## $$\n",
    "When you deploy, you are paying for the endpoint server!! E.g. deploy your model to a p2.xlarge (which costs $1.26/hr) and you are paying whether you use it or not!  \n",
    "\n",
    "### DON'T LEAVE YOUR ENDPOINT RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where you'll be glad you used a new version number\n",
    "\n",
    "model_name = 'model-mobilenet-v1-ssd-cfa-products-{}'.format(VERSION)\n",
    "endpoint_name = 'ep-mobilenet-v1-ssd-cfa-products-{}'.format(VERSION)\n",
    "print (\"model:\", model_name)\n",
    "print (\"endpoint:\", endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Model\n",
    "Again, if you don't change the version number, it might re-use some stuff which makes it tougher to understand this step.\n",
    "\n",
    "As the estimator.deploy() is in progress, you'll see:\n",
    "- a model listed on the console\n",
    "- an endpoint configuration\n",
    "- an endpoint\n",
    "\n",
    "you can now use that endpoint - but you can also re-use the endpoint configuration.\n",
    "\n",
    "DON'T leave your endpoints running - this costs money - EC2 instance rates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, \n",
    "#  - I'm using a p2.2xlarge for SageMaker server\n",
    "#  - We trained on a p3.2xlarge (sent the training job)\n",
    "#  - now we can deploy to yet a 3rd machine - in this case, I'm selecting a p2 because it's the cheapest GPU\n",
    "\n",
    "# THIS WILL TAKE A FEW MINUTES - Dockerizing (and it's probably carrying your data around if you put it in the code directory)\n",
    "predictor = estimator.deploy(initial_instance_count=1,\n",
    "                             instance_type='ml.p2.xlarge',\n",
    "                             model_name=model_name,\n",
    "                             endpoint_name=endpoint_name\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the console:\n",
    "- Endpoints - you'll see ep-mobilenet-v1-ssd-cfa-products-<version> \n",
    "- Endpoint Configurations - you'll see it here\n",
    "- Models - you'll see it here\n",
    "    \n",
    "S3 - you'll see the training job artifacts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Endpoint\n",
    "Jump to the notebook:  DetectModel_Step2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
