{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "#### tensorflow_p36 environment\n",
    "\n",
    "ref:  https://github.com/aws-samples/amazon-sagemaker-script-mode/blob/master/tf-eager-script-mode/tf-eager-sm-scriptmode.ipynb\n",
    "\n",
    "Note:  AWS tutorials tend to name the post-training data = 'test'.   Most books call this 'val' for validation or 'eval' for model evaluation.   I named it 'val'.   So if you follow the example, AWS calls it 'test', I call it 'val'\n",
    "\n",
    "## Step 3 - SageMaker HOSTED Training\n",
    "At this point, you know you have a working training script (train.py).  So, you can have SageMaker deploy it to outside (not local) resources.  \n",
    "\n",
    "### Output\n",
    "After training in the HOSTED SageMaker environment, the model is pushed to S3.  This notebook pulls that newly trained model checkpoint to this (SageMaker) computer.   This notebook will then convert that checkpoint to a tflite model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "SageMaker will pull the data from S3.    This is much faster than putting it in your Docker.   However, this is somewhat confusing because the MobileNet software (and utilities) were looking for data path in the config file.  We need to merge this approach:\n",
    "- allow SageMaker to pull from S3\n",
    "- AND, we want to continue leveraging the config design pattern\n",
    "\n",
    "The other challenge is working with tarballs versus tfrecord files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefix = 'cfaanalyticsresearch-sagemaker'\n",
    "\n",
    "traindata_s3_prefix = '{}/datasets/cfa_products/train'.format(s3_prefix)\n",
    "valdata_s3_prefix = '{}/datasets/cfa_products/val'.format(s3_prefix)\n",
    "print (traindata_s3_prefix)\n",
    "print (valdata_s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIP\n",
    "you would be wise to test and make sure you path is good before continuing!!  \n",
    "cut/paste the printed value and put it into the following form.   You can run this AWS CLI command in a new cell.  \n",
    "\n",
    "! aws s3 ls s3://cfaanalyticsresearch-sagemaker/datasets/cfa_products/train/  \n",
    "! aws s3 ls s3://cfaanalyticsresearch-sagemaker/datasets/cfa_products/val/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy data from local (SageMaker instance) to S3\n",
    "If you ran the TrainModel_Step1 notebook, the data was moved to:\n",
    "- code/tfrecords/train \n",
    "- code/tfrecords/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd\n",
    "# train_s3 == a full s3 URL, note that it is a folder, not a file\n",
    "# this operation may take a few seconds (depending on data size) - it is silently copying\n",
    "#     data from local drive on SageMaker to s3\n",
    "train_s3 = sagemaker.Session().upload_data(path='./code/tfrecords/train/', key_prefix=traindata_s3_prefix)\n",
    "val_s3 = sagemaker.Session().upload_data(path='./code/tfrecords/val/', key_prefix=valdata_s3_prefix)\n",
    "\n",
    "inputs = {'train':train_s3, 'val': val_s3}\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/opt/ml/model'     # this is related to how it gets deployed in the Docker\n",
    "                                # this is a SAGEMAKER thing - don't confuse with the model_dir \n",
    "                                # that we have inside our code\n",
    "# p2.xlarge == $1/hr\n",
    "# p3.2xlarge = $3/hr\n",
    "# this is a very controlled train & quick so the better server makes sense\n",
    "# if you are developing - use the p2\n",
    "train_instance_type = 'ml.p3.2xlarge'   \n",
    "# TODO\n",
    "#  o  try a different config for p3.2xlarge that has more images in the batch size to \n",
    "#     take advantage of the GPU memory\n",
    "#  o  still have to figure out the data\n",
    "#     - including the data under code/ directory means it is in the tarball\n",
    "#     - but that means the inputs is a wasted step (and it takes longer to create the Docker image)\n",
    "hyperparameters = {'pipeline_config_path' : 'sagemaker_mobilenet_v1_ssd_retrain.config',\n",
    "                   'num_train_steps' : '5000',\n",
    "                   'num_eval_steps' : '1000'\n",
    "                  }\n",
    "\n",
    "# SageMaker Execution Role\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir='code',\n",
    "                       model_dir=model_dir,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=role,\n",
    "                       base_job_name='cfa-products-mobilenet-v1-SSD',\n",
    "                       framework_version='1.13',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print (\"start time: {:.4f}\".format(start_time))\n",
    "\n",
    "# this will create the training job\n",
    "# you can also see this on the SageMaker Training Job console\n",
    "# 3 minute overhead prepping the servers, downloading data\n",
    "# 5000 steps on a p3.2xlarge == 18 min (training time)\n",
    "# overall ~ 20 minutes\n",
    "estimator.fit(inputs)\n",
    "\n",
    "# show the time\n",
    "finish_time = time.time()\n",
    "minutes = (finish_time - start_time) / 60\n",
    "print(\"time spent: {:.4f}\".format(finish_time - start_time))\n",
    "print(\"in minutes: {:.4f}\".format(minutes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreiving the Trained Model\n",
    "SageMaker created a Docker job to train our model and sent it off to external resources (external meaning - not this computer.)   Now we need to get the result - it's not on this computer.\n",
    "\n",
    "- ./trained_model:  this local directory is (should be) empty\n",
    "- the trained model is on s3 - in the next step we are copying the result to code/model\n",
    "- now you'll see the tarball\n",
    "\n",
    "/trained_model is NOT under the code/ directory.  Primarily because there is no reason to include it inside the Docker training job (in the event you re-run.)  That would just carry extra baggage around for no reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current directory is still the top project directory - NOT the code directory\n",
    "!aws s3 cp {estimator.model_data} ./trained_model/model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.ckpt-XXXX\n",
    "Make note of the checkpoint files.   For example, if you  said run 5000 steps, there should be a checkpoint file:  \n",
    "model.ckpt-5000*\n",
    "\n",
    "This is the file that will be converted \n",
    "- frozen graph\n",
    "- tflite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf ./trained_model/model.tar.gz -C ./trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert (Trained) Model Checkpoint to a tflite Model\n",
    "\n",
    "WARNING: labels.txt - not included, don't think we need this though it was in the Coral project.  We think it's getting the label name from the label map (*.pbtxt) file\n",
    "\n",
    "ALSO NOTE:  the script is named convert_checkpoint_to_edge_tflite.sh  \n",
    "Well... the name is no longer totally accurate\n",
    "- I took this from the original Coral TPU tutorial\n",
    "- Another step is required for compiling for the EdgeTPU (not really relevant here since we are confined to AWS where there is no TPU -- so we skip that stuff)\n",
    "- And, I added a step that converts the checkpoint to a TENSORFLOW frozen graph \n",
    "  - note that this generates a frozen_inference_graph.pb\n",
    "  - it ALSO generates a saved model graph.pb  \n",
    "  THESE (frozen graph & saved model) are NOT the SAME!!  \n",
    "  https://stackoverflow.com/questions/46547319/error-when-parsing-graph-def-from-string\n",
    "\n",
    "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/exporting_models.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAKE UP - make sure the checkpoint num == hyperparameters/num_train_steps\n",
    "# convert checkpoint is a task script - located in the tasks/ directory\n",
    "os.chdir(\"tasks\")  \n",
    "! ./convert_checkpoint_to_edgetpu_tflite.sh --checkpoint_num 5000 --pipeline_config sagemaker_mobilenet_v1_ssd_retrain.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory back to the project directory\n",
    "# see the tflite model artifacts\n",
    "os.chdir(PROJECT_DIR)\n",
    "! ls tflite_model -l\n",
    "! ls tensorflow_model -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the converted model artifacts to S3 (SageMaker Training Job Data)\n",
    "Add these artifacts:\n",
    "- frozen model graph\n",
    "- tflite model\n",
    "\n",
    "To the SageMaker folder that has all of the training job artifacts - so everything is together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator.model_data == the s3 url & file for the model output tarball\n",
    "# we need the s3 url only; so you have to extract it; and, you need a / at the end\n",
    "s3_model_artifacts = os.path.dirname(estimator.model_data) + '/'\n",
    "\n",
    "# now you can copy these converted files up to s3\n",
    "!aws s3 cp  ./tensorflow_model {s3_model_artifacts}tensorflow_model --recursive\n",
    "!aws s3 cp  ./tflite_model {s3_model_artifacts}tflite --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy\n",
    "## Optional\n",
    "This is not required.  You can take your model artifacts:\n",
    "- tensorflow frozen graph\n",
    "- tensorflow Lite frozen graph\n",
    "- tensorflow Lite model file\n",
    "They are completely useable.  Note that TensorFlow is quite different than TensorFlow Lite (which is much different than the EdgeTPU model.)  Missing is a TensorRT variant by the way.   You can test these models locally or on a different machine.   You don't have to deploy (but if you don't you'll lose the estimator object)\n",
    "\n",
    "At this moment, you have an SageMaker Estimator.  This is a bunch of information about a model.   At this time, you can't recreate an Estimator from a file (i.e. restore/create from file).   So if you're going to deploy it - now is the time!   \n",
    "\n",
    "### $$\n",
    "When you deploy, you are paying for the endpoint server!! E.g. deploy your model to a p2.xlarge (which costs $1.26/hr) and you are paying whether you use it or not!  \n",
    "\n",
    "### DON'T LEAVE YOUR ENDPOINT RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, \n",
    "#  - I'm using a p2.2xlarge for SageMaker server\n",
    "#  - We trained on a p3.2xlarge (sent the training job)\n",
    "#  - now we can deploy to yet a 3rd machine - in this case, I'm selecting a p2 because it's the cheapest GPU\n",
    "\n",
    "# THIS WILL TAKE A FEW MINUTES - Dockerizing (and it's probably carrying your data around if you put it in the code directory)\n",
    "predictor = estimator.deploy(initial_instance_count=1,instance_type='ml.p2.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Endpoint\n",
    "Jump to the notebook:  DetectModel_Step2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
