{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding tf.Example w/ TF 2.0 \n",
    "\n",
    "First, understand this is the world's WORST named class tf.Example - not an example - it is an Example of data (I guess).   TF Version - there are issues here!!!\n",
    "- If you are using SageMaker, you may run into version issues;  you can unintall & reinstall, but MAY run into SageMaker compatibility issues - as of 20190930\n",
    "- If you run w/ TF v 1.14, you can try part of this with Eager Execution, but you'll run into some problems with the data types\n",
    "- Or, you can run most of this with TF v 1.14 with NO Eager - \n",
    "\n",
    "If you start getting errors - you need to understand if you are in Eager or Graph Mode and what version you're on. \n",
    "\n",
    "\n",
    "https://www.tensorflow.org/beta/tutorials/load_data/images  \n",
    "https://www.tensorflow.org/tutorials/load_data/tf_records\n",
    "\n",
    "You will need these skills!  What is an image, tf.Example, serialized example etc.  \n",
    "You won't get far with served models without this understanding.\n",
    "\n",
    "## TensorFlow 2.0 Beta\n",
    "There is an issue with feature2 in the tutorial - a byte string.   It will definitely work if you get the right combination of versions and Eager Execution - but it's confusing and it's not really critical to your overall objective - so, I commented OUT including feature2 in features.   This github issue documents the problem:\n",
    "https://github.com/tensorflow/tensorflow/issues/27181\n",
    "\n",
    "### TensorFlow Versions\n",
    "Restart the Kernel everytime you switch versions or Eager settings - if you just re-run, it won't take effect.\n",
    "* v 1.14 - withOUT Eager Execution: There isn't much value in working w/ TF v1.x witout Eager on since Eager is the future of 2.0.  But, there are things to be learned.   You'll gain a better understanding of Eager.  This is particularly valuable if you have worked in Graph Mode w/ TF 1.x for years\n",
    "  - f2.numpy() won't work;  f2 is a byte string - which has no .numpy() method\n",
    "  - writer.write (writing the tfrecord) - no error but it doesn't do anything.   This probably needs to be executed in a graph.   This would be an old pattern (graph) so it's not worth dealing with.\n",
    "* v 1.14 - WITH Eager Execution:\n",
    "  - the graph step won't work of course\n",
    "  - the f2.numpy() DOES work because f2 is no longer a byte string, it is now a Tensor dtype=string.  Being a Tensor, there IS a .numpy() method to return bytes\n",
    "  - the TEST - tf_serialize_example(f0,f1,f2,f3) Won't work - you'll get the wierd can't copy bytes error.  This is probably because this py_function() runs in graph mode.  So this is probably to be expected -- it's not in the tutorial - I added it :)\n",
    "  - ds.map(tf_serialize_example) - DOES work (probably because I took out the features = feature2 line\n",
    "  - And, it DOES write a tfrecord file\n",
    "* v 2.0 beta (20191001)\n",
    "  - Everything work (with feature2 commented out in the features= function)\n",
    "  - even the TEST - tf_serialize_example(f0,f1,f2,f3)\n",
    "* v 2.0 (20191001) pip install tensororflow-gpu gives you 1.14, not 2.0, best you can get today is 2.0b1\n",
    "\n",
    "#### Conclusion - as of 20191001, TensorFlow 2.0 is supposedly out - but it's not really.   Everything works with 1.14 & Eager + it's compatible with SageMaker - so stick with v 1.14 for today \n",
    "\n",
    "### Do this THIRD, first, do UnderstandingTF_IO & UnderstandingImages.ipynb\n",
    "\n",
    "### tf.Example w/ SSD Images\n",
    "There is another similar notebook:  UnderstandingObjectDetectionExamples  \n",
    "Work through these basic concepts then you can apply to the SSD Images in this other notebook.   Also, this other notebook (along with Make_TFRecords) will show you how to make tfrecord files - which are based on tf.Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.train.Example\n",
    "Can't find python API documentation - what are the methods?  \n",
    "- FromString\n",
    "- SerializeToString\n",
    "- ? ParseSingle\n",
    "- ? Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib\n",
    "import random\n",
    "import IPython.display as display\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# be sure to reset the kernel\n",
    "# !pip uninstall -y tensorflow-gpu\n",
    "# !pip install -q tensorflow==1.14\n",
    "# !pip install -q tensorflow==2.0.0-beta1\n",
    "# doesn't exist yet\n",
    "# !pip install -q tensorflow-gpu==2.0\n",
    "# you'll get a tensorflow-serving-api error\n",
    "# do your tensorflow-serving (SageMaker work in another session)\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this won't work with Tensorflow 2.0\n",
    "# if you have TF 2.0 loaded, you can't set eager - it's forced on\n",
    "\n",
    "print ('TensorFlow Version:', tf.__version__)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = os.getcwd()\n",
    "\n",
    "IMAGE_DIR = os.path.join(PROJECT_DIR, \"data/jpeg_images\")\n",
    "ANNOTATION_DIR = os.path.join(PROJECT_DIR, \"data/annotations\")\n",
    "\n",
    "MODEL_PATH = os.path.join(PROJECT_DIR, \"trained_model/export/Servo/1564778509\")\n",
    "LABEL_MAP = os.path.join(PROJECT_DIR, \"code/cfa_prod_label_map.pbtxt\")\n",
    "\n",
    "# you can get data using the TrainModel_Step1_Local notebook\n",
    "TEST_TFRECORDS_PATH =  os.path.join(PROJECT_DIR, \"code/tfrecords/test/\")\n",
    "                                    \n",
    "# scratch area\n",
    "TMP_DIR = os.path.join(PROJECT_DIR, 'tmp')\n",
    "\n",
    "# NAME - get this from the console\n",
    "ENDPOINT_NAME = \"ep-mobilenet-ssd\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a description of the features.\n",
    "FEATURE_DESCRIPTION = {\n",
    "    'feature0': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'feature1': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    # 'feature2': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'feature3': tf.io.FixedLenFeature([], tf.float32, default_value=0.0),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_glob = os.path.join(IMAGE_DIR, '*.jpg')\n",
    "all_image_paths = tf.io.gfile.glob(image_glob)\n",
    "\n",
    "# inplace shuffle\n",
    "random.shuffle(all_image_paths)\n",
    "all_image_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for data type conversions\n",
    "\n",
    "### Note: \n",
    "To stay simple, this example only uses scalar inputs. The simplest way to handle non-scalar features is to use tf.serialize_tensor to convert tensors to binary-strings. Strings are scalars in tensorflow. Use tf.parse_tensor to convert the binary-string back to a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING - this bytes feature seems to be a problem\n",
    "#         - i am not going to use it - taking the byte feature out of this example\n",
    "#         - but leaving the function\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_string = b'test_string'\n",
    "ft_byte_string = _bytes_feature(byte_string)\n",
    "print(\"Feature - bytes:\", type(ft_byte_string))\n",
    "print(\"   value:\", ft_byte_string)\n",
    "\n",
    "# unencoded bytes - encoded to utf-8\n",
    "print(_bytes_feature(u'test_bytes'.encode('utf-8')))\n",
    "\n",
    "print(_float_feature(np.exp(1)))\n",
    "print(_int64_feature(True))\n",
    "print(_int64_feature(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Scalar Example - an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = tf.io.read_file(all_image_paths[3])\n",
    "print (\"img tensor:\", type(img_tensor), img_tensor.dtype)\n",
    "\n",
    "# serialize the tensor\n",
    "ser_img_tensor =tf.io.serialize_tensor(img_tensor)\n",
    "print (\"serialized:\", type(ser_img_tensor), ser_img_tensor.dtype)\n",
    "\n",
    "print (img_tensor == ser_img_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tf.Example message\n",
    "\n",
    "#### Make up some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of observations in the dataset\n",
    "n_observations = int(1e4)\n",
    "\n",
    "# boolean feature, encoded as False or True\n",
    "feature0 = np.random.choice([False, True], n_observations)\n",
    "print (type(feature0), feature0.shape, feature0[:5])\n",
    "\n",
    "# integer feature, random from 0 .. 4\n",
    "feature1 = np.random.randint(0, 5, n_observations)\n",
    "print (type(feature1), feature1.shape, feature1[:5])\n",
    "\n",
    "# string feature - note: string as a byte array\n",
    "strings = np.array([b'cat', b'dog', b'chicken', b'horse', b'goat'])\n",
    "feature2 = strings[feature1]\n",
    "print (type(feature2), feature2.shape, feature2[:5])\n",
    "\n",
    "# float feature, from a standard normal distribution\n",
    "feature3 = np.random.randn(n_observations)\n",
    "print (type(feature3), feature3.shape, feature3[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize Example\n",
    "\n",
    "#### DIFFERENT FROM TUTORIAL\n",
    "I think this is better :)\n",
    "- define the feature as a global\n",
    "- thus, define it in ONE PLACE only\n",
    "- that way you have consistency everywhere\n",
    "\n",
    "It nets out the same - but it enforces the idea of having the same format everywhere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example(feature0, feature1, feature2, feature3):\n",
    "  \"\"\"\n",
    "  Creates a tf.Example message ready to be written to a file.\n",
    "  \"\"\"\n",
    "\n",
    "  # Create a dictionary mapping the feature name to the tf.Example-compatible\n",
    "  # data type.\n",
    "  feature = FEATURE_DESCRIPTION.copy()\n",
    "  feature['feature0'] = _int64_feature(feature0)\n",
    "  feature['feature1'] = _int64_feature(feature1)\n",
    "  # skipping feature 2 - it causes problems\n",
    "  feature['feature3'] = _float_feature(feature3)\n",
    "    \n",
    "#    {\n",
    "#      'feature0': _int64_feature(feature0),\n",
    "#      'feature1': _int64_feature(feature1),\n",
    "#      # 'feature2': _bytes_feature(feature2),   # take bytes feature out\n",
    "#      'feature3': _float_feature(feature3),\n",
    "#    }\n",
    "\n",
    "  # Create a Features message using tf.train.Example.\n",
    "\n",
    "  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "  return example_proto\n",
    "\n",
    "def serialize_example(example_proto):\n",
    "  return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example observation from the dataset.\n",
    "\n",
    "example_observation = []\n",
    "\n",
    "example = make_example(False, 4, b'goat', 0.9876)\n",
    "print (\"example protobuf\", type(example), '\\n', example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_serialized = serialize_example(example)\n",
    "print (\"example serialized\", type(example_serialized), '\\n', example_serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dict = {'serialized_example' : example_serialized}\n",
    "print (type(predict_dict), '\\n', predict_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode Example\n",
    "also called a \"message\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_deserialized = tf.train.Example.FromString(example_serialized)\n",
    "print (\"deserialized equivalent:\", example_deserialized == example)\n",
    "print (example_deserialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset from Slices (numpy arrays)\n",
    "you can create from a single numpy array  \n",
    "you can also create with a tuple of arrays\n",
    "\n",
    "here the Dataset is NOT tf.Example and it's not Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature1 - from above, numpy of int between 1 & 4\n",
    "print (\"feature1\", type(feature1), feature1)\n",
    "\n",
    "# create a Dataset from a numpy array\n",
    "# from_tensor_slices\n",
    "ds = tf.data.Dataset.from_tensor_slices(feature1)\n",
    "print (\"created Dataset from numpy array (slice):\",type(ds), '\\n', ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a ds with multiple features\n",
    "ds = tf.data.Dataset.from_tensor_slices((feature0, feature1, feature2, feature3))\n",
    "print (\"created Dataset from numpy array (slice):\",type(ds), '\\n', ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !! Graph Mode - SKIP THIS if you are in Eager Execution Mode  or TF 2.0 !!\n",
    "initialize the graph & run it  \n",
    "-- this won't work with Eager Enabled --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This won't work in Eager Execution\n",
    "# if you really want to run this, set it to\n",
    "# if True\n",
    "if False:\n",
    "    iter = ds.make_initializable_iterator()\n",
    "    e_tuple = iter.get_next()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(iter.initializer)  # initialize the graph\n",
    "        result = sess.run(e_tuple)  # run the graph - only once\n",
    "        print(result)\n",
    "        f0 = result[0]\n",
    "        f1 = result[1]\n",
    "        f2 = result[2]\n",
    "        f3 = result[3]\n",
    "        print(\"feature0:\", type(f0), f0)\n",
    "        print(\"feature1:\", type(f1), f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager Mode \n",
    "REMEMBER - this notebook will have issues w/ Eager.  But, this will work correctly - take(1) will run just once.   If you run this WITHOUT Eager, it will generate an infinite loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out because you should NOT be in Eager mode\n",
    "# - but this WILL work fine in Eager mode\n",
    "\n",
    "# take 1\n",
    "# as a tuple-4\n",
    "\n",
    "for t4 in ds.take(1):\n",
    "    print (type(t4))\n",
    "    f0,f1,f2,f3 = t4\n",
    "    print (f0)\n",
    "    print (f1)\n",
    "    print (f2)\n",
    "    print (f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Dataset.map - without using Example in this case\n",
    "you have to take the features and make a tensor.   Pretty clunky\n",
    "\n",
    "the tutorial says:  \n",
    "The mapped function must operate in TensorFlow graph mode—it must operate on and return tf.Tensors. A non-tensor function, like create_example, can be wrapped with tf.py_function to make it compatible.\n",
    "\n",
    "graph mode is the opposite of Eager Execution -- so don't enable Eager Execution.    That also means this stuff is probably going to change with TF 2.0 where Eager is the default. !?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need a function that will take the data in the Dataset and create an example - and serialize it\n",
    "# I changed the tutorial function so i need to recreate something similiar to make this work\n",
    "\n",
    "# thus ser_fx_example == tutorial's serialize_example \n",
    "def ser_fX_example(feature0,feature1,feature2, feature3):\n",
    "    \"\"\"\n",
    "    Create a tf.Example Message mapping the feature name to a tf.Example-compatible\n",
    "    \"\"\"\n",
    "    #create a dictionary - a copy of the GLOBAL definition\n",
    "    feature = FEATURE_DESCRIPTION.copy()\n",
    "    feature['feature0'] = _int64_feature(feature0)\n",
    "    feature['feature1'] = _int64_feature(feature1)\n",
    "    # 'feature2' : _bytes_feature(feature2), # take this feature out\n",
    "    feature['feature3'] = _float_feature(feature3)\n",
    "    \n",
    "    # create a f.train.Features()\n",
    "    features = tf.train.Features(feature=feature)\n",
    "    # create a tf.train.Example from a Features\n",
    "    example_proto = tf.train.Example(features=features)\n",
    "    # serialize the Example - byte array\n",
    "    example_serialized = example_proto.SerializeToString()\n",
    "    # this returns a byte string\n",
    "    return example_serialized\n",
    "\n",
    "def tf_serialize_example(f0,f1,f2,f3):\n",
    "    tf_string = tf.py_function(\n",
    "    ser_fX_example,   # the function\n",
    "    (f0,f1,f2,f3),    # the function's input\n",
    "    tf.string)        # the function's output\n",
    "    # return a Tensor\n",
    "    return tf.reshape(tf_string,())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test your functions\n",
    "ser_fX_example: features in -> bytes out  \n",
    "tf_serialize_example:  takes a function (ser_fX_example), passes the data in and outputs a Tensor of type bytes string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with f2 (the bytes variable) - This will <span style=\"color:red\">NOT WORK</span> if you you have Eager Execution enabled.  You'll get an error where it it has a tensor and it was expecting bytes - or something like that.   \n",
    "\n",
    "I never fully figured out the root problem.   On StackOverflow, other people had similar problem.   It's not central to what we're trying to do so I took f2 out of this example - and it works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('f0 type:', type(f0), f0.dtype)\n",
    "print ('f1 type:', type(f1), f1.dtype)\n",
    "# f2 must be type \n",
    "print ('f2 type:', type(f2), f2.dtype)\n",
    "\n",
    "# -- if Eager, you need this --\n",
    "f2n = f2.numpy()\n",
    "print ('f2.numpy() type:', type(f2n))\n",
    "\n",
    "print ('f3 type:', type(f3), f3.dtype)\n",
    "\n",
    "s = ser_fX_example(f0,f1,f2,f3)\n",
    "print (\"\\nser_fX_example:\", type(s), '\\n', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.reshape(input, ())\n",
    "this function will take a byte string and make it a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_string = b'Jay Duff'\n",
    "print (byte_string)\n",
    "print (tf.reshape(byte_string, ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function\n",
    "# - this will work with 1.14 Eager off\n",
    "# - Won't work with 1.14 Eager on\n",
    "# - it will probably work with 2.0 Eager On (which is the default)\n",
    "\n",
    "# commented out since you should have Eager on\n",
    "# serialized_feature_example = tf_serialize_example(f0,f1,f2,f3)\n",
    "# print (type(serialized_feature_example))\n",
    "# print (serialized_feature_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_features_dataset = ds.map(tf_serialize_example)\n",
    "print (serialized_features_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataset from a Generator\n",
    "This is an alternative to creating a Dataset from slices\n",
    "\n",
    "### Generator\n",
    "Using a generator means the sequence is NOT stored in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "    for features in ds:\n",
    "        yield ser_fX_example(*features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_features_dataset2 = tf.data.Dataset.from_generator(generator, output_types=tf.string, output_shapes=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (serialized_features_dataset2)\n",
    "print (serialized_features_dataset == serialized_features_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with TF Records - Part 1 - tf.data.experimental \n",
    "\n",
    "#### <span style=\"color:red\">CONCEPTS ONLY - tf.data.experimental is NOT the way to do this in production.   See TFRecords Part 2 with tf.io</span>  \n",
    "Not sure why the tutorial goes through tf.data.experimental ???  \n",
    "Below we repeat the same exercises with tf.io  \n",
    "### Write TF Record File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can write a Dataset of serialized examples to a TFRecord file\n",
    "tfrecord_file_output = os.path.join(TMP_DIR, 'tutorial.tfrecord')\n",
    "print (tfrecord_file_output)\n",
    "\n",
    "# create the writer \n",
    "writer = tf.data.experimental.TFRecordWriter(tfrecord_file_output)\n",
    "writer.write(serialized_features_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read TF Record File\n",
    "We have 2 files - we just put this tutorial TFRecord file in /tmp -- because we don't care about it.  \n",
    "The ssd-project (w/ CFA Product data) has train/val/test tfrecord files in /data/tfrecord directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial tfrecord file (from above)\n",
    "tfrecord_file_exists = tf.io.gfile.exists(tfrecord_file_output)\n",
    "print (\"tfrecords file path - exists:\", tfrecord_file_exists, tfrecord_file_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CFA Product Images - for SSD\n",
    "# this will be a different tf.Example structure than this tutorial\n",
    "tfrecord_file_exists = tf.io.gfile.exists(TEST_TFRECORDS_PATH)\n",
    "print (\"tfrecords file path - exists:\", tfrecord_file_exists, TEST_TFRECORDS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Dataset / tfrecord\n",
    "This notebook will continue with the simple tf.Example where Feature = {feature0, feature1, feature3}.  Remember, we dropped feature2.\n",
    "\n",
    "Feature is based on whatever format YOU elect.   It's NOT a self-describing format (not good!)  So it's important YOU keep track of the format throughout these pipelines.   in parallel, there is a notebook:  \n",
    "#### UnderstandingObjectDetectionExample\n",
    "This notebook will have equivalent operations with the SSD tf.Example (which stores an image).  The tf.Example in this notebook is based on the original MobileNet model - and of course it's different than this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get length of dataset\n",
    "# - at this time, there is no simple function\n",
    "#   this is brute force - ii just want this for QA reasons\n",
    "\n",
    "def get_dataset_length(ds):\n",
    "    num_elements = 0\n",
    "    for element in ds:\n",
    "        num_elements += 1\n",
    "    return num_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutorial tfrecord file\n",
    "# - note - this is a list of files, you should shard your files if there are more than 5,000\n",
    "tfrecord_filenames = [tfrecord_file_output]\n",
    "raw_dataset = tf.data.TFRecordDataset(tfrecord_filenames)\n",
    "print (\"tfrecord file:\", type(raw_dataset), '\\n', raw_dataset)\n",
    "\n",
    "# cache the dataset in memory\n",
    "raw_dataset.cache()\n",
    "print (\"dataset size:\", get_dataset_length(raw_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Serialized Data\n",
    "\n",
    "#### TF 1.14 vs 2.0\n",
    "- 1.14:  single record is DatasetV1Adapter\n",
    "- 2.0 :  it is a TakeDataset\n",
    "\n",
    "You can't take a single record in either version and do:\n",
    "single_record.numpy()  \n",
    "\n",
    "But you can iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_record = raw_dataset.take(1)\n",
    "print (\"record from file:\", type(single_record), '\\n', single_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iterating through the dataset\n",
    "\n",
    "#### TF 1.14 & TF 2.0\n",
    "EagerTensor - and you can get the contents w/ .numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_record in raw_dataset.take(3):\n",
    "    print (type(raw_record))\n",
    "    print (repr(raw_record))\n",
    "    print (\"contents:\", raw_record.numpy())\n",
    "    # the contents are a serialized example?\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing\n",
    "you have EagerTensors, you have to parse the serialized data (deserialize) back to a feature.  So its important you use the SAME feature definition as when you created the dataset (and wrote it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(FEATURE_DESCRIPTION), 'n', FEATURE_DESCRIPTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "    # Parse the input using the standard dictionary\n",
    "    feature = tf.io.parse_single_example(example_proto, FEATURE_DESCRIPTION)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dataset = raw_dataset.map(_parse_function)\n",
    "print (parsed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now you see your dataset is features\n",
    "for parsed_record in parsed_dataset.take(3):\n",
    "    print(repr(parsed_record))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Records - Part 2 - tf.io\n",
    "This should be the preferred way.   It's safe to say tf.data.experimental is not long term!!  Not sure why the tutorial included experimental.  (gee, that was a waste of time)\n",
    "\n",
    "### Recreate a TFRecord file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new tfrecord file\n",
    "tfrecord_file_output = os.path.join(TMP_DIR, 'tutorial_part2.tfrecord')\n",
    "print (tfrecord_file_output)\n",
    "\n",
    "with tf.io.TFRecordWriter(tfrecord_file_output) as writer:\n",
    "        for i in range(n_observations):\n",
    "            example = ser_fX_example(feature0[i], feature1[i], feature2[i], feature3[i])\n",
    "            writer.write(example)\n",
    "            \n",
    "! ls -l {tfrecord_file_output}\n",
    "! du -sh {tfrecord_file_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the TFRecord File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_file_list_input = [tfrecord_file_output]\n",
    "raw_dataset = tf.data.TFRecordDataset(tfrecord_file_list_input)\n",
    "print (type(raw_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_record in raw_dataset.take(1):\n",
    "    print(\"raw record type:\", type(raw_record))  # serialized Example\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())  # Parse will de-serialize it\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFServing (and SageMaker Endpoint)\n",
    "SageMaker endpoint is based on TensorFlow Serving.  \n",
    "\n",
    "Assuming you have an active endpoint - in this case a mobilenet-ssd model - CLEARLY NOT the model for this bogus data structure.  The desire here is simply to test the data format & serialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "from sagemaker.predictor import json_serializer, json_deserializer\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "predictor=sagemaker.tensorflow.model.TensorFlowPredictor(ENDPOINT_NAME, sagemaker_session)\n",
    "\n",
    "print (type(predictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "for raw_record in raw_dataset.take(3):\n",
    "    print(\"raw record type:\", type(raw_record))  # serialized Example\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    print (example)\n",
    "    #\n",
    "    predict_request = {'serialized_example' : repr(example)}\n",
    "    predict_string = json.dumps(predict_request)\n",
    "    predict_json = json.loads(predict_string)\n",
    "    output_dict = predictor.predict(predict_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
