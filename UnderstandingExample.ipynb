{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding tf.Example w/ TF 2.0 \n",
    "\n",
    "First, understand this is the world's worst named class - not an example - it is an Example of data (I guess).   Here I'm using TF 2.0beta - because the Understanding Images required it.    The on-line  tutorial shows you can use tf 1.14\n",
    "\n",
    "https://www.tensorflow.org/beta/tutorials/load_data/images  \n",
    "https://www.tensorflow.org/tutorials/load_data/tf_records\n",
    "\n",
    "You will need these skills!  What is an image, tf.Example, serialized example etc.  \n",
    "You won't get far with served models without this understanding.\n",
    "\n",
    "## TensorFlow 2.0 Beta\n",
    "\n",
    "### Do this THIRD, first, do UnderstandingTF_IO & UnderstandingImages.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib\n",
    "import random\n",
    "import IPython.display as display\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# !pip install -q tensorflow==2.0.0-beta1\n",
    "# you'll get a tensorflow-serving-api error\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = os.getcwd()\n",
    "\n",
    "IMAGE_DIR = os.path.join(PROJECT_DIR, \"data/jpeg_images\")\n",
    "ANNOTATION_DIR = os.path.join(PROJECT_DIR, \"data/annotations\")\n",
    "\n",
    "MODEL_PATH = os.path.join(PROJECT_DIR, \"trained_model/export/Servo/1564778509\")\n",
    "LABEL_MAP = os.path.join(PROJECT_DIR, \"code/cfa_prod_label_map.pbtxt\")\n",
    "\n",
    "# you can get data using the TrainModel_Step1_Local notebook\n",
    "TEST_TFRECORDS_PATH =  os.path.join(PROJECT_DIR, \"code/tfrecords/test/\")\n",
    "                                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_glob = os.path.join(IMAGE_DIR, '*.jpg')\n",
    "all_image_paths = tf.io.gfile.glob(image_glob)\n",
    "\n",
    "# inplace shuffle\n",
    "random.shuffle(all_image_paths)\n",
    "all_image_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for data type conversions\n",
    "\n",
    "### Note: \n",
    "To stay simple, this example only uses scalar inputs. The simplest way to handle non-scalar features is to use tf.serialize_tensor to convert tensors to binary-strings. Strings are scalars in tensorflow. Use tf.parse_tensor to convert the binary-string back to a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_string = b'test_string'\n",
    "ft_byte_string = _bytes_feature(byte_string)\n",
    "print(\"Feature - bytes:\", type(ft_byte_string))\n",
    "print(\"   value:\", ft_byte_string)\n",
    "\n",
    "# unencoded bytes - encoded to utf-8\n",
    "print(_bytes_feature(u'test_bytes'.encode('utf-8')))\n",
    "\n",
    "print(_float_feature(np.exp(1)))\n",
    "print(_int64_feature(True))\n",
    "print(_int64_feature(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Scalar Example - an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = tf.io.read_file(all_image_paths[3])\n",
    "print (\"img tensor:\", type(img_tensor), img_tensor.dtype)\n",
    "\n",
    "# serialize the tensor\n",
    "ser_img_tensor =tf.io.serialize_tensor(img_tensor)\n",
    "print (\"serialized:\", type(ser_img_tensor), ser_img_tensor.dtype)\n",
    "\n",
    "print (img_tensor == ser_img_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tf.Example message\n",
    "\n",
    "#### Make up some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of observations in the dataset\n",
    "n_observations = int(1e4)\n",
    "\n",
    "# boolean feature, encoded as False or True\n",
    "feature0 = np.random.choice([False, True], n_observations)\n",
    "print (type(feature0), feature0.shape, feature0[:5])\n",
    "\n",
    "# integer feature, random from 0 .. 4\n",
    "feature1 = np.random.randint(0, 5, n_observations)\n",
    "print (type(feature1), feature1.shape, feature1[:5])\n",
    "\n",
    "# string feature - note: string as a byte array\n",
    "strings = np.array([b'cat', b'dog', b'chicken', b'horse', b'goat'])\n",
    "feature2 = strings[feature1]\n",
    "print (type(feature2), feature2.shape, feature2[:5])\n",
    "\n",
    "# float feature, from a standard normal distribution\n",
    "feature3 = np.random.randn(n_observations)\n",
    "print (type(feature3), feature3.shape, feature3[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example(feature0, feature1, feature2, feature3):\n",
    "  \"\"\"\n",
    "  Creates a tf.Example message ready to be written to a file.\n",
    "  \"\"\"\n",
    "\n",
    "  # Create a dictionary mapping the feature name to the tf.Example-compatible\n",
    "  # data type.\n",
    "\n",
    "  feature = {\n",
    "      'feature0': _int64_feature(feature0),\n",
    "      'feature1': _int64_feature(feature1),\n",
    "      'feature2': _bytes_feature(feature2),\n",
    "      'feature3': _float_feature(feature3),\n",
    "  }\n",
    "\n",
    "  # Create a Features message using tf.train.Example.\n",
    "\n",
    "  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "  return example_proto\n",
    "\n",
    "def serialize_example(example_proto):\n",
    "  return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example observation from the dataset.\n",
    "\n",
    "example_observation = []\n",
    "\n",
    "example = make_example(False, 4, b'goat', 0.9876)\n",
    "print (\"example protobuf\", type(example), '\\n', example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_serialized = serialize_example(example)\n",
    "print (\"example serialized\", type(example_serialized), '\\n', example_serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dict = {'serialized_example' : example_serialized}\n",
    "print (type(predict_dict), '\\n', predict_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# research/utils/dataset_util/recurive_parse_xml_to_dict(xml)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
