{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect (with the) Model\n",
    "## Step 1 - run the TENSORFLOW model locally\n",
    "\n",
    "This example uses TensorFlow FROZEN_INFERENCE_GRAPH.pb. (not a saved_model.pb)  Which begs the question, what is the difference between a saved_model.pb and a frozen_inference_graph.pb?  Well, funny you should ask:  \n",
    "https://stackoverflow.com/questions/52934795/what-is-difference-frozen-inference-graph-pb-and-saved-model-pb\n",
    "\n",
    "In this notebook, we are replicating  some of the original MobileNet TensorFlow functionality.  \n",
    "ref: https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\n",
    "\n",
    "(Don't confuse with TensorFlow Lite - for tflite - we have a detect.py program to make this easy and tflite is easier by nature.)\n",
    "\n",
    "We are doing this because I don't understand how to get data into the TensorFlow model - and we need to fully understand that before interacting with a SageMaker HOSTED endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from distutils.version import StrictVersion\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# This is needed since we cloned tensorflow/models under code.\n",
    "cwd = os.getcwd()\n",
    "models = os.path.join(cwd, 'code/models/research/')\n",
    "slim = os.path.join(cwd, 'code/models/research/slim')\n",
    "sys.path.append(models)\n",
    "sys.path.append(slim)\n",
    "\n",
    "from object_detection.utils import ops as utils_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Environment Variables\n",
    "\n",
    "We are using objects and scripts in the project as much as possible.   They require environment variables to pass along where stuff is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = os.getcwd()\n",
    "IMAGE_DIR = os.path.join(PROJECT_DIR, \"data/new_jpeg_images\")\n",
    "MODEL_PATH = os.path.join(PROJECT_DIR, \"tensorflow_model/frozen_inference_graph.pb\")\n",
    "LABEL_MAP = os.path.join(PROJECT_DIR, \"code/cfa_prod_label_map.pbtxt\")\n",
    "ANNOTATION_DIR = os.path.join(PROJECT_DIR, \"data/unverified_annotations\")\n",
    "\n",
    "S3_TEST_IMAGES = \"s3://cfaanalyticsresearch-sagemaker/datasets/cfa_products/test_images/\"\n",
    "SAMPLE_IMAGE = \"/home/ec2-user/SageMaker/ssd-dag/data/new_jpeg_images/20190710_variety_1562781002.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "\n",
    "- The training job generated a checkpoint file.  \n",
    "- Then you converted the checkpoint\n",
    "  - to a frozen_inference_graph.pb\n",
    "  - to a saved_graph.pb  \n",
    "  BOTH using TensorFlow Utilities - BUT - they aren't the same format\n",
    "  \n",
    "#### Here you need a frozen INFERENCE Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  print (type(od_graph_def))\n",
    "  print (\"model:\", MODEL_PATH)\n",
    "  with tf.gfile.GFile(MODEL_PATH, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')\n",
    "    \n",
    "print (\"detection graph:\", type(detection_graph))\n",
    "print (detection_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Label Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(LABEL_MAP, use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreive Data\n",
    "Copy the test images locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp {S3_TEST_IMAGES} {IMAGE_DIR} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "# load an image and resturn a numpy array\n",
    "\n",
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(IMAGE_DIR)\n",
    "image_list = list()\n",
    "for f in dir_list:\n",
    "    full_path = os.path.join(IMAGE_DIR, f)\n",
    "    if os.path.isfile(full_path) and os.path.splitext(f)[1].lower() == '.jpg':\n",
    "        image_list.append(full_path)\n",
    "\n",
    "# limitations with the way we are displaying\n",
    "image_list = image_list[:1]\n",
    "print (\"Image Count:\", len(image_list))\n",
    "print (\"Sample:\",image_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (6, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Model\n",
    "\n",
    "TensorFlow provides the framework for the model - but you really don't know how the model was programmed for input & output.    You could dissect the MobileNet source code but the easier method in this case is to examine a sample (that came from tensorflow/models/object_detection)\n",
    "\n",
    "#### How do you pass data into the Model?\n",
    "The model has 3400+ operations and a lot of tensors.  \n",
    "Image Tensor: Tensor(\"image_tensor:0\", shape=(?, ?, ?, 3), dtype=uint8)  \n",
    "\n",
    "##### Input Tensor\n",
    "\n",
    "##### Output Tensors\n",
    "We can are looking for 4 tensors (and they are float32)  \n",
    "What is a detection mask?  \n",
    "\n",
    "#### How do you process the inference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME - get this from the console\n",
    "ENDPOINT_NAME = \"ep-mobilenet-ssd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "predictor=sagemaker.tensorflow.model.TensorFlowPredictor(ENDPOINT_NAME, sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get attributes from the SAVED GRAPH  \n",
    "# - not the frozen graph.pb\n",
    "def run_inference_for_single_image(image, graph):\n",
    "  with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.get_default_graph().get_operations()\n",
    "      # print (\"ALL model operations:\", type(ops), len(ops), ops)\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      print (\"tensor names:\", type(all_tensor_names), len(all_tensor_names))\n",
    "      tensor_dict = {}\n",
    "      for key in [\n",
    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
    "          'detection_classes', 'detection_masks'\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_value = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "          print (tensor_name, tensor_value)\n",
    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "      if 'detection_masks' in tensor_dict:\n",
    "        print (\"*** detection mask in the tensor dict ***\")\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[1], image.shape[2])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "      print (\"image_tensor:\", image_tensor)\n",
    "        \n",
    "      # Run inference\n",
    "      print (\" --- run the model ---\")\n",
    "      print (\"tensor dict:\", tensor_dict)\n",
    "      print (\"image:\", image.shape)\n",
    "      output_dict = sess.run(tensor_dict,feed_dict={image_tensor: image})\n",
    "      # output_dict = predictor.predict({'image_tensor': image})\n",
    "\n",
    "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "      output_dict['detection_classes'] = output_dict[\n",
    "          'detection_classes'][0].astype(np.int64)\n",
    "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "      if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting Note\n",
    "#### Could Not initialize convolution ...\n",
    "This is super odd.   No idea what causes this.  And, the fix is to stop the SageMaker notebook and restart it.   Like it's probably a GPU problem.  Rebooting (not restarting the notebook) fixes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in image_list:\n",
    "  image = Image.open(image_path)\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = load_image_into_numpy_array(image)\n",
    "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "  # Actual detection.\n",
    "  output_dict = run_inference_for_single_image(image_np_expanded, detection_graph)\n",
    "  # Visualization of the results of a detection.\n",
    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks'),\n",
    "      use_normalized_coordinates=True,\n",
    "      line_thickness=8)\n",
    "  plt.figure(figsize=IMAGE_SIZE)\n",
    "  plt.imshow(image_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "If you want the annotations - you'll find the annotations tarball in data/ directory\n",
    "use the Notebook browser to download it\n",
    "\n",
    "The fastest, easiest way to review (and correct / verify) is to use labelImg program which will merge the image and annotation\n",
    "\n",
    "The main conclusion here is our model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (detection_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(SAMPLE_IMAGE)\n",
    "image_np = load_image_into_numpy_array(image)\n",
    "# Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "print (image_np_expanded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image_np_expanded\n",
    "print (\"image shape:\", image.shape)\n",
    "\n",
    "with detection_graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.get_default_graph().get_operations()\n",
    "      # print (\"ALL model operations:\", type(ops), len(ops), ops)\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      print (\"tensor names:\", type(all_tensor_names), len(all_tensor_names))\n",
    "      tensor_dict = {}\n",
    "      for key in [\n",
    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
    "          'detection_classes', 'detection_masks'\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_value = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "          print (tensor_name, tensor_value)\n",
    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "      if 'detection_masks' in tensor_dict:\n",
    "        print (\"*** detection mask in the tensor dict ***\")\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[1], image.shape[2])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "      print (\"image_tensor:\", image_tensor)\n",
    "        \n",
    "      # Run inference\n",
    "      print (\"tensor dict:\", tensor_dict)\n",
    "        \n",
    "      # input into the model must be JSON\n",
    "      # output_dict = sess.run(tensor_dict,feed_dict={image_tensor: image})\n",
    "      # output_dict = predictor.predict(tensor_dict,feed_dict={image_tensor: image})  # doesn't like feed_dict\n",
    "      # output_dict = predictor.predict(tensor_dict, {image_tensor: image})\n",
    "      # output_dict = predictor.predict(tensor_dict, {\"image_tensor\": image_tensor})\n",
    "      output_dict = predictor.predict(tensor_dict, {\"image_tensor\": image})\n",
    "      # ops includes:\n",
    "      # <tf.Operation 'image_tensor' type=Placeholder>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
