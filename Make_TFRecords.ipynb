{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make TFRecords from VOC XML & jpgs\n",
    "\n",
    "THIS IS REDUNDANTE with UnderstandingObjectDetectionExample  \n",
    "Use the other notebook for a full understanding\n",
    "\n",
    "## Prerequitistes\n",
    "### Input\n",
    "1. you have jpeg images\n",
    "2. you have annotations - XML format, VOC Pascal format standard\n",
    "\n",
    "### Output\n",
    "tfrecords_dir needs to have 3 subdirectories\n",
    "/train\n",
    "/val\n",
    "/test\n",
    "\n",
    "this code leverages the standards and templates as much as possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import IPython.display as display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as patches\n",
    "# This is needed since we cloned tensorflow/models under code.\n",
    "# - if you don't know what this means\n",
    "#   Look at the notebook TrainModel_Step1_Local\n",
    "#      in this notebook, you basically set up the project with includes cloning \n",
    "#      and compiling the tensorflow/models repo\n",
    "#   we are using the utilities found in that repo\n",
    "\n",
    "cwd = os.getcwd()\n",
    "models = os.path.join(cwd, 'code/models/research/')\n",
    "slim = os.path.join(cwd, 'code/models/research/slim')\n",
    "sys.path.append(models)\n",
    "sys.path.append(slim)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils.visualization_utils import STANDARD_COLORS\n",
    "from object_detection.utils.visualization_utils import draw_bounding_box_on_image\n",
    "\n",
    "# this is the standard feature dict\n",
    "from code.cfa_utils.example_utils import feature_obj_detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code.cfa_utils.example_utils import voc_to_tfrecord_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this won't work with Tensorflow 2.0\n",
    "# if you have TF 2.0 loaded, you can't set eager - it's forced on\n",
    "\n",
    "print ('TensorFlow Version:', tf.__version__)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "# project directories\n",
    "PROJECT = os.getcwd()\n",
    "CODE = os.path.join(PROJECT, \"code\")\n",
    "DATA = os.path.join(PROJECT, \"data\")\n",
    "\n",
    "IMAGE_DIR = os.path.join(DATA, \"jpeg_images\")\n",
    "ANNOTATION_DIR = os.path.join(DATA, \"annotations\")\n",
    "LABEL_MAP_FILE = os.path.join(CODE, 'cfa_prod_label_map.pbtxt')\n",
    "TFRECORD_DIR = os.path.join(PROJECT, 'tmp')\n",
    "TRAINING_SPLIT_TUPLE =  (60,30,10)\n",
    "INCLUDE_CLASSES = 'all'\n",
    "EXCLUDE_TRUNCATED = False,\n",
    "EXCLUDE_DIFFICULT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Labels\n",
    "\n",
    "if you get an error like:  \n",
    "!!! label map error: 20190625_polySauce_spicyBag_1561494037 smallSauce  skipped\n",
    "\n",
    "This is telling you that the image_id:  20190625_polySauce_spicyBag_1561494037  \n",
    "has a class label:  smallSauce  \n",
    "which is not defined in the label map.  (don't be fooled!  'smallSauce' is the problem, not polySauce in the filename)\n",
    "\n",
    "If there are a few - you could ignore it.   To fix the data locally:\n",
    "1. review the label_map - $ cat code/cfa_prod_label_map.pbtxt;   youll see 7 == cfaSauce, 10 == polySauce, \n",
    "2. you need to change any 'smallSauce' to one of the labels in the label_map; we will choose cfaSauce \n",
    "3. add a sed command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to fix the labels\n",
    "# it's in the other notebook:  UnderstaningObjectDetectionExample\n",
    "\n",
    "# this is the label_map as a dict\n",
    "#    {'smallHotDrink': 2, 'nuggBox': 5, 'sandBag': 6, 'smallFry': 8, \n",
    "#     'largeFry': 9, 'cfaSauce': 7, 'mediumColdDrink': 3, 'sandBox': 4, \n",
    "#   'hand': 1, 'spicyBag': 11, 'polySauce': 10}\n",
    "\n",
    "# you might have to replace some names due to inconsistencies in labeling and the map\n",
    "\n",
    "! sed -i 's/smHotDrink/smallHotDrink/g' data/annotations/*.xml\n",
    "! sed -i 's/medColdDrink/mediumColdDrink/g' data/annotations/*.xml\n",
    "! sed -i 's/smallSauce/cfaSauce/g' data/annotations/*.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## voc_to_tfrecord_file()\n",
    "This program is in code/cfa_utils/example_utils.py    (Reminder - this isn't an example, it is tf.Example - ugh!) \n",
    "\n",
    "This progrm leverages as much of the standard TensorFlow code as possible.  That means:\n",
    "- annotations are based on VOC PASCAL data standards.   There are hundreds of program examples that use this data which is an XML annotation format.\n",
    "- the tf.Example(Feature) format is based on the format used in the MobileNet model.    I lifted it out of the /models code and placed it here where you can import it.  It is important that you have a consistent format through tfrecord generation, training, predictoin.   \n",
    "\n",
    "I used a pattern where I imported the SSD Feature dictionary then copied it to a dict - then used that dict in the serialization.   You'll see that in the program.   The point is, the feature (dict) format is defined only once in one place.    (Look at the code.)  Odd side effect:   It seems that you must define every element of the dict.  If you don't, you'll get an error:  \n",
    "--> 214             features = tf.train.Features(feature=feature)\n",
    "    215 \n",
    "    216             tf_example = tf.train.Example(features=features)\n",
    "\n",
    "TypeError: MergeFrom() takes exactly one argument (3 given)\n",
    "\n",
    "This program will tell you if it skips image/annotations due to bad labels.  (explained above).\n",
    "\n",
    "### Result:\n",
    "\n",
    "This is telling you that 3149 had a 'verified' (XML attribute) annotation and 22 were not verified.   That is normal.  WHen you label (using labelImg for example), you can skip a questionable training image by simply not verifying it.\n",
    "\n",
    "This dict also shows label map, e.g. hand == class_id = 1\n",
    "\n",
    "  verified: 3149   not: 22\n",
    "{'hand': 1, 'smallHotDrink': 2, 'mediumColdDrink': 3, 'sandBox': 4, 'nuggBox': 5, 'sandBag': 6, 'cfaSauce': 7, 'smallFry': 8, 'largeFry': 9, 'polySauce': 10, 'spicyBag': 11}\n",
    "\n",
    "This is telling you 1889 images were written to the train.tfrecord file.  (not sharded)  \n",
    "169 objects were class_id = 6 (sandBag)  \n",
    "568 objects were class_id = 4 (nuggBox)  \n",
    "These totals will sum >= 1889 because there may be multiple objects per image.\n",
    "\n",
    " -- images 1889  writing to: /home/ec2-user/SageMaker/ssd-dag/tmp/train/train.tfrecord\n",
    "     image count: 1889   class_count: {6: 169, 9: 286, 5: 563, 11: 178, 2: 441, 4: 568, 8: 291, 1: 927, 3: 572, 10: 412, 7: 157}\n",
    "     \n",
    "### file output\n",
    "NOTE - these files were written (depending on your GLOBAL value) to /tmp.    Write to tmp, then promote to S3 if you want to use these.    Look at the training program (notebook) to see where it pulls tfrecords (hint: it won't be /tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_to_tfrecord_file(IMAGE_DIR,\n",
    "                    ANNOTATION_DIR,\n",
    "                    LABEL_MAP_FILE,\n",
    "                    TFRECORD_DIR,\n",
    "                    TRAINING_SPLIT_TUPLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Test your TFRecords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFRECORD_DIR = '/home/ec2-user/SageMaker/ssd-dag/data/tfrecords'\n",
    "# TFRECORD_DIR = '/home/ec2-user/SageMaker/ssd-dag/tmp'\n",
    "print (TFRECORD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will read a list of files\n",
    "# let's combine train, val & test\n",
    "\n",
    "\n",
    "tfrecord_file_list_input = [os.path.join(TFRECORD_DIR, 'train/train.tfrecord'),\n",
    "                            os.path.join(TFRECORD_DIR, 'val/val.tfrecord'),\n",
    "                            os.path.join(TFRECORD_DIR, 'test/test.tfrecord')]\n",
    "print (\"reading:\", tfrecord_file_list_input)\n",
    "raw_dataset = tf.data.TFRecordDataset(tfrecord_file_list_input)\n",
    "raw_dataset.cache()  # cache to memory\n",
    "raw_dataset.shuffle(buffer_size=5000)\n",
    "print (type(raw_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the TFRecords\n",
    "- The file is read into a dataset (TFRecordDatasetV1 to be exact)\n",
    "- Iterating through the records:\n",
    "  - each record is an EagerTensor (you must have Eager Execution enabled)\n",
    "  - This tensor has a serialized tf.Example\n",
    "      - byte string\n",
    "      - get the value (byte string) with .numpy()\n",
    "  - parse the serialized byte string into an Example\n",
    "      - tf.Example is made of Features\n",
    "          - feature[key] == each part of the observation or data point\n",
    "          \n",
    "So, make sure this is correct.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this iteration will show you:\n",
    "# - each record\n",
    "# - each tf.Example\n",
    "# BUT - it is not a parsed tf.Example,  it looks readable, but it's not yet consumable\n",
    "#   look at the next code block for that\n",
    "\n",
    "# VERIFY your mapping using this loop\n",
    "\n",
    "for raw_record in raw_dataset.take(1):\n",
    "    print(\"raw record type:\", type(raw_record))  # serialized Example\n",
    "    print(\"Tensor.dtype:\", raw_record.dtype)\n",
    "    print(\"       value:\", raw_record.numpy()[:50], '\\n')\n",
    "    \n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())  # Parse will de-serialize it\n",
    "    # review this to verify the features were mapped correctly\n",
    "    print(type(example), '\\n', example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing each tf.Example record\n",
    "\n",
    "This parses the serialized tf.Example into the feature (dict).   This is where we use that common feature definition to make sure the format is good.   feature_obj_detect is imported from:\n",
    "code.cfa_utils.example_utils.py  \n",
    "\n",
    "This isn't something I defined - I lifted it out of the code in tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"feature_obj_detect:\", type(feature_obj_detect), '\\n', feature_obj_detect)\n",
    "def _parse_function(example_proto):\n",
    "    # Parse the input using the standard dictionary\n",
    "    feature = tf.io.parse_single_example(example_proto, feature_obj_detect)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dataset = raw_dataset.map(_parse_function)\n",
    "print (parsed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ref:  UnderstandingTF_IO notebook\n",
    "this will show you some of the tf.io utilities that we are using here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using matplotlib\n",
    "\n",
    "mcolors_unique = ['b','g','r','c','m','y','k']  # built in colors\n",
    "mcolors = mcolors_unique + mcolors_unique       # 2x tso we have enough\n",
    "\n",
    "for n,i in enumerate(parsed_dataset.take(3)):\n",
    "    print (\"record type:\", type(i))\n",
    "    print (\"image/encoded type:\", type(i['image/encoded']))\n",
    "    image_tensor = i['image/encoded'].numpy()  # bytes\n",
    "    print (\"image/encoded EagerTensor.numpy():\", type(image_tensor))\n",
    "    print(\"is jpeg:\", tf.io.is_jpeg(image_tensor))\n",
    "    \n",
    "    jpeg_decoded_tensor = tf.image.decode_jpeg(image_tensor)\n",
    "    jpeg_numpy = jpeg_decoded_tensor.numpy()\n",
    "    print (\"tf.image.decode_jpeg(image_tensor):\", jpeg_numpy.shape)\n",
    "    \n",
    "    # get height/width\n",
    "    height = i['image/height'].numpy()\n",
    "    width =  i['image/width'].numpy()\n",
    "    \n",
    "    # get object classes\n",
    "    obj_class_names = i['image/object/class/text'].values.numpy()\n",
    "    obj_class_ids = i['image/object/class/label'].values.numpy()\n",
    "    obj_count = len(obj_class_ids)\n",
    "    \n",
    "    print (obj_class_names)\n",
    "    # get the bounding box coordinates\n",
    "    xmins = i['image/object/bbox/xmin'].values.numpy()\n",
    "    xmaxs = i['image/object/bbox/xmax'].values.numpy()\n",
    "    ymins = i['image/object/bbox/ymin'].values.numpy()\n",
    "    ymaxs = i['image/object/bbox/ymax'].values.numpy()\n",
    "    print ('xmins:', type(xmins), xmins)\n",
    "    xmins_pixel = xmins * width\n",
    "    xmaxs_pixel = xmaxs * width\n",
    "    ymins_pixel = ymins * height\n",
    "    ymaxs_pixel = ymaxs * height\n",
    "    print (xmins_pixel)\n",
    "    \n",
    "    # display with \n",
    "    #plt.figure(figsize=(16,16))\n",
    "    #plt.subplot(2,2,n+1)\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(jpeg_numpy)\n",
    "    \n",
    "    for idx in range(obj_count):\n",
    "        rect_height = ymaxs_pixel[idx] - ymins_pixel[idx]\n",
    "        rect_width =  xmaxs_pixel[idx] - xmins_pixel[idx]\n",
    "        rect = patches.Rectangle((xmins_pixel[idx],ymins_pixel[idx]),rect_width,rect_height,\n",
    "                                 linewidth=1,edgecolor=mcolors[obj_class_ids[idx]],facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    #plt.imshow(jpeg_numpy)\n",
    "    #currentAxis = plt.gca()\n",
    "    #currentAxis.add_patch(Rectangle((100 - 50, 100 - 50), 0.2, 0.2,\n",
    "    #                  alpha=1, facecolor='none'))\n",
    "    #plt.grid(False)\n",
    "    #plt.xticks([])\n",
    "    #plt.yticks([])\n",
    "    plt.show()\n",
    "    \n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow/models/object_detection/visualization_util.py\n",
    "\n",
    "THIS is the way to display - much easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow/models/object_detection\n",
    "\n",
    "for n,i in enumerate(parsed_dataset.take(3)):\n",
    "    print (\"record type:\", type(i))\n",
    "    print (\"image/encoded type:\", type(i['image/encoded']))\n",
    "    image_tensor = i['image/encoded'].numpy()  # bytes\n",
    "    print (\"image/encoded EagerTensor.numpy():\", type(image_tensor))\n",
    "    print(\"is jpeg:\", tf.io.is_jpeg(image_tensor))\n",
    "    \n",
    "    jpeg_decoded_tensor = tf.image.decode_jpeg(image_tensor)\n",
    "    jpeg_numpy = jpeg_decoded_tensor.numpy()\n",
    "    print (\"tf.image.decode_jpeg(image_tensor):\", jpeg_numpy.shape)\n",
    "    \n",
    "    # get height/width\n",
    "    height = i['image/height'].numpy()\n",
    "    width =  i['image/width'].numpy()\n",
    "    \n",
    "    # get object classes\n",
    "    obj_class_names = i['image/object/class/text'].values.numpy()\n",
    "    obj_class_ids = i['image/object/class/label'].values.numpy()\n",
    "    obj_count = len(obj_class_ids)\n",
    "    \n",
    "    print (obj_class_names)\n",
    "    # get the bounding box coordinates\n",
    "    xmins = i['image/object/bbox/xmin'].values.numpy()\n",
    "    xmaxs = i['image/object/bbox/xmax'].values.numpy()\n",
    "    ymins = i['image/object/bbox/ymin'].values.numpy()\n",
    "    ymaxs = i['image/object/bbox/ymax'].values.numpy()\n",
    "    print ('xmins:', type(xmins), xmins)\n",
    "    xmins_pixel = xmins * width\n",
    "    xmaxs_pixel = xmaxs * width\n",
    "    ymins_pixel = ymins * height\n",
    "    ymaxs_pixel = ymaxs * height\n",
    "   \n",
    "    pil_image = Image.fromarray(jpeg_numpy)    \n",
    "    for idx in range(obj_count):\n",
    "        draw_bounding_box_on_image(pil_image,ymins[idx],xmins[idx], ymaxs[idx], xmaxs[idx],\n",
    "                                  color=STANDARD_COLORS[obj_class_ids[idx]], \n",
    "                                  thickness=4, display_str_list=[obj_class_names[idx]],\n",
    "                                  use_normalized_coordinates=True)\n",
    "        \n",
    "    display.display(pil_image)\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
