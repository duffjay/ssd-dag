{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding TensorFlow Object Detection Configuration\n",
    "\n",
    "The MobileNet model pipeline (and assuming all models) relies upon a protobuf (.pbtxt) file.   This notebook simply illustrates how to use the utilities in TensorFlow to consume this.\n",
    "\n",
    "Why is that important?  I found 90% of my errors were configuration issues - file not found.  Which leads to \"well, what is it looking for?\" and \"from what relative path\".   Use the TF Utilities as much as possible - you'll find they are always one step ahead of you!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed since we cloned tensorflow/models under code.\n",
    "# - if you don't know what this means\n",
    "#   Look at the notebook TrainModel_Step1_Local\n",
    "#      in this notebook, you basically set up the project with includes cloning \n",
    "#      and compiling the tensorflow/models repo\n",
    "#   we are using the utilities found in that repo\n",
    "\n",
    "cwd = os.getcwd()\n",
    "models = os.path.join(cwd, 'code/models/research/')\n",
    "slim = os.path.join(cwd, 'code/models/research/slim')\n",
    "sys.path.append(models)\n",
    "sys.path.append(slim)\n",
    "\n",
    "from code.cfa_utils.example_utils import feature_obj_detect\n",
    "from code.models.research.object_detection.utils import config_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLOBALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_DIR = os.path.join(cwd, 'code')\n",
    "TF_TRAIN_CONFIG = os.path.join(CODE_DIR, 'sagemaker_mobilenet_v1_ssd_retrain.config')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Utilities\n",
    "\n",
    "### tf.io\n",
    "this is not domain specific - i.e. not tied to object detection\n",
    "\n",
    "#### tf.io.gfile\n",
    "file i/o related utilites - probably everything you'll need to do with directories (but not os.path operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file exists utility\n",
    "tf.io.gfile.exists(TF_TRAIN_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### object_detection/utils\n",
    "these utilities are specific to the object detection \n",
    "#### hint \n",
    "Reading this output is difficult.   Read the underlying pbtxt file (in github) - it's much easier to read.  The main elements are:\n",
    "- model\n",
    "- train_config\n",
    "- train_input_reader\n",
    "- eval_config\n",
    "- eval_input_reader\n",
    "- graph_rewriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training pipeline parameters\n",
    "pipeline_config_dict = config_util.get_configs_from_pipeline_file(TF_TRAIN_CONFIG)\n",
    "print (pipeline_config_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = pipeline_config_dict['model']\n",
    "train_config = pipeline_config_dict['train_config']\n",
    "train_input_config = pipeline_config_dict['train_input_config']\n",
    "eval_config = pipeline_config_dict['eval_config']\n",
    "eval_input_config = pipeline_config_dict['eval_input_configs'] # !! note the inconsistent config(s)\n",
    "graph_rewriter_config = pipeline_config_dict['graph_rewriter_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"train_input_config:\", type(train_input_config))\n",
    "print (\"                   \", train_input_config)\n",
    "\n",
    "tf_record_input_reader = train_input_config.tf_record_input_reader\n",
    "print (\"tf_record_input_reader:\", type(tf_record_input_reader))\n",
    "print (\"                       \", tf_record_input_reader.input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary \n",
    "This is redundant - but here's the basic code to read the input sources and verify they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_input_data_existance(pipeline_config_dict):\n",
    "    input_keys = ['train_input_config', 'eval_input_config']\n",
    "    for input_key in input_keys:\n",
    "        print (\"checking inputs for:\", input_key)\n",
    "        input_config = pipeline_config_dict[input_key]\n",
    "        path_list = input_config.tf_record_input_reader.input_path\n",
    "        for p in path_list:\n",
    "            exists = tf.io.gfile.exists(p)\n",
    "            print (\"path:\", exists, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config_dict = config_util.get_configs_from_pipeline_file(TF_TRAIN_CONFIG)  # pipeline config dict\n",
    "\n",
    "check_input_data_existance(pipeline_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
