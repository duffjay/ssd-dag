{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "#### tensorflow_p36 environment\n",
    "\n",
    "## Prerequisites (done by Step 1)\n",
    "- Global Constants: not used, not needed by this notebook\n",
    "- Data: \n",
    "  - data was pulled from S3 and processed in Step 1\n",
    "  - TODO - decide if you want to pull from s3 in the Docker - issues:  multiple tarballs\n",
    "- Code:  Step 1 build the code (including pulling the tensorflow/models and compiling protobufs)\n",
    "\n",
    "## Step 2 - Test your train.py with Docker\n",
    "\n",
    " USE SAGEMAKER.   Don't get confused - running jobs on the local SageMaker server isn't really what it was designed for.  It is designed to take your program and send it to outside resouces (using a Docker container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker\n",
    "Set up Docker - if this is an instance with a GPU, it will configure accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/bin/bash ./sagemaker_docker_setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a local SageMaker estimator\n",
    "\n",
    "code/train_model - this entire directory goes to the Docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an Estimator is a SageMaker class\n",
    "# and, you're using the tensorflow flavor\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/opt/ml/model'     # this is related to how it gets deployed in the Docker\n",
    "                                # this is a SAGEMAKER thing - don't confuse with the model_dir \n",
    "                                # that we have inside our code\n",
    "    \n",
    "train_instance_type = 'local'   # local vs another server\n",
    "\n",
    "# these typical parameters are in the config file\n",
    "#hyperparameters = {'epochs': 5, 'batch_size': 128, 'learning_rate': 0.01}\n",
    "\n",
    "hyperparameters = {'pipeline_config_path' : 'sagemaker_mobilenet_v1_ssd_retrain.config',\n",
    "                   'num_train_steps' : '502',\n",
    "                   'num_eval_steps' : '10'\n",
    "                  }\n",
    "\n",
    "# SageMaker Execution Role\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'test' changed to 'val'\n",
    "inputs = {'train' : f'file://code/tfrecords/train', 'val' : f'file://code/tfrecords/val'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir='code',\n",
    "                       model_dir=model_dir,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=role,\n",
    "                       base_job_name='cfa_products_mobilenet_v1_SSD',\n",
    "                       framework_version='1.13',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging\n",
    "Debugging once the model is in a container can be difficult because you have so many nested operations.   The error message will be there someplace but it's tough to find.\n",
    "\n",
    "Write your train.py with a lot of environment logging and verification to make this easier.   Display the environmental variables and verify that everything exists.   Display results to console - and they will show up in the log.  This will make debugging easier.   If the model trained locally and fails in this configuration - it's probably due to the setup.\n",
    "\n",
    "you can open a terminal tab and $ top to see that this is running.   You can also monitor the notebook instance from the console.  \n",
    "- training Jobs - scroll down to Monitor\n",
    "  - you'll see resouces\n",
    "  - you can also go to instance monitoring - which takes you to CloudWatch - hit GraphSearch\n",
    "\n",
    "It's CPU bound - no GPU is being used so it's pretty slow.   You'll appreciate the GPU next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this took nearly 90 minutes - running locally \n",
    "# - you'll see in the log it did NOT use a GPU (not sure why because there was one on the server)\n",
    "local_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorRT)",
   "language": "python",
   "name": "tensorrt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
