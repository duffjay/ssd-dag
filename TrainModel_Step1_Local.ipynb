{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "## Step 1 - Develop a train.py script\n",
    "\n",
    "This is SageMaker Script Mode.   This is relatively new and much easier than the original SageMaker design.   You need to develop a train.py program that will:\n",
    "1. run locally - that means it will run on the local resources\n",
    "2. then you will test it locally with a Docker test\n",
    "\n",
    "If it runs in these tests, then it will/should run fine when you create a SageMaker Training job.   THIS IS THE CORRECT WAY TO USE SAGEMAKER.   Don't get confused - running jobs on the local SageMaker server isn't really what it was designed for.  It is designed to take your program and send it to outside resouces (using a Docker container)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-TensorFlow 2.0\n",
    "# set up eager execution\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(0)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet Model\n",
    "Why use a MobileNet Model?  Because the end objective is a lightweight model - one that will run on a Googl Coral TPU.    This requires a quantized model (int8 - not float32).  And, you get there from a TensorFlow Lite model.  The recommended path is to start with a model structure that you know is compatible (MobileNet) then retrain on top of it.  \n",
    "1. We pull the MobileNet v1 (there is a v2 that we aren't using) trained on COCO images\n",
    "2. We train on top of it (xfer learning) with our CFA Products\n",
    "3. That generates a TensorFlow Lite model (.tflite)\n",
    "4. We will later conver .tflite to an edge TPU model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_TFRECORDS_PATH = \"s3://cfaanalyticsresearch-sagemaker/datasets/cfa_products/tfrecords/\"\n",
    "TFRECORDS_TARBALL = \"20190718_tfrecords.tar.gz\"\n",
    "\n",
    "\n",
    "S3_MODEL_PATH = \"s3://cfaanalyticsresearch-sagemaker/trained-models/tensorflow_mobilenet/\"\n",
    "# base model - starting point that we train on top of\n",
    "BASE_MODEL_FOLDER = \"20180718_coco14_mobilenet_v1_ssd300_quantized\"\n",
    "\n",
    "# our CFA model\n",
    "# note the COINCIDENCE - 2018-0718 vs 2019-0718, don't let this confuse you!\n",
    "CFA_MODEL_FOLDER = \"20190718_cfa_prod_mobilenet_v1_ssd300/\"\n",
    "\n",
    "# project directories\n",
    "PROJECT = os.getcwd()\n",
    "CODE = os.path.join(PROJECT, \"code\")\n",
    "TASKS = os.path.join(PROJECT, \"tasks\")\n",
    "\n",
    "print (\"project directory:\", PROJECT)\n",
    "print (\"code directory:\", CODE)\n",
    "print (\"task directory:\", TASKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Get the data from s3.  you'll need to pass a directory into the training job\n",
    "### NOTE\n",
    "still unclear if data is in the Docker or passed in with the SageMaker job  \n",
    "TODO - figure this out, it's faster to NOT put it in the Docker (code/tfrecords), it just makes the Docker step slower.   the AWS fetch when the Docker starts is much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you're in top project directory\n",
    "s3_tfrecords = os.path.join(S3_TFRECORDS_PATH, TFRECORDS_TARBALL)\n",
    "print (s3_tfrecords)\n",
    "! aws s3 cp $s3_tfrecords code/tfrecords  \n",
    "\n",
    "# tarball is now in code/tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -xvf code/tfrecords/$TFRECORDS_TARBALL --strip=1 -C code/tfrecords\n",
    "! rm code/tfrecords/$TFRECORDS_TARBALL\n",
    "\n",
    "# tarball is gone, tfrecord files are in code/tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model\n",
    "this exercise will RETRAIN an existing model.  So, you need the starting point.  In this example, we are training on top of the BASE == MobileNet V1 that was trained with COCO images.   You could train on top of a CFA model - just make sure you config everything properly.\n",
    "\n",
    "Copy the model from S3.    You are coping a model from an S3 folder.  There may be a label map and config file - that would make sense so you can reproduce that model.   However, if you are training on top of this model - those files aren't useful - MAKE SURE YOU UNDERSTAND THIS.   \n",
    "\n",
    "So when you pull the model from the folder - just make sure you understand if you are re-using those meta files (e.g. reproducing a model) or or if you need something new (xfer learning).  The training process will NOT read from this download.  The training program will read the config from the code/ just to help avoid this confusion.\n",
    "\n",
    "#### CKPT\n",
    "When you retrain, the config file has a train_config / fine_tune_checkpoint attribute.  You are going to download this BASE model and put it in the code/ckpt/ directory.   The training job will start with the checkpoint file you specify.   For example:\n",
    "\n",
    "fine_tune_checkpoint: \"ckpt/model.ckpt\"\n",
    "\n",
    "#### WARNING code/ckpt/checkpoints\n",
    "When you run training, it will checkpoint to code/ckpt/checkpoints.  \n",
    "- if you train for 5000 steps, then repeat, it will do nothing basically because it will just reload the 5000 checkpoint file.\n",
    "- then you'll think you're smart and you'll remove the 5000 checkpoint file.  Not so fast bucko!\n",
    "- because then you'll discover  there is some pointer in the checkpoints/ that told the system the 5000 checkpoint exists - but now it doesn't because you just wiped it - so you'll get an error (that's difficult to figure out)\n",
    "\n",
    "just delete the checkpoints directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_folder = os.path.join(S3_MODEL_PATH, BASE_MODEL_FOLDER)\n",
    "! aws s3 cp $s3_model_folder code/ckpt --recursive\n",
    "\n",
    "# code/ckpt now has model.ckpt.* files\n",
    "# there is also a pipeline.config file (this one was configured for the Google Coral - you don't want it)\n",
    "# there are also some tflite files - we don't want them either"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local (Script) Mode Training\n",
    "\n",
    "see the AWS SageMaker tutorials notably:  \n",
    "https://github.com/aws-samples/amazon-sagemaker-script-mode/blob/master/tf-eager-script-mode/tf-eager-sm-scriptmode.ipynb\n",
    "\n",
    "The point here is, you can develop a training script locally, then know (have a high degree of confidence) it will run as a SageMaker training job.   (This is relatively new, the old way was more difficult and cumbersome.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do you have a training script that will run locally - without Docker?\n",
    "\n",
    "considering what is coming up, you want all code needed to train in one directory. (in this example, that will be the code/ directory.) That directory will be included in the Docker image.    \n",
    "\n",
    "This is going to get a little more cumbersome because we took a bunch of stuff from the (official) github tensorflow/models project.   - we are using the MobileNet model and a BUNCH of utilities.    To make sure we keep up to date, we will get all of this programmatically - i.e. clone the most recent version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### github tensorflow/models\n",
    "manually git clone the FIRST TIME.   The official TensorFlow github repo has a related repo with a bunch of models, tutorials, utilities etc.   We are using them.  So clone them to this machine.   In a subsequent step, we'll get the files we need from this local copy.\n",
    "\n",
    "git clone https://github.com/tensorflow/models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the latest software\n",
    "# - git clone\n",
    "# - get the protobuf compiler\n",
    "# - compile\n",
    "# - clean up\n",
    "os.chdir(TASKS)\n",
    "! ./install_tf_models.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(TF_MODEL)\n",
    "! use the install script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Packages not in the tensorflow_p36 conda environment\n",
    "so add them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python packages that are required\n",
    "! pip install pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Configuration\n",
    "\n",
    "trained-models/ may have a config file and a label map in the directories.  You can start with one of these.  BUT - there may be environmental variable values that you don't want - and you don't want the s3 pull operation to keep overwriting your config.   So, you can pull a model from s3.  Review the .config and label map files BUT !!! put YOUR config & label map file in the code/ directory.\n",
    "\n",
    "#### .config file\n",
    "See the config file for all parameters. the IN USE .config file is in the code/ direcory But you DEFINITELY need to look at these!\n",
    "- num_classes = should be consistent with labels.txt & label map\n",
    "- label_map_path (train & eval)\n",
    "    - there may be one in the model/ (that you pulled from s3)\n",
    "    - but move your desired label map to code/\n",
    "- inputs (train & eval) - not sure, SageMaker is passing that in\n",
    "- check all of the path statements \n",
    "- fine_tune_checkpoint - make sure you are fine tuning the correct file\n",
    "    - don't cross a _v1 with a _v2 - that definitely work\n",
    "   \n",
    "#### label map .pbtxt\n",
    "- classes start with 1 (not 0 based)\n",
    "- make sure your label map class count matches the config file\n",
    "- and it should match the label \n",
    "\n",
    "NOTE - a missing file will generate a complex error message.  NOT something as simple as file not found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(CODE)   # this will be the training directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python train.py \\\n",
    "  --pipeline_config_path=\"sagemaker_mobilenet_v1_ssd_retrain.config\" \\\n",
    "  --model_dir=\"output\" \\\n",
    "  --num_train_steps=\"500\" \\\n",
    "  --num_eval_steps=\"10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "1. if you run for 500 steps, then rerun the exact process, it is going to restore /ckpt/checkpoints (ckpt-500) and then thinks it is done.  So, basically does nothing\n",
    "2. Don't delete ckpt/  (rm ckpt/*.*) WITHOUT removing ckpt/checkpoints/   The program is always checking that checkpoints subdirectory and trying to restore.  For exampmle, you delete ckpt/ but leave ckpt/checkpoints, it finds a reference to ckpt-500 but you just deleted it - so it aborts\n",
    "3. Always check your files & paths carefully - the error messages that get thrown with a missing file are not always clear - and my send you on a wild goose chase when in reality - it was just a missing file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.  Now try training locally - but your training goes into a Docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/bin/bash ./sagemaker_docker_setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a local SageMaker estimator\n",
    "\n",
    "code/train_model - this entire directory goes to the Docker image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an Estimator is a SageMaker class\n",
    "# and, you're using the tensorflow flavor\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "model_dir = '/opt/ml/model'     # this is related to how it gets deployed in the Docker\n",
    "train_instance_type = 'local'   # local vs another server\n",
    "hyperparameters = {'epochs': 5, 'batch_size': 128, 'learning_rate': 0.01}\n",
    "local_estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir='train_model',\n",
    "                       model_dir=model_dir,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       base_job_name='tf-eager-scriptmode-bostonhousing',\n",
    "                       framework_version='1.13',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
