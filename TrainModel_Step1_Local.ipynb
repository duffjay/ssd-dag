{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "#### tensorflow_p36 environment\n",
    "\n",
    "There are several ways to run this code\n",
    "- on a SageMaker notebook (the original intent)\n",
    "- on a physical machine with a well configured dev environment\n",
    "- on a physical machine using a Docker (grilledclub/cuda-100-tf114:*)\n",
    "\n",
    "## Step 1 - Develop a train.py script\n",
    "\n",
    "This is SageMaker Script Mode.   This is relatively new and much easier than the original SageMaker design.   You need to develop a train.py program that will:\n",
    "1. run locally - that means it will run on the local resources\n",
    "2. then you will test it locally with a Docker test\n",
    "\n",
    "If it runs in these tests, then it will/should run fine when you create a SageMaker Training job.   THIS IS THE CORRECT WAY TO USE SAGEMAKER.   Don't get confused - running jobs on the local SageMaker server isn't really what it was designed for.  It is designed to take your program and send it to outside resouces (using a Docker container)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker is at 1.15\n",
    "# - kernel = conda_python3\n",
    "# ! pip install tensorflow-gpu==1.14\n",
    "#\n",
    "# - kernel = conda_tensorflow_p36\n",
    "#   1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2018 NVIDIA Corporation\r\n",
      "Built on Sat_Aug_25_21:08:01_CDT_2018\r\n",
      "Cuda compilation tools, release 10.0, V10.0.130\r\n"
     ]
    }
   ],
   "source": [
    "# currently CUDA 10.0\n",
    "! nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nvidia-smi\n",
    "this will show you how much memory is available in the GPU.   This is important if you start getting OOM (out of memory) errors.\n",
    "\n",
    "SageMaker p2.xlarge == 10+ GB  \n",
    "Note what is available.\n",
    "\n",
    "you can run (at a terminal)    \n",
    "  $ nvidia-smi -l 1   \n",
    "to see the GPU being used during training.  On SageMaker, you'll see the GPU is about 50% busy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 13 11:58:10 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 1080    On   | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 29%   44C    P0    40W / 180W |   1007MiB /  8117MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1723      G   /usr/lib/xorg/Xorg                            40MiB |\r\n",
      "|    0      2059      G   /usr/bin/gnome-shell                          49MiB |\r\n",
      "|    0      4640      G   /usr/lib/xorg/Xorg                           309MiB |\r\n",
      "|    0      4779      G   /usr/bin/gnome-shell                         164MiB |\r\n",
      "|    0      5418      G   ...quest-channel-token=8754471375505810511   439MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your GPU\n",
    "this should verify your GPU is correct\n",
    "\n",
    "## WARNING\n",
    "this is a good test but...  \n",
    "If you run it, it may not release  the GPU memory.   I didn't figure this out fully.   When I ran it, I would get an OOM error when the model started the training cycle - even with super small batch size.   So, something is up here.   You could play around and try stopping the notebook - check nvidia-smi to verify it released the GPU RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet Model\n",
    "Why use a MobileNet Model?  Because the end objective is a lightweight model - one that will run on a Googl Coral TPU.    This requires a quantized model (int8 - not float32).  And, you get there from a TensorFlow Lite model.  The recommended path is to start with a model structure that you know is compatible (MobileNet) then retrain on top of it.  \n",
    "1. We pull the MobileNet v1 (there is a v2 that we aren't using) trained on COCO images\n",
    "2. We train on top of it (xfer learning) with our CFA Products\n",
    "3. That generates a TensorFlow Lite model (.tflite)\n",
    "4. We will later conver .tflite to an edge TPU model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project directory: /media/home/jay/projects/ssd-dag\n",
      "code directory: /media/home/jay/projects/ssd-dag/code\n",
      "task directory: /media/home/jay/projects/ssd-dag/tasks\n"
     ]
    }
   ],
   "source": [
    "S3_TFRECORDS_PATH = \"s3://cfa-eadatasciencesb-sagemaker/datasets/cfa_products/tfrecords/\"\n",
    "TFRECORDS_TARBALL = \"20190718_tfrecords.tar.gz\"\n",
    "\n",
    "\n",
    "S3_MODEL_PATH = \"s3://cfa-eadatasciencesb-sagemaker/trained-models/tensorflow_mobilenet/\"\n",
    "# base model - starting point that we train on top of\n",
    "BASE_MODEL_FOLDER = \"20180718_coco14_mobilenet_v1_ssd300_quantized\"\n",
    "\n",
    "# our CFA model\n",
    "# note the COINCIDENCE - 2018-0718 vs 2019-0718, don't let this confuse you!\n",
    "CFA_MODEL_FOLDER = \"20190718_cfa_prod_mobilenet_v1_ssd300/\"\n",
    "\n",
    "# project directories\n",
    "PROJECT = os.getcwd()\n",
    "CODE = os.path.join(PROJECT, \"code\")\n",
    "TASKS = os.path.join(PROJECT, \"tasks\")\n",
    "\n",
    "print (\"project directory:\", PROJECT)\n",
    "print (\"code directory:\", CODE)\n",
    "print (\"task directory:\", TASKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data - 1x only\n",
    "\n",
    "Get the data from s3.  You only need to pull the data once - unless of course you update it.  you'll need to pass a directory into the training job\n",
    "\n",
    "### NOTE\n",
    "still unclear if data is in the Docker or passed in with the SageMaker job  \n",
    "TODO - figure this out, it's faster to NOT put it in the Docker (code/tfrecords), it just makes the Docker step slower.   the AWS fetch when the Docker starts is much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical or Docker\n",
    "# you can run the script\n",
    "# $ cd /task\n",
    "\n",
    "# check the Globals values in the script\n",
    "# $ bash local_get_s3_files.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://cfa-eadatasciencesb-sagemaker/datasets/cfa_products/tfrecords/20190718_tfrecords.tar.gz\n",
      "download: s3://cfa-eadatasciencesb-sagemaker/datasets/cfa_products/tfrecords/20190718_tfrecords.tar.gz to code/tfrecords/20190718_tfrecords.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# SAGEMAKER\n",
    "#  you're in top project directory\n",
    "s3_tfrecords = os.path.join(S3_TFRECORDS_PATH, TFRECORDS_TARBALL)\n",
    "print (s3_tfrecords)\n",
    "! aws s3 cp $s3_tfrecords code/tfrecords  \n",
    "\n",
    "# tarball is now in code/tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190718_tfrecords/test.tfrecord\n",
      "20190718_tfrecords/train.tfrecord\n",
      "20190718_tfrecords/val.tfrecord\n",
      "/media/home/jay/projects/ssd-dag\n"
     ]
    }
   ],
   "source": [
    "! tar -xvf code/tfrecords/$TFRECORDS_TARBALL --strip=1 -C code/tfrecords\n",
    "\n",
    "# tfrecords are all in the tfrecords/ directory\n",
    "# SageMaker likes train/test subdirectories\n",
    "# - warning - confusion with 'test' vs 'eval'\n",
    "#      I feel eval is the post train loop to evaluate the training loop - thus called val(uaion)\n",
    "#         and test is to test a model with random real-world data\n",
    "#      SageMaker calls what I call val == test\n",
    "! pwd\n",
    "! rm code/tfrecords/train/*.tfrecord* -f\n",
    "! rm code/tfrecords/val/*.tfrecord*   -f\n",
    "! rm code/tfrecords/test/*.tfrecord* -f\n",
    "\n",
    "! mv code/tfrecords/train*.* code/tfrecords/train\n",
    "! mv code/tfrecords/val*.* code/tfrecords/val\n",
    "! mv code/tfrecords/test*.* code/tfrecords/test\n",
    "\n",
    "! rm code/tfrecords/$TFRECORDS_TARBALL\n",
    "\n",
    "# tarball is gone, tfrecord files are in code/tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model - 1x only\n",
    "\n",
    "You only have to pull the model once.  This exercise will RETRAIN an existing model.  So, you need the starting point.  In this example, we are training on top of the BASE == MobileNet V1 that was trained with COCO images.   You could train on top of a CFA model - just make sure you config everything properly.\n",
    "\n",
    "Copy the model from S3.    You are coping a model from an S3 folder.  There may be a label map and config file - that would make sense so you can reproduce that model.   However, if you are training on top of this model - those files aren't useful - MAKE SURE YOU UNDERSTAND THIS.   \n",
    "\n",
    "So when you pull the model from the folder - just make sure you understand if you are re-using those meta files (e.g. reproducing a model) or or if you need something new (xfer learning).  The training process will NOT read from this download.  The training program will read the config from the code/ just to help avoid this confusion.\n",
    "\n",
    "#### CKPT\n",
    "When you retrain, the config file has a train_config / fine_tune_checkpoint attribute.  You are going to download this BASE model and put it in the code/ckpt/ directory.   The training job will start with the checkpoint file you specify.   For example:\n",
    "\n",
    "fine_tune_checkpoint: \"ckpt/model.ckpt\"\n",
    "\n",
    "#### WARNING code/ckpt/checkpoints\n",
    "When you run training, it will checkpoint to code/ckpt/checkpoints.  \n",
    "- if you train for 5000 steps, then repeat, it will do nothing basically because it will just reload the 5000 checkpoint file.\n",
    "- then you'll think you're smart and you'll remove the 5000 checkpoint file.  Not so fast bucko!\n",
    "- because then you'll discover  there is some pointer in the checkpoints/ that told the system the 5000 checkpoint exists - but now it doesn't because you just wiped it - so you'll get an error (that's difficult to figure out)\n",
    "\n",
    "just delete the checkpoints directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical or Docker\n",
    "# - you may have to delete stuff first\n",
    "# $ cd code\n",
    "# $ rm -rf models\n",
    "\n",
    "# $ cd ../tasks\n",
    "# $ bash install_tf_models.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://cfa-eadatasciencesb-sagemaker/trained-models/tensorflow_mobilenet/20180718_coco14_mobilenet_v1_ssd300_quantized/pipeline.config to code/ckpt/pipeline.config\n",
      "download: s3://cfa-eadatasciencesb-sagemaker/trained-models/tensorflow_mobilenet/20180718_coco14_mobilenet_v1_ssd300_quantized/model.ckpt.index to code/ckpt/model.ckpt.index\n",
      "download: s3://cfa-eadatasciencesb-sagemaker/trained-models/tensorflow_mobilenet/20180718_coco14_mobilenet_v1_ssd300_quantized/model.ckpt.meta to code/ckpt/model.ckpt.meta\n",
      "download: s3://cfa-eadatasciencesb-sagemaker/trained-models/tensorflow_mobilenet/20180718_coco14_mobilenet_v1_ssd300_quantized/model.ckpt.data-00000-of-00001 to code/ckpt/model.ckpt.data-00000-of-00001\n",
      "download: s3://cfa-eadatasciencesb-sagemaker/trained-models/tensorflow_mobilenet/20180718_coco14_mobilenet_v1_ssd300_quantized/tflite_graph.pb to code/ckpt/tflite_graph.pb\n",
      "download: s3://cfa-eadatasciencesb-sagemaker/trained-models/tensorflow_mobilenet/20180718_coco14_mobilenet_v1_ssd300_quantized/tflite_graph.pbtxt to code/ckpt/tflite_graph.pbtxt\n"
     ]
    }
   ],
   "source": [
    "# SageMaker (& Local?)\n",
    "# -- warning - something not right here\n",
    "#    I think you have to do this local or SageMaker (gotta have a base model)\n",
    "s3_model_folder = os.path.join(S3_MODEL_PATH, BASE_MODEL_FOLDER)\n",
    "! aws s3 cp $s3_model_folder code/ckpt --recursive\n",
    "\n",
    "# code/ckpt now has model.ckpt.* files\n",
    "# there is also a pipeline.config file (this one was configured for the Google Coral - you don't want it)\n",
    "# there are also some tflite files - we don't want them either"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### github tensorflow/models - 1x Only\n",
    "manually git clone the FIRST TIME.   The official TensorFlow github repo has a related repo with a bunch of models, tutorials, utilities etc.   We are using them.  So clone them to this machine.   In a subsequent step, we'll get the files we need from this local copy.\n",
    "\n",
    "!! - hold it -  \n",
    "!! this doesn't make sense, try not doing this - I don't think you need to git clone  \n",
    "!! doesn't the install_tf_models.sh do all of this?  \n",
    "!! I think we no longer copy, set just clone to code/models\n",
    "!! thus, you don't need this manual git clone, just run install_tf_models.sh in the next cell\n",
    "\n",
    "\n",
    "PHYSICAL COMPUTER  \n",
    "`cd ~/projects`  \n",
    "SAGEMAKER  \n",
    "`you should be in the SageMaker directory`  \n",
    "\n",
    "#### this will put /models into ~/projects  (you'll have ~/projects/models)\n",
    "`git clone https://github.com/tensorflow/models.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- git clone ---\n",
      "Cloning into 'models'...\n",
      "remote: Enumerating objects: 40, done.\u001b[K\n",
      "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
      "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
      "remote: Total 34071 (delta 11), reused 17 (delta 3), pack-reused 34031\u001b[K\n",
      "Receiving objects: 100% (34071/34071), 512.16 MiB | 51.16 MiB/s, done.\n",
      "Resolving deltas: 100% (21830/21830), done.\n",
      "Checking out files: 100% (2987/2987), done.\n",
      "--- get the protobuf compiler ---\n",
      "--2020-02-13 12:00:00--  https://github.com/google/protobuf/releases/download/v3.0.0/protoc-3.0.0-linux-x86_64.zip\n",
      "Resolving github.com (github.com)... 140.82.114.3\n",
      "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://github.com/protocolbuffers/protobuf/releases/download/v3.0.0/protoc-3.0.0-linux-x86_64.zip [following]\n",
      "--2020-02-13 12:00:01--  https://github.com/protocolbuffers/protobuf/releases/download/v3.0.0/protoc-3.0.0-linux-x86_64.zip\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/23357588/c692d808-54ca-11e6-90f6-ef943b0908bf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200213%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200213T170001Z&X-Amz-Expires=300&X-Amz-Signature=488c063527ce8792b78d9f3942645e0c0b6c2fa57380c736d57f02fb8b71e693&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dprotoc-3.0.0-linux-x86_64.zip&response-content-type=application%2Foctet-stream [following]\n",
      "--2020-02-13 12:00:01--  https://github-production-release-asset-2e65be.s3.amazonaws.com/23357588/c692d808-54ca-11e6-90f6-ef943b0908bf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200213%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200213T170001Z&X-Amz-Expires=300&X-Amz-Signature=488c063527ce8792b78d9f3942645e0c0b6c2fa57380c736d57f02fb8b71e693&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dprotoc-3.0.0-linux-x86_64.zip&response-content-type=application%2Foctet-stream\n",
      "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.176.219\n",
      "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.176.219|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1296281 (1.2M) [application/octet-stream]\n",
      "Saving to: ‘protobuf.zip’\n",
      "\n",
      "protobuf.zip        100%[===================>]   1.24M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2020-02-13 12:00:01 (11.9 MB/s) - ‘protobuf.zip’ saved [1296281/1296281]\n",
      "\n",
      "Archive:  protobuf.zip\n",
      "   creating: include/\n",
      "   creating: include/google/\n",
      "   creating: include/google/protobuf/\n",
      "  inflating: include/google/protobuf/struct.proto  \n",
      "  inflating: include/google/protobuf/type.proto  \n",
      "  inflating: include/google/protobuf/descriptor.proto  \n",
      "  inflating: include/google/protobuf/api.proto  \n",
      "  inflating: include/google/protobuf/empty.proto  \n",
      "   creating: include/google/protobuf/compiler/\n",
      "  inflating: include/google/protobuf/compiler/plugin.proto  \n",
      "  inflating: include/google/protobuf/any.proto  \n",
      "  inflating: include/google/protobuf/field_mask.proto  \n",
      "  inflating: include/google/protobuf/wrappers.proto  \n",
      "  inflating: include/google/protobuf/timestamp.proto  \n",
      "  inflating: include/google/protobuf/duration.proto  \n",
      "  inflating: include/google/protobuf/source_context.proto  \n",
      "   creating: bin/\n",
      "  inflating: bin/protoc              \n",
      "  inflating: readme.txt              \n",
      "--- compile protobufs ---\n",
      "--- clean up ---\n",
      "--- done! ---\n"
     ]
    }
   ],
   "source": [
    "# 1 time only\n",
    "\n",
    "# get the latest software\n",
    "# - git clone (to <project>/code/models)\n",
    "# - get the protobuf compiler\n",
    "# - compile the protobufs\n",
    "# - clean up\n",
    "os.chdir(TASKS)\n",
    "! ./install_tf_models.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local (Script) Mode Training\n",
    "\n",
    "#### -> if you know what you're doing, (you have a working SageMaker HOSTED training job) - you can jump out here!\n",
    "\n",
    "see the AWS SageMaker tutorials notably:  \n",
    "https://github.com/aws-samples/amazon-sagemaker-script-mode/blob/master/tf-eager-script-mode/tf-eager-sm-scriptmode.ipynb\n",
    "\n",
    "The point here is, you can develop a training script locally, then know (have a high degree of confidence) it will run as a SageMaker training job.   (This is relatively new, the old way was more difficult and cumbersome.)\n",
    "\n",
    "### What is Local?\n",
    "- local on THIS SageMaker Notebook (EC2) server\n",
    "  - p2.xlarge - no problem\n",
    "  - t2.medium - probably not (I think this is the same footprint as the feeble Workspace)\n",
    "- A desktop computer.\n",
    "  - works great on an Ubuntu laptop with GPU\n",
    "  - should work on a Windows laptop if you have a python environment set up\n",
    "- An AWS Workspace - not enough memory, you'll get a memory error.   The code runs - but fails on a memory allocation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do you have a training script that will run locally - without Docker?\n",
    "\n",
    "considering what is coming up, you want all code needed to train in one directory. (in this example, that will be the code/ directory.) That directory will be included in the Docker image.    \n",
    "\n",
    "This is going to get a little more cumbersome because we took a bunch of stuff from the (official) github tensorflow/models project.   - we are using the MobileNet model and a BUNCH of utilities.    To make sure we keep up to date, we will get all of this programmatically - i.e. clone the most recent version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Configuration\n",
    "\n",
    "trained-models/ may have a config file and a label map in the directories.  You can start with one of these.  BUT - there may be environmental variable values that you don't want - and you don't want the s3 pull operation to keep overwriting your config.   So, you can pull a model from s3.  Review the .config and label map files BUT !!! put YOUR config & label map file in the code/ directory.\n",
    "\n",
    "#### .config file\n",
    "See the config file for all parameters. the IN USE .config file is in the code/ direcory But you DEFINITELY need to look at these!\n",
    "- num_classes = should be consistent with labels.txt & label map\n",
    "- label_map_path (train & eval)\n",
    "    - there may be one in the model/ (that you pulled from s3)\n",
    "    - but move your desired label map to code/\n",
    "- inputs (train & eval) - not sure, SageMaker is passing that in\n",
    "- check all of the path statements \n",
    "- fine_tune_checkpoint - make sure you are fine tuning the correct file\n",
    "    - don't cross a _v1 with a _v2 - that definitely work\n",
    "   \n",
    "#### label map .pbtxt\n",
    "- classes start with 1 (not 0 based)\n",
    "- make sure your label map class count matches the config file\n",
    "- and it should match the label \n",
    "\n",
    "#### NOTE - a missing file will generate a complex error message.  NOT something as simple as file not found. \n",
    "\n",
    "#### NOTE - --model_dir parameter: \n",
    "- local mode, it needs to be model\n",
    "- SageMaker HOST mode, it needs to be /opt/ml/model\n",
    "\n",
    "--num_train_steps  \n",
    "   500 very quick test  \n",
    "   5000 more like it  \n",
    "-- num_eval_steps  \n",
    "   10 verify quick test  \n",
    "   100 more like it\n",
    "   \n",
    "beware of batch size - if you run out of GPU memory - see the config file, batch_size: 32;  you may need to decrease it if you have a small GPU\n",
    "\n",
    "GPU should be 95% utilized.  \n",
    "`nvidia-smi -l 1`\n",
    "CPU will be about 30%  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(CODE)   # this will be the training directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! Warning !!!\n",
    "# I changed the pipeline_config_path = local*.config\n",
    "# this local version expects the data to be in code/tfrecords\n",
    "\n",
    "# sagemaker*.config\n",
    "#  uses S3 to move the data\n",
    "\n",
    "# !!! I haven't tested !!!\n",
    "\n",
    "# 20200122 - physical computer (Inspiron)\n",
    "#  using Jupyter (below) error: ModuleNotFoundError: No module named 'absl'\n",
    "#  but, ran fine from terminal\n",
    "#\n",
    "#  nvidia-smi\n",
    "#      NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. \n",
    "#      Make sure that the latest NVIDIA driver is installed and running.\n",
    "#  but it ran trained fine so CUDA was good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "--> installing: cython\n",
      "Requirement already satisfied: cython in /media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages (0.29.15)\n",
      "--> installing: pycocotools\n",
      "Requirement already satisfied: pycocotools in /media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages (2.0.0)\n",
      "--> installing: matplotlib\n",
      "Requirement already satisfied: matplotlib in /media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages (3.1.3)\n",
      "Requirement already satisfied: numpy>=1.11 in /media/home/jay/.local/lib/python3.6/site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: setuptools in /media/home/jay/.local/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (45.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /media/home/jay/.local/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.14.0)\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From train115.py:186: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
      "\n",
      "*** train.py/main()\n",
      "*** FLAGS ***\n",
      "pipeline_config_path: local_mobilenet_v1_ssd_retrain.config\n",
      "config exists: True\n",
      "file: tf_serving_inference.py\n",
      "file: cfa_utils\n",
      "file: pipeline.config\n",
      "file: __pycache__\n",
      "file: annotation.py\n",
      "file: train115.py\n",
      "file: train.py\n",
      "file: display.py\n",
      "file: ckpt\n",
      "file: object_detection\n",
      "file: utils\n",
      "file: cfa_prod_label_map.pbtxt\n",
      "file: detect.py\n",
      "file: tflite_interpreter.py\n",
      "file: __init__.py\n",
      "file: local_mobilenet_v1_ssd_retrain.config\n",
      "file: sagemaker_mobilenet_v1_ssd_retrain.config\n",
      "file: model\n",
      "file: requirements.txt\n",
      "file: tfrecords\n",
      "file: models\n",
      "model_dir: model\n",
      "train: tfrecords/train/train.tfrecord\n",
      "val: tfrecords/val/val.tfrecord\n",
      "sample_1_of_n_eval_examples: 1\n",
      "hparams_overrides: None\n",
      "checkpoint_dir: None\n",
      "WARNING:tensorflow:From /media/home/jay/projects/ssd-dag/code/models/research/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W0213 12:00:21.389533 140318746122048 deprecation_wrapper.py:119] From /media/home/jay/projects/ssd-dag/code/models/research/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "checking inputs for: train_input_config\n",
      "path: True tfrecords/train/train.tfrecord\n",
      "checking inputs for: eval_input_config\n",
      "path: True tfrecords/val/val.tfrecord\n",
      " - - - - - - - - -\n",
      "WARNING:tensorflow:From train115.py:122: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
      "\n",
      "W0213 12:00:21.392251 140318746122048 deprecation_wrapper.py:119] From train115.py:122: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
      "\n",
      "WARNING:tensorflow:From train115.py:123: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "W0213 12:00:21.392372 140318746122048 deprecation_wrapper.py:119] From train115.py:123: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From train115.py:124: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "W0213 12:00:21.392472 140318746122048 deprecation_wrapper.py:119] From train115.py:124: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From train115.py:124: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n",
      "\n",
      "W0213 12:00:21.392545 140318746122048 deprecation_wrapper.py:119] From train115.py:124: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n",
      "\n",
      "- creating Estimator -\n",
      "checkpoint_dir: None\n",
      "- creating train_spec & eval_spec\n",
      "- train & evaluate\n",
      "- IF YOU GET NOTHING AFTER THIS, verify num_training_steps > largest checkpoint in /model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0213 12:00:25.417277 140318746122048 variables_helper.py:154] Variable [BoxPredictor_0/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[273]], model variable shape: [[36]]. This variable will not be initialized from the checkpoint.\n",
      "W0213 12:00:25.417373 140318746122048 variables_helper.py:154] Variable [BoxPredictor_0/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 512, 273]], model variable shape: [[1, 1, 512, 36]]. This variable will not be initialized from the checkpoint.\n",
      "W0213 12:00:25.417433 140318746122048 variables_helper.py:154] Variable [BoxPredictor_1/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[72]]. This variable will not be initialized from the checkpoint.\n",
      "W0213 12:00:25.417473 140318746122048 variables_helper.py:154] Variable [BoxPredictor_1/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 1024, 546]], model variable shape: [[1, 1, 1024, 72]]. This variable will not be initialized from the checkpoint.\n",
      "W0213 12:00:25.417523 140318746122048 variables_helper.py:154] Variable [BoxPredictor_2/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[72]]. This variable will not be initialized from the checkpoint.\n",
      "W0213 12:00:25.417561 140318746122048 variables_helper.py:154] Variable [BoxPredictor_2/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 512, 546]], model variable shape: [[1, 1, 512, 72]]. This variable will not be initialized from the checkpoint.\n",
      "W0213 12:00:25.417610 140318746122048 variables_helper.py:154] Variable [BoxPredictor_3/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[72]]. This variable will not be initialized from the checkpoint.\n",
      "W0213 12:00:25.417646 140318746122048 variables_helper.py:154] Variable [BoxPredictor_3/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 72]]. This variable will not be initialized from the checkpoint.\n",
      "W0213 12:00:25.417693 140318746122048 variables_helper.py:154] Variable [BoxPredictor_4/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[72]]. This variable will not be initialized from the checkpoint.\n",
      "W0213 12:00:25.417728 140318746122048 variables_helper.py:154] Variable [BoxPredictor_4/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 72]]. This variable will not be initialized from the checkpoint.\n",
      "W0213 12:00:25.417775 140318746122048 variables_helper.py:154] Variable [BoxPredictor_5/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[72]]. This variable will not be initialized from the checkpoint.\n",
      "W0213 12:00:25.417809 140318746122048 variables_helper.py:154] Variable [BoxPredictor_5/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 128, 546]], model variable shape: [[1, 1, 128, 72]]. This variable will not be initialized from the checkpoint.\n",
      "W0213 12:00:25.418802 140318746122048 variables_helper.py:157] Variable [global_step] is not available in checkpoint\n",
      "2020-02-13 12:00:37.825289: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-02-13 12:00:37.829199: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
      "2020-02-13 12:00:37.907426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-13 12:00:37.907882: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b5043446c0 executing computations on platform CUDA. Devices:\n",
      "2020-02-13 12:00:37.907896: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1\n",
      "2020-02-13 12:00:37.927616: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696000000 Hz\n",
      "2020-02-13 12:00:37.928263: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b5043a83d0 executing computations on platform Host. Devices:\n",
      "2020-02-13 12:00:37.928276: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2020-02-13 12:00:37.928405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-13 12:00:37.928694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\n",
      "pciBusID: 0000:01:00.0\n",
      "2020-02-13 12:00:37.928824: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2020-02-13 12:00:37.929539: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2020-02-13 12:00:37.930165: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
      "2020-02-13 12:00:37.930319: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
      "2020-02-13 12:00:37.931149: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2020-02-13 12:00:37.931752: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2020-02-13 12:00:37.933519: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-02-13 12:00:37.933617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-13 12:00:37.933976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-13 12:00:37.934194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2020-02-13 12:00:37.934220: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2020-02-13 12:00:37.934731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-02-13 12:00:37.934741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2020-02-13 12:00:37.934745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "2020-02-13 12:00:37.934806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-13 12:00:37.935049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-13 12:00:37.935444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6592 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-13 12:00:40.031764: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "2020-02-13 12:00:55.183174: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2020-02-13 12:00:56.021903: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-02-13 12:10:54.707870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-13 12:10:54.708114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\n",
      "pciBusID: 0000:01:00.0\n",
      "2020-02-13 12:10:54.708172: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2020-02-13 12:10:54.708196: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2020-02-13 12:10:54.708204: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
      "2020-02-13 12:10:54.708211: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
      "2020-02-13 12:10:54.708232: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2020-02-13 12:10:54.708239: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2020-02-13 12:10:54.708271: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-02-13 12:10:54.708318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-13 12:10:54.708535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-13 12:10:54.708756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2020-02-13 12:10:54.708788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-02-13 12:10:54.708792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2020-02-13 12:10:54.708796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "2020-02-13 12:10:54.708884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-13 12:10:54.709089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-13 12:10:54.709272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6592 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "2020-02-13 12:11:33.338303: W tensorflow/core/framework/op_kernel.cc:1490] Invalid argument: TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/media/home/jay/.local/lib/python3.6/site-packages/numpy/core/function_base.py\", line 117, in linspace\n",
      "    num = operator.index(num)\n",
      "\n",
      "TypeError: 'numpy.float64' object cannot be interpreted as an integer\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 209, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/media/home/jay/projects/ssd-dag/code/object_detection/metrics/coco_evaluation.py\", line 384, in first_value_func\n",
      "    self._metrics = self.evaluate()\n",
      "\n",
      "  File \"/media/home/jay/projects/ssd-dag/code/object_detection/metrics/coco_evaluation.py\", line 215, in evaluate\n",
      "    coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)\n",
      "\n",
      "  File \"/media/home/jay/projects/ssd-dag/code/object_detection/metrics/coco_tools.py\", line 176, in __init__\n",
      "    iouType=iou_type)\n",
      "\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/pycocotools/cocoeval.py\", line 76, in __init__\n",
      "    self.params = Params(iouType=iouType) # parameters\n",
      "\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/pycocotools/cocoeval.py\", line 527, in __init__\n",
      "    self.setDetParams()\n",
      "\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/pycocotools/cocoeval.py\", line 507, in setDetParams\n",
      "    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n",
      "\n",
      "  File \"<__array_function__ internals>\", line 6, in linspace\n",
      "\n",
      "  File \"/media/home/jay/.local/lib/python3.6/site-packages/numpy/core/function_base.py\", line 121, in linspace\n",
      "    .format(type(num)))\n",
      "\n",
      "TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.OutOfRangeError: 2 root error(s) found.\n",
      "  (0) Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "  (1) Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[Loss/assert_equal/Assert/Assert/data_0/_2377]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/evaluation.py\", line 272, in _evaluate_once\n",
      "    session.run(eval_ops, feed_dict)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 754, in run\n",
      "    run_metadata=run_metadata)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1252, in run\n",
      "    run_metadata=run_metadata)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1353, in run\n",
      "    raise six.reraise(*original_exc_info)\n",
      "  File \"/media/home/jay/.local/lib/python3.6/site-packages/six.py\", line 703, in reraise\n",
      "    raise value\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1338, in run\n",
      "    return self._sess.run(*args, **kwargs)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1411, in run\n",
      "    run_metadata=run_metadata)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1169, in run\n",
      "    return self._sess.run(*args, **kwargs)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 950, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1173, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.OutOfRangeError: 2 root error(s) found.\n",
      "  (0) Out of range: End of sequence\n",
      "\t [[node IteratorGetNext (defined at train115.py:181) ]]\n",
      "  (1) Out of range: End of sequence\n",
      "\t [[node IteratorGetNext (defined at train115.py:181) ]]\n",
      "\t [[Loss/assert_equal/Assert/Assert/data_0/_2377]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "Original stack trace for 'IteratorGetNext':\n",
      "  File \"train115.py\", line 186, in <module>\n",
      "    tf.app.run()\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/absl/app.py\", line 299, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"train115.py\", line 181, in main\n",
      "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\n",
      "    return executor.run()\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\n",
      "    return self.run_local()\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\n",
      "    saving_listeners=saving_listeners)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 367, in train\n",
      "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1158, in _train_model\n",
      "    return self._train_model_default(input_fn, hooks, saving_listeners)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1192, in _train_model_default\n",
      "    saving_listeners)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1484, in _train_with_estimator_spec\n",
      "    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 754, in run\n",
      "    run_metadata=run_metadata)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1252, in run\n",
      "    run_metadata=run_metadata)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1338, in run\n",
      "    return self._sess.run(*args, **kwargs)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1419, in run\n",
      "    run_metadata=run_metadata))\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 594, in after_run\n",
      "    if self._save(run_context.session, global_step):\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 619, in _save\n",
      "    if l.after_save(session, step):\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 519, in after_save\n",
      "    self._evaluate(global_step_value)  # updates self.eval_result\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 539, in _evaluate\n",
      "    self._evaluator.evaluate_and_export())\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 920, in evaluate_and_export\n",
      "    hooks=self._eval_spec.hooks)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 477, in evaluate\n",
      "    name=name)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 519, in _actual_eval\n",
      "    return _evaluate()\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 501, in _evaluate\n",
      "    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1501, in _evaluate_build_graph\n",
      "    self._call_model_fn_eval(input_fn, self.config))\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1534, in _call_model_fn_eval\n",
      "    input_fn, ModeKeys.EVAL)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1022, in _get_features_and_labels_from_input_fn\n",
      "    self._call_input_fn(input_fn, mode))\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py\", line 65, in parse_input_fn_result\n",
      "    result = iterator.get_next()\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 426, in get_next\n",
      "    output_shapes=self._structure._flat_shapes, name=name)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1947, in iterator_get_next\n",
      "    output_shapes=output_shapes, name=name)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/media/home/jay/.local/lib/python3.6/site-packages/numpy/core/function_base.py\", line 117, in linspace\n",
      "    num = operator.index(num)\n",
      "\n",
      "TypeError: 'numpy.float64' object cannot be interpreted as an integer\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 209, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/media/home/jay/projects/ssd-dag/code/object_detection/metrics/coco_evaluation.py\", line 384, in first_value_func\n",
      "    self._metrics = self.evaluate()\n",
      "\n",
      "  File \"/media/home/jay/projects/ssd-dag/code/object_detection/metrics/coco_evaluation.py\", line 215, in evaluate\n",
      "    coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)\n",
      "\n",
      "  File \"/media/home/jay/projects/ssd-dag/code/object_detection/metrics/coco_tools.py\", line 176, in __init__\n",
      "    iouType=iou_type)\n",
      "\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/pycocotools/cocoeval.py\", line 76, in __init__\n",
      "    self.params = Params(iouType=iouType) # parameters\n",
      "\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/pycocotools/cocoeval.py\", line 527, in __init__\n",
      "    self.setDetParams()\n",
      "\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/pycocotools/cocoeval.py\", line 507, in setDetParams\n",
      "    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n",
      "\n",
      "  File \"<__array_function__ internals>\", line 6, in linspace\n",
      "\n",
      "  File \"/media/home/jay/.local/lib/python3.6/site-packages/numpy/core/function_base.py\", line 121, in linspace\n",
      "    .format(type(num)))\n",
      "\n",
      "TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc_3}}]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"train115.py\", line 186, in <module>\n",
      "    tf.app.run()\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/absl/app.py\", line 299, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"train115.py\", line 181, in main\n",
      "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\n",
      "    return executor.run()\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\n",
      "    return self.run_local()\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\n",
      "    saving_listeners=saving_listeners)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 367, in train\n",
      "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1158, in _train_model\n",
      "    return self._train_model_default(input_fn, hooks, saving_listeners)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1192, in _train_model_default\n",
      "    saving_listeners)\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1484, in _train_with_estimator_spec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 754, in run\r\n",
      "    run_metadata=run_metadata)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1252, in run\r\n",
      "    run_metadata=run_metadata)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1353, in run\r\n",
      "    raise six.reraise(*original_exc_info)\r\n",
      "  File \"/media/home/jay/.local/lib/python3.6/site-packages/six.py\", line 703, in reraise\r\n",
      "    raise value\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1338, in run\r\n",
      "    return self._sess.run(*args, **kwargs)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1419, in run\r\n",
      "    run_metadata=run_metadata))\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 594, in after_run\r\n",
      "    if self._save(run_context.session, global_step):\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 619, in _save\r\n",
      "    if l.after_save(session, step):\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 519, in after_save\r\n",
      "    self._evaluate(global_step_value)  # updates self.eval_result\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 539, in _evaluate\r\n",
      "    self._evaluator.evaluate_and_export())\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 920, in evaluate_and_export\r\n",
      "    hooks=self._eval_spec.hooks)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 477, in evaluate\r\n",
      "    name=name)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 519, in _actual_eval\r\n",
      "    return _evaluate()\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 508, in _evaluate\r\n",
      "    output_dir=self.eval_dir(name))\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1609, in _evaluate_run\r\n",
      "    config=self._session_config)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/evaluation.py\", line 272, in _evaluate_once\r\n",
      "    session.run(eval_ops, feed_dict)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 854, in __exit__\r\n",
      "    self._close_internal(exception_type)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 887, in _close_internal\r\n",
      "    h.end(self._coordinated_creator.tf_sess)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 951, in end\r\n",
      "    self._final_ops, feed_dict=self._final_ops_feed_dict)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 950, in run\r\n",
      "    run_metadata_ptr)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n",
      "    feed_dict_tensor, options, run_metadata)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n",
      "    run_metadata)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n",
      "    raise type(e)(node_def, op, message)\r\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\r\n",
      "Traceback (most recent call last):\r\n",
      "\r\n",
      "  File \"/media/home/jay/.local/lib/python3.6/site-packages/numpy/core/function_base.py\", line 117, in linspace\r\n",
      "    num = operator.index(num)\r\n",
      "\r\n",
      "TypeError: 'numpy.float64' object cannot be interpreted as an integer\r\n",
      "\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 209, in __call__\r\n",
      "    ret = func(*args)\r\n",
      "\r\n",
      "  File \"/media/home/jay/projects/ssd-dag/code/object_detection/metrics/coco_evaluation.py\", line 384, in first_value_func\r\n",
      "    self._metrics = self.evaluate()\r\n",
      "\r\n",
      "  File \"/media/home/jay/projects/ssd-dag/code/object_detection/metrics/coco_evaluation.py\", line 215, in evaluate\r\n",
      "    coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)\r\n",
      "\r\n",
      "  File \"/media/home/jay/projects/ssd-dag/code/object_detection/metrics/coco_tools.py\", line 176, in __init__\r\n",
      "    iouType=iou_type)\r\n",
      "\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/pycocotools/cocoeval.py\", line 76, in __init__\r\n",
      "    self.params = Params(iouType=iouType) # parameters\r\n",
      "\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/pycocotools/cocoeval.py\", line 527, in __init__\r\n",
      "    self.setDetParams()\r\n",
      "\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/pycocotools/cocoeval.py\", line 507, in setDetParams\r\n",
      "    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\r\n",
      "\r\n",
      "  File \"<__array_function__ internals>\", line 6, in linspace\r\n",
      "\r\n",
      "  File \"/media/home/jay/.local/lib/python3.6/site-packages/numpy/core/function_base.py\", line 121, in linspace\r\n",
      "    .format(type(num)))\r\n",
      "\r\n",
      "TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\r\n",
      "\r\n",
      "\r\n",
      "\t [[node PyFunc_3 (defined at /media/home/jay/projects/ssd-dag/code/object_detection/metrics/coco_evaluation.py:394) ]]\r\n",
      "\r\n",
      "Original stack trace for 'PyFunc_3':\r\n",
      "  File \"train115.py\", line 186, in <module>\r\n",
      "    tf.app.run()\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"train115.py\", line 181, in main\r\n",
      "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\r\n",
      "    return executor.run()\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\r\n",
      "    return self.run_local()\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\r\n",
      "    saving_listeners=saving_listeners)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 367, in train\r\n",
      "    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1158, in _train_model\r\n",
      "    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1192, in _train_model_default\r\n",
      "    saving_listeners)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1484, in _train_with_estimator_spec\r\n",
      "    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 754, in run\r\n",
      "    run_metadata=run_metadata)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1252, in run\r\n",
      "    run_metadata=run_metadata)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1338, in run\r\n",
      "    return self._sess.run(*args, **kwargs)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1419, in run\r\n",
      "    run_metadata=run_metadata))\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 594, in after_run\r\n",
      "    if self._save(run_context.session, global_step):\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 619, in _save\r\n",
      "    if l.after_save(session, step):\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 519, in after_save\r\n",
      "    self._evaluate(global_step_value)  # updates self.eval_result\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 539, in _evaluate\r\n",
      "    self._evaluator.evaluate_and_export())\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 920, in evaluate_and_export\r\n",
      "    hooks=self._eval_spec.hooks)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 477, in evaluate\r\n",
      "    name=name)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 519, in _actual_eval\r\n",
      "    return _evaluate()\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 501, in _evaluate\r\n",
      "    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1501, in _evaluate_build_graph\r\n",
      "    self._call_model_fn_eval(input_fn, self.config))\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1537, in _call_model_fn_eval\r\n",
      "    features, labels, ModeKeys.EVAL, config)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1146, in _call_model_fn\r\n",
      "    model_fn_results = self._model_fn(features=features, **kwargs)\r\n",
      "  File \"/media/home/jay/projects/ssd-dag/code/object_detection/model_lib.py\", line 470, in model_fn\r\n",
      "    eval_config, list(category_index.values()), eval_dict)\r\n",
      "  File \"/media/home/jay/projects/ssd-dag/code/object_detection/eval_util.py\", line 927, in get_eval_metric_ops_for_evaluators\r\n",
      "    eval_dict))\r\n",
      "  File \"/media/home/jay/projects/ssd-dag/code/object_detection/metrics/coco_evaluation.py\", line 394, in get_estimator_eval_metric_ops\r\n",
      "    first_value_op = tf.py_func(first_value_func, [], tf.float32)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 480, in py_func\r\n",
      "    return py_func_common(func, inp, Tout, stateful, name=name)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 462, in py_func_common\r\n",
      "    func=func, inp=inp, Tout=Tout, stateful=stateful, eager=False, name=name)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 285, in _internal_py_func\r\n",
      "    input=inp, token=token, Tout=Tout, name=name)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/gen_script_ops.py\", line 159, in py_func\r\n",
      "    \"PyFunc\", input=input, token=token, Tout=Tout, name=name)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n",
      "    op_def=op_def)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n",
      "    op_def=op_def)\r\n",
      "  File \"/media/home/jay/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n",
      "    self._traceback = tf_stack.extract_stack()\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# These parameters can be set, if ommitted, takes values from SM_CHANNEL_ {_MODEL_DIR, _TRAIN, _VAL}\n",
    "# --model_dir\n",
    "# --train\n",
    "# --val\n",
    "# \n",
    "\n",
    "! python train115.py \\\n",
    "  --pipeline_config_path=\"local_mobilenet_v1_ssd_retrain.config\" \\\n",
    "  --num_train_steps=\"4000\" \\\n",
    "  --num_eval_steps=\"200\"  \\\n",
    "  --model_dir='model' \\\n",
    "  --train='tfrecords/train/train.tfrecord' \\\n",
    "  --val='tfrecords/val/val.tfrecord'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained Model Output -- IMPORTANT\n",
    "Where did it go? - THERE IS A BIG DIFFERENCE BETWEEN LOCAL TRAIN AND HOSTED TRAIN -- important !!\n",
    "\n",
    "train*.py will put the output in code/model    This is true for local or SageMaker hosted trained.   In this case, you trained locally, so the output is in code/model  -- end of story.\n",
    "\n",
    "\n",
    "When you train with a SageMaker Hosted train, the output still goes to code/model -- HOWEVER - that is in a docker image (that you will never see).  Then it gets coped to S3.   Then the notebook (TrainModel_Step3_TrainingJob) pulls a model output from S3.   Then extracts the tarball to {PROJECT}/trained_model   SO AT THIS POINT THE OUTPUT IS IN A DIFFERENT LOCATION !!\n",
    "\n",
    "The convert graph script is pulling from {PROJECT}/trained_model (not the native code/model location).    The easiest solution (you will see below) is to copy the desired checkpoint graph to the {PROJECT}/trained_model location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -la  model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "1. if you run for 500 steps, then rerun the exact process, it is going to restore /ckpt/checkpoints (ckpt-500) and then thinks it is done.  So, basically does nothing\n",
    "2. Don't delete ckpt/  (rm ckpt/*.*) WITHOUT removing ckpt/checkpoints/   The program is always checking that checkpoints subdirectory and trying to restore.  For exampmle, you delete ckpt/ but leave ckpt/checkpoints, it finds a reference to ckpt-500 but you just deleted it - so it aborts\n",
    "3. Always check your files & paths carefully - the error messages that get thrown with a missing file are not always clear - and my send you on a wild goose chase when in reality - it was just a missing file\n",
    "4. can't import nets - this is a PATH problem (models/research/slim needs to be in your path) - in the train.py program, it's programmatically added\n",
    "5. OOM when allocating tensor of shape [32,19,19,512] and type float\n",
    "\t [[{{node gradients/zeros_97}}]] -- go to the config file and change batch size to be smaller (e.g. 16)\n",
    "6. AttributeError: 'ParallelInterleaveDataset' object has no attribute '_flat_structure --- check your directories, like something didn't get installed correction (base model?  models/research stuff?  training data) -- seems to be a problem with the TF build from scratch;   use a pip install and this went away\n",
    "7. if you are mixing local ops and Docker runs - you may have messed up the ownership file outputs and checkpoints - try deleting everything and a new pull\n",
    "8. trains - then error:  TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a useable model\n",
    "At this point you have checkpoint files.   You need models (graphs).   There are many flavors:\n",
    "    - saved graph\n",
    "    - frozen graph\n",
    "    - TensorFlow Lite\n",
    "    - TensorRT\n",
    "    - EdgeTPU\n",
    "    \n",
    "The notebook:  TrainingJob_Step3_TrainingJob will show you how to convert a checkpoint file to a graph (frozen graph & tflite).   There is a bash file to do this.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAKE UP - make sure NUM_TRAINING_STEPS = the max number in the checkpoint files you listed above\n",
    "#  e.g. \n",
    "# ls model\n",
    "# -rw-rw-r--  1 ec2-user ec2-user 41116528 Jan 28 15:16 model.ckpt-6000.data-00000-of-00001\n",
    "# -rw-rw-r--  1 ec2-user ec2-user    27275 Jan 28 15:16 model.ckpt-6000.index\n",
    "# -rw-rw-r--  1 ec2-user {ec2-user  6987305 Jan 28 15:16 model.ckpt-6000.meta\n",
    "NUM_TRAINING_STEPS = 6000\n",
    "! cp {CODE}/model/*{NUM_TRAINING_STEPS}* {PROJECT}/trained_model\n",
    "! ls {PROJECT}/trained_model/*{NUM_TRAINING_STEPS}*\n",
    "\n",
    "# get the config from the train*.py parameters above\n",
    "PIPELINE_CONFIG = 'local_mobilenet_v1_ssd_retrain.config'\n",
    "! ls {CODE}/{PIPELINE_CONFIG}\n",
    "\n",
    "# if you don't see your checkpoint in */trained_model/  STOP - and fix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert checkpoint is a task script - located in the tasks/ directory\n",
    "os.chdir(TASKS)  \n",
    "! ./convert_checkpoint_to_edgetpu_tflite.sh --checkpoint_num {NUM_TRAINING_STEPS} --pipeline_config {PIPELINE_CONFIG}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow FROZEN GRAPH\n",
    "! ls {PROJECT}/tensorflow_model -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow Lite model\n",
    "! ls {PROJECT}/tflite_model -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf115)",
   "language": "python",
   "name": "tf115"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
