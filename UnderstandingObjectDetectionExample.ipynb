{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Object Detection tf.Examlpe w/ TF 1.14 \n",
    "# !! NOT TF 2.0 !!\n",
    "### Don't Load TF 2.0 Beta\n",
    "The tensorflow/models/research/object_detection scripts are NOT 2.0 compatible.   And, you don't want to change them.   You'll get an error on a GFile call in a proto buf utililty.   If you have loaded 2.0, you might have to just restart SageMaker.   I had problems going from 2.0 -> 1.14\n",
    "\n",
    "First, understand this is the world's worst named class - not an example - it is an Example of data (I guess).   Here I'm using TF 2.0beta - because the Understanding Images required it.    The on-line  tutorial shows you can use tf 1.14\n",
    "\n",
    "https://www.tensorflow.org/beta/tutorials/load_data/images  \n",
    "https://www.tensorflow.org/tutorials/load_data/tf_records\n",
    "\n",
    "code references:\n",
    "https://github.com/tensorflow/models/blob/master/research/object_detection/utils/dataset_util.py\n",
    "https://github.com/tensorflow/models/blob/master/research/object_detection/data_decoders/tf_example_decoder.py\n",
    "\n",
    "-- highly deprecated --\n",
    "https://stackoverflow.com/questions/46687348/decoding-tfrecord-with-tfslim\n",
    "\n",
    "You will need these skills!  What is an image, tf.Example, serialized example etc.  \n",
    "You won't get far with served models without this understanding.\n",
    "\n",
    "## TensorFlow 1.14\n",
    "\n",
    "### Do this SECOND, first, do UnderstandingImages.ipynb\n",
    "But, Understanding Images relied on 2.0.  So this might drive you crazy.  \n",
    "You want to use 2.0 as soon as possible - but here we use some utilities in the models repo and they have not been upgraded (which is odd - 20190814)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'object_detection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-67203b377700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_detection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_decoders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_example_decoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfExampleDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/SageMaker/ssd-dag/code/models/research/object_detection/data_decoders/tf_example_decoder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstandard_fields\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_reader_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'object_detection'"
     ]
    }
   ],
   "source": [
    "import os, sys, pathlib\n",
    "import random\n",
    "import IPython.display as display\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from code.models.research.object_detection.data_decoders.tf_example_decoder import TfExampleDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed since we cloned tensorflow/models under code.\n",
    "cwd = os.getcwd()\n",
    "models = os.path.join(cwd, 'code/models/research/')\n",
    "slim = os.path.join(cwd, 'code/models/research/slim')\n",
    "sys.path.append(models)\n",
    "sys.path.append(slim)\n",
    "\n",
    "# cfa_utils\n",
    "# DEL? from code.cfa_utils.gen_imagesets import gen_imageset_list\n",
    "from code.cfa_utils.example_utils import voc_to_tfrecord_file\n",
    "from code.cfa_utils.example_utils import feature_obj_detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (tf.__version__)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = os.getcwd()\n",
    "\n",
    "IMAGE_DIR = os.path.join(PROJECT_DIR, \"data/jpeg_images\")\n",
    "ANNOTATION_DIR = os.path.join(PROJECT_DIR, \"data/annotations\")\n",
    "\n",
    "MODEL_PATH = os.path.join(PROJECT_DIR, \"trained_model/export/Servo/1564778509\")\n",
    "LABEL_MAP = os.path.join(PROJECT_DIR, \"code/cfa_prod_label_map.pbtxt\")\n",
    "TFRECORD_DIR = os.path.join(PROJECT_DIR, \"data/tfrecords\")\n",
    "TRAINING_SPLIT_TUPLE = (60,30,10)\n",
    "\n",
    "# you can get data using the TrainModel_Step1_Local notebook\n",
    "TEST_TFRECORDS_PATH =  os.path.join(PROJECT_DIR, \"code/tfrecords/test/\")\n",
    "                                    \n",
    "# NAME - get this from the console\n",
    "ENDPOINT_NAME = \"ep-mobilenet-ssd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the label_map as a dict\n",
    "#    {'smallHotDrink': 2, 'nuggBox': 5, 'sandBag': 6, 'smallFry': 8, \n",
    "#     'largeFry': 9, 'cfaSauce': 7, 'mediumColdDrink': 3, 'sandBox': 4, \n",
    "#   'hand': 1, 'spicyBag': 11, 'polySauce': 10}\n",
    "\n",
    "# you might have to replace some names due to inconsistencies in labeling and the map\n",
    "\n",
    "! sed -i 's/smHotDrink/smallHotDrink/g' data/annotations/*.xml\n",
    "! sed -i 's/medColdDrink/mediumColdDrink/g' data/annotations/*.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRITE tfrecords\n",
    "this CFA script will:\n",
    "- read the /annotations/*.xml files\n",
    "- for each one that is verified (e.g. using labelImg)\n",
    "  - get the image file\n",
    "  - encode the label & image into a Feature (a dict that is defined for object detection - but not really a standard)\n",
    "  - then create a tf.Example from the Feature\n",
    "  - then add the example to the tfrecord file\n",
    "  \n",
    "#### Note\n",
    "The tfrecords should be sharded for performance reasons.   Ideally one per processor thread which will yield the best performance.\n",
    "\n",
    "good exampmle -  \n",
    "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tfrecords\n",
    "voc_to_tfrecord_file(IMAGE_DIR, ANNOTATION_DIR, LABEL_MAP, TFRECORD_DIR, TRAINING_SPLIT_TUPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ tfrecords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls data/tfrecords/train -l\n",
    "! ls data/tfrecords/val -l\n",
    "! ls data/tfrecords/test -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# note this supports a list - very handy if you sharded the tfrecord files\n",
    "# to make it interesting, just use train + val + test (until we actually shard the files)\n",
    "tfrecord_file_list = ['data/tfrecords/train/train.tfrecord',\n",
    "                     'data/tfrecords/val/val.tfrecord',\n",
    "                     'data/tfrecords/test/test.tfrecord']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset(tfrecord_file_list)\n",
    "print (\"raw dataset:\", type(raw_dataset))\n",
    "print (\"            \", raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Decoder Function\n",
    "First, you need a Feature, definition.    Feature is just a dict.   There is a commonly used standard in SSD models - as long as you use the same Feature definition on training, validation, inference - it doesn't matter, as long as  it is consistent.   We are using as much Google code as possible - which leverages the VOC PASCAL XML format (originally).  So we did NOT make this up.    \n",
    "\n",
    "At this time, there was no easy utility in models/research/object_detection -- but I'm sure there will be eventually.   I pulled the definition out of TfExampleDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_decoder = TfExampleDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Dataset\n",
    "\n",
    "When you read the tfrecord file, you get a tensorflow TFRecordDataSetV1  \n",
    "V1 = Tensorflow 1.x  \n",
    "V2 = is coming  \n",
    "\n",
    "It is a tensor with a string in it.   It has not be decoded into a Feature.  \n",
    "\n",
    "The Example Decoder function - taken from the tensorflow/models/research/object_detection/ software, will read the string and decode it into a \"standard - object detection\" Feature (which is simply a dict.  and returns a tensor.  Thus, this function is intended to be used as a .map() function.\n",
    "\n",
    "DON'T WRITE THIS YOURSELF - wherever possible, you want to use the tensorflow utilities\n",
    "\n",
    "### Thus,\n",
    "Below you see we use that function and map across the raw dataset - which returns a new dataset of decoded dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset = raw_dataset.map(example_decoder.decode)\n",
    "print (\"raw_dataset:\", type(raw_dataset))\n",
    "print (\"            \", raw_dataset)\n",
    "print (\"\\nexample_dataset:\", type(example_dataset))\n",
    "print (\"                 \", example_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, e in enumerate(example_dataset.take(4)):\n",
    "    print (n,type(e)) \n",
    "    image_decoded = e['image']\n",
    "    print (type(image_decoded))\n",
    "    print (image_decoded)\n",
    "    if n == 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions can be used to convert a value to a type compatible\n",
    "# with tf.Example.\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of observations in the dataset.\n",
    "n_observations = int(1e4)\n",
    "\n",
    "# Boolean feature, encoded as False or True.\n",
    "feature0 = np.random.choice([False, True], n_observations)\n",
    "\n",
    "# Integer feature, random from 0 to 4.\n",
    "feature1 = np.random.randint(0, 5, n_observations)\n",
    "\n",
    "# String feature\n",
    "strings = np.array([b'cat', b'dog', b'chicken', b'horse', b'goat'])\n",
    "feature2 = strings[feature1]\n",
    "\n",
    "# Float feature, from a standard normal distribution\n",
    "feature3 = np.random.randn(n_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(feature0, feature1, feature2, feature3):\n",
    "  \"\"\"\n",
    "  Creates a tf.Example message ready to be written to a file.\n",
    "  \"\"\"\n",
    "  # Create a dictionary mapping the feature name to the tf.Example-compatible\n",
    "  # data type.\n",
    "  feature = {\n",
    "      'feature0': _int64_feature(feature0),\n",
    "      'feature1': _int64_feature(feature1),\n",
    "      'feature2': _bytes_feature(feature2),\n",
    "      'feature3': _float_feature(feature3),\n",
    "  }\n",
    "\n",
    "  # Create a Features message using tf.train.Example.\n",
    "  print (\"feature:\", type(feature), feature)\n",
    "  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "  return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example observation from the dataset.\n",
    "\n",
    "example_observation = []\n",
    "\n",
    "serialized_example = serialize_example(False, 4, b'goat', 0.9876)\n",
    "type(serialized_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Serving\n",
    "using the saved_model_cli (see notebook: ?)  \n",
    "We know the input signature of the method:  \n",
    "- Method name is: tensorflow/serving/predict   \n",
    "\n",
    "The given SavedModel SignatureDef contains the following input(s):  \n",
    "  inputs['serialized_example'] tensor_info:  \n",
    "      dtype: DT_STRING  \n",
    "      shape: ()  \n",
    "      name: tf_example:0  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "predictor=sagemaker.tensorflow.model.TensorFlowPredictor(ENDPOINT_NAME, sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = predictor.predict(serialized_example_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dataset = raw_dataset.map(tf.train.Example.FromString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
