{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect (with the) Model\n",
    "## Step 1 - run the TENSORFLOW model locally\n",
    "\n",
    "This example uses TensorFlow saved_model.pb.   Which begs the question, what is the difference between a saved_model.pb and a frozen_inference_graph.pb?  Well\n",
    "https://stackoverflow.com/questions/52934795/what-is-difference-frozen-inference-graph-pb-and-saved-model-pb\n",
    "\n",
    "ref:  \n",
    "https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb  \n",
    "https://blog.algorithmia.com/deep-dive-into-object-detection-with-open-images-using-tensorflow/  \n",
    "https://pythonprogramming.net/tensorflow-object-detection-api-self-driving-car/  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "from distutils.version import StrictVersion\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# This is needed since we cloned tensorflow/models under code.\n",
    "cwd = os.getcwd()\n",
    "models = os.path.join(cwd, 'code/models/research/')\n",
    "slim = os.path.join(cwd, 'code/models/research/slim')\n",
    "sys.path.append(models)\n",
    "sys.path.append(slim)\n",
    "\n",
    "from object_detection.utils import ops as utils_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Environment Variables\n",
    "\n",
    "We are using objects and scripts in the project as much as possible.   They require environment variables to pass along where stuff is stored\n",
    "\n",
    "### Where did the model come from?\n",
    "When you trained (Step 3 - SageMaker HOSTED), \n",
    "- The eval_spec (inside your training program) includes an Exporter\n",
    "  - the Exporter will export the model to export/Server/<some timestamp> creating:\n",
    "    - saved_graph.pb\n",
    "    - variables\n",
    "- SageMaker included this in the output/model.tar.gz that it sent to S3\n",
    "- at the end of the Step 3 notebook, you retrieved the tarball and extracted it\n",
    "  - so now it is in trained_model/export/Servo/(you may have serveral here if your ran repeatedly)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = os.getcwd()\n",
    "IMAGE_DIR = os.path.join(PROJECT_DIR, \"data/new_jpeg_images\")\n",
    "\n",
    "MODEL_PATH = os.path.join(PROJECT_DIR, \"trained_model/export/Servo/1564865938\")\n",
    "LABEL_MAP = os.path.join(PROJECT_DIR, \"code/cfa_prod_label_map.pbtxt\")\n",
    "ANNOTATION_DIR = os.path.join(PROJECT_DIR, \"data/unverified_annotations\")\n",
    "\n",
    "S3_TEST_IMAGES = \"s3://cfaanalyticsresearch-sagemaker/datasets/cfa_products/test_images/\"\n",
    "SAMPLE_IMAGE = \"/home/ec2-user/SageMaker/ssd-dag/data/new_jpeg_images/20190710_variety_1562781002.jpg\"\n",
    "\n",
    "# you can get data using the TrainModel_Step1_Local notebook\n",
    "TEST_TFRECORDS_PATH =  os.path.join(PROJECT_DIR, \"code/tfrecords/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Serving\n",
    "\n",
    "https://www.tensorflow.org/tfx/serving/serving_basic\n",
    "\n",
    "SageMaker uses TensorFlow Serving - the advantage is the automation associated with directing traffic between models and upgrading.   Most of this is transparent to you.\n",
    "\n",
    "### Exploring the Model\n",
    "This utility is part of tensorflow (models?).  It will tell you about the input & output tensors\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/saved_model_cli.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls {MODEL_PATH}\n",
    "!saved_model_cli show --dir {MODEL_PATH} --tag_set serve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir {MODEL_PATH} --tag_set serve --signature_def tensorflow/serving/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir {MODEL_PATH} --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently the Exporter uses something called a SavedModel.simple  \n",
    "\n",
    "You need the tags, ref:  https://github.com/tensorflow/tensorflow/issues/25288  \n",
    "The stackoverflow reference above had [].  Without them - i.e. [] - you'll get an error \n",
    "  \n",
    "#### Here you need a saved model Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative 1\n",
    "loaded_model = tf.saved_model.load(sess=tf.Session(), tags=[tf.saved_model.tag_constants.SERVING], export_dir=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative 2\n",
    "# this is what is in the tutorial:\n",
    "# https://www.tensorflow.org/guide/saved_model\n",
    "! ls {MODEL_PATH}\n",
    "with tf.Session() as sess:\n",
    "    detection_graph = tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = loaded_model.signature_def[\"tf_example\"]\n",
    "print (type(predict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SavedModel\n",
    "\n",
    "### Understanding the MetaGraphDef\n",
    "\n",
    "https://www.tensorflow.org/tfx/serving/signature_defs\n",
    "\n",
    "Pay attention to versions.   Old: Signatures New: SignatureDefs   Much of the documentation I'm reading is out of date (and frustrating because it won't work!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the model signature keys?\n",
    "print (\"loaded model:\", type(loaded_model))   # don't attempt to print loaded_model -- too big?\n",
    "# print (\"Signature Keys:\", list(loaded_model.signatures.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.core.protobuf.meta_graph_pb2\n",
    "\n",
    "sde = loaded_model.SignatureDefEntry\n",
    "print (type(sde))\n",
    "#sde['tensorflow/serving/predict']\n",
    "print (sde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = loaded_model.SignatureDefEntry\n",
    "print (type(l), l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Label Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(LABEL_MAP, use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreive Data\n",
    "Copy the test images locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp {S3_TEST_IMAGES} {IMAGE_DIR} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "# load an image and resturn a numpy array\n",
    "\n",
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(IMAGE_DIR)\n",
    "image_list = list()\n",
    "for f in dir_list:\n",
    "    full_path = os.path.join(IMAGE_DIR, f)\n",
    "    if os.path.isfile(full_path) and os.path.splitext(f)[1].lower() == '.jpg':\n",
    "        image_list.append(full_path)\n",
    "\n",
    "# limitations with the way we are displaying\n",
    "image_list = image_list[:1]\n",
    "print (\"Image Count:\", len(image_list))\n",
    "print (\"Sample:\",image_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (6, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Model\n",
    "\n",
    "TensorFlow provides the framework for the model - but you really don't know how the model was programmed for input & output.    You could dissect the MobileNet source code but the easier method in this case is to examine a sample (that came from tensorflow/models/object_detection)\n",
    "\n",
    "#### How do you pass data into the Model?\n",
    "The model has 3400+ operations and a lot of tensors.  \n",
    "Image Tensor: Tensor(\"image_tensor:0\", shape=(?, ?, ?, 3), dtype=uint8)  \n",
    "\n",
    "##### Input Tensor\n",
    "\n",
    "##### Output Tensors\n",
    "We can are looking for 4 tensors (and they are float32)  \n",
    "What is a detection mask?  \n",
    "\n",
    "#### How do you process the inference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME - get this from the console\n",
    "ENDPOINT_NAME = \"ep-mobilenet-ssd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "predictor=sagemaker.tensorflow.model.TensorFlowPredictor(ENDPOINT_NAME, sagemaker_session)\n",
    "\n",
    "print (type(predictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get attributes from the SAVED GRAPH  \n",
    "# - not the frozen graph.pb\n",
    "def run_inference_for_single_image(image, graph):\n",
    "  with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.get_default_graph().get_operations()\n",
    "      print (\"ALL model operations:\", type(ops), len(ops), ops)\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      print (\"tensor names:\", type(all_tensor_names), len(all_tensor_names))\n",
    "      tensor_dict = {}\n",
    "      for key in [\n",
    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
    "          'detection_classes', 'detection_masks'\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_value = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "          print (tensor_name, tensor_value)\n",
    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "      if 'detection_masks' in tensor_dict:\n",
    "        print (\"*** detection mask in the tensor dict ***\")\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[1], image.shape[2])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "      print (\"image_tensor:\", image_tensor)\n",
    "        \n",
    "      # Run inference\n",
    "      print (\"tensor dict:\", tensor_dict)\n",
    "      output_dict = sess.run(tensor_dict,feed_dict={image_tensor: image})\n",
    "      # output_dict = predictor.predict({'image_tensor': image})\n",
    "\n",
    "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "      output_dict['detection_classes'] = output_dict[\n",
    "          'detection_classes'][0].astype(np.int64)\n",
    "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "      if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in image_list:\n",
    "  image = Image.open(image_path)\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = load_image_into_numpy_array(image)\n",
    "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "  # Actual detection.\n",
    "  output_dict = run_inference_for_single_image(image_np_expanded, detection_graph)\n",
    "  # Visualization of the results of a detection.\n",
    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks'),\n",
    "      use_normalized_coordinates=True,\n",
    "      line_thickness=8)\n",
    "  plt.figure(figsize=IMAGE_SIZE)\n",
    "  plt.imshow(image_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model CLI - run\n",
    "\n",
    "```\n",
    "saved_model_cli run [-h] --dir DIR --tag_set TAG_SET --signature_def\n",
    "                           SIGNATURE_DEF_KEY [--inputs INPUTS]\n",
    "                           [--input_exprs INPUT_EXPRS]\n",
    "                           [--input_examples INPUT_EXAMPLES] [--outdir OUTDIR]\n",
    "                           [--overwrite] [--tf_debug]\n",
    "                           ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_cli run [-h] --dir DIR --tag_set TAG_SET --signature_def\n",
    "                           SIGNATURE_DEF_KEY [--inputs INPUTS]\n",
    "                           [--input_exprs INPUT_EXPRS]\n",
    "                           [--input_examples INPUT_EXAMPLES] [--outdir OUTDIR]\n",
    "                           [--overwrite] [--tf_debug]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Examples \n",
    "Not an example of input - input = tf.train.Example  \n",
    "\n",
    "So, how to create Example from a tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed since we cloned tensorflow/models under code.\n",
    "cwd = os.getcwd()\n",
    "models = os.path.join(cwd, 'code/models/research/')\n",
    "slim = os.path.join(cwd, 'code/models/research/slim')\n",
    "sys.path.append(models)\n",
    "sys.path.append(slim)\n",
    "\n",
    "from object_detection.inference import detection_inference\n",
    "\n",
    "# cfa_utils\n",
    "#  - some feature helper functions\n",
    "#  - a Feature - (common format) for object detection\n",
    "from code.cfa_utils.example_utils import feature_obj_detect\n",
    "from code.cfa_utils.example_utils import int64_feature, int64_list_feature, bytes_feature, bytes_list_feature, float_list_feature\n",
    "\n",
    "\n",
    "! ls {TEST_TFRECORDS_PATH}\n",
    "input_tfrecord_paths = [TEST_TFRECORDS_PATH]\n",
    "\n",
    "\n",
    "serialized_example_tensor, image_tensor = detection_inference.build_input( input_tfrecord_paths)\n",
    "\n",
    "print (\"serialized_example_tensor\", serialized_example_tensor)\n",
    "print (\"image_tensor\", image_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature dict was defined on the fly\n",
    "img_path = os.path.join(SAMPLE_IMAGE)\n",
    "with tf.io.gfile.GFile(img_path, 'rb') as fid:\n",
    "   encoded_jpg = fid.read()\n",
    "\n",
    "type(encoded_jpg)\n",
    "\n",
    "tf_example = tf.train.Example(features=tf.train.Features(feature={'image/encoded': bytes_feature(encoded_jpg)}))\n",
    "print (\"tf.train.Example:\", type(tf_example))\n",
    "# print (\"tf.train.Example Value:\", tf_example)\n",
    "ex_list = [tf_example]\n",
    "print (type(ex_list), type(ex_list[0]))\n",
    "# = {'image/encoded': bytes_feature(encoded_jpg)}\n",
    "d = {'image/encoded' : bytes_feature(encoded_jpg)}\n",
    "# print (type(d), d)\n",
    "# INPUT_EXAMPLES = 'serialized_example=[' + d + ']'\n",
    "d_str = json.dumps(d)\n",
    "print (d_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! saved_model_cli run --dir {MODEL_PATH} --tag_set serve --signature_def tensorflow/serving/predict --input_examples serialized_example=ex_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "If you want the annotations - you'll find the annotations tarball in data/ directory\n",
    "use the Notebook browser to download it\n",
    "\n",
    "The fastest, easiest way to review (and correct / verify) is to use labelImg program which will merge the image and annotation\n",
    "\n",
    "The main conclusion here is our model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (detection_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(SAMPLE_IMAGE)\n",
    "image_np = load_image_into_numpy_array(image)\n",
    "# Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "print (image_np_expanded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image_np_expanded\n",
    "\n",
    "with detection_graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.get_default_graph().get_operations()\n",
    "      print (\"ALL model operations:\", type(ops), len(ops))\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      print (\"tensor names:\", type(all_tensor_names), len(all_tensor_names))\n",
    "      tensor_dict = {}\n",
    "      for key in [\n",
    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
    "          'detection_classes', 'detection_masks'\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_value = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "          print (tensor_name, tensor_value)\n",
    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "      if 'detection_masks' in tensor_dict:\n",
    "        print (\"*** detection mask in the tensor dict ***\")\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[1], image.shape[2])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "      print (\"image_tensor:\", image_tensor)\n",
    "        \n",
    "      # Run inference\n",
    "      print (\"tensor dict:\", tensor_dict)\n",
    "        \n",
    "      # input into the model must be JSON\n",
    "      output_dict = sess.run(tensor_dict,feed_dict={image_tensor: image})\n",
    "      # output_dict = predictor.predict(tensor_dict,feed_dict={image_tensor: image})  # doesn't like feed_dict\n",
    "      # output_dict = predictor.predict(tensor_dict, {image_tensor: image})\n",
    "      # output_dict = predictor.predict(tensor_dict, feed_dict={\"image_tensor\": image_tensor})\n",
    "\n",
    "      # ops includes:\n",
    "      # <tf.Operation 'image_tensor' type=Placeholder>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(image_np_expanded), image_np_expanded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predict(tf.constant(image_np_expanded))[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img = tf.keras.preprocessing.image.load_img(SAMPLE_IMAGE, target_size=[300, 300])\n",
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "print (type(x), x.shape)\n",
    "\n",
    "x = tf.keras.applications.mobilenet.preprocess_input(x[tf.newaxis,...])\n",
    "print (type(x), x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x):\n",
    "    example = tf.train.Example()\n",
    "    example.features.feature[\"x\"].float_list.value.extend([x])\n",
    "    return loaded_model.signatures[\"predict\"](examples=tf.constant([example.SerializeToString()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
