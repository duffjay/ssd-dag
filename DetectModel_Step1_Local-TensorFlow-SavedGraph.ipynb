{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect (with the) Model\n",
    "## Step 1 - run the TENSORFLOW model locally\n",
    "\n",
    "This example uses TensorFlow saved_model.pb.   Which begs the question, what is the difference between a saved_model.pb and a frozen_inference_graph.pb?  Well\n",
    "https://stackoverflow.com/questions/52934795/what-is-difference-frozen-inference-graph-pb-and-saved-model-pb\n",
    "\n",
    "ref:  \n",
    "https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb  \n",
    "https://blog.algorithmia.com/deep-dive-into-object-detection-with-open-images-using-tensorflow/  \n",
    "https://pythonprogramming.net/tensorflow-object-detection-api-self-driving-car/  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "from distutils.version import StrictVersion\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# This is needed since we cloned tensorflow/models under code.\n",
    "cwd = os.getcwd()\n",
    "models = os.path.join(cwd, 'code/models/research/')\n",
    "slim = os.path.join(cwd, 'code/models/research/slim')\n",
    "sys.path.append(models)\n",
    "sys.path.append(slim)\n",
    "\n",
    "from object_detection.utils import ops as utils_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Environment Variables\n",
    "\n",
    "We are using objects and scripts in the project as much as possible.   They require environment variables to pass along where stuff is stored\n",
    "\n",
    "### Where did the model come from?\n",
    "When you trained (Step 3 - SageMaker HOSTED), \n",
    "- The eval_spec (inside your training program) includes an Exporter\n",
    "  - the Exporter will export the model to export/Server/<some timestamp> creating:\n",
    "    - saved_graph.pb\n",
    "    - variables\n",
    "- SageMaker included this in the output/model.tar.gz that it sent to S3\n",
    "- at the end of the Step 3 notebook, you retrieved the tarball and extracted it\n",
    "  - so now it is in trained_model/export/Servo/(you may have serveral here if your ran repeatedly)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = os.getcwd()\n",
    "IMAGE_DIR = os.path.join(PROJECT_DIR, \"data/new_jpeg_images\")\n",
    "\n",
    "MODEL_PATH = os.path.join(PROJECT_DIR, \"trained_model/export/Servo/1564865938\")\n",
    "LABEL_MAP = os.path.join(PROJECT_DIR, \"code/cfa_prod_label_map.pbtxt\")\n",
    "ANNOTATION_DIR = os.path.join(PROJECT_DIR, \"data/unverified_annotations\")\n",
    "\n",
    "S3_TEST_IMAGES = \"s3://cfaanalyticsresearch-sagemaker/datasets/cfa_products/test_images/\"\n",
    "SAMPLE_IMAGE = \"/home/ec2-user/SageMaker/ssd-dag/data/new_jpeg_images/20190710_variety_1562781002.jpg\"\n",
    "\n",
    "# you can get data using the TrainModel_Step1_Local notebook\n",
    "TEST_TFRECORDS_PATH =  os.path.join(PROJECT_DIR, \"code/tfrecords/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Serving\n",
    "\n",
    "https://www.tensorflow.org/tfx/serving/serving_basic\n",
    "\n",
    "SageMaker uses TensorFlow Serving - the advantage is the automation associated with directing traffic between models and upgrading.   Most of this is transparent to you.\n",
    "\n",
    "### Exploring the Model\n",
    "This utility is part of tensorflow (models?).  It will tell you about the input & output tensors\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/saved_model_cli.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_model.pb\tvariables\n",
      "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\n",
      "SignatureDef key: \"serving_default\"\n",
      "SignatureDef key: \"tensorflow/serving/predict\"\n"
     ]
    }
   ],
   "source": [
    "! ls {MODEL_PATH}\n",
    "!saved_model_cli show --dir {MODEL_PATH} --tag_set serve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\r\n",
      "  inputs['serialized_example'] tensor_info:\r\n",
      "      dtype: DT_STRING\r\n",
      "      shape: ()\r\n",
      "      name: tf_example:0\r\n",
      "The given SavedModel SignatureDef contains the following output(s):\r\n",
      "  outputs['detection_boxes'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (1, 100, 4)\r\n",
      "      name: detection_boxes:0\r\n",
      "  outputs['detection_classes'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (1, 100)\r\n",
      "      name: detection_classes:0\r\n",
      "  outputs['detection_multiclass_scores'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (1, 100, 12)\r\n",
      "      name: detection_multiclass_scores:0\r\n",
      "  outputs['detection_scores'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (1, 100)\r\n",
      "      name: detection_scores:0\r\n",
      "  outputs['num_detections'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (1)\r\n",
      "      name: num_detections:0\r\n",
      "  outputs['raw_detection_boxes'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (1, 1917, 4)\r\n",
      "      name: raw_detection_boxes:0\r\n",
      "  outputs['raw_detection_scores'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (1, 1917, 12)\r\n",
      "      name: raw_detection_scores:0\r\n",
      "Method name is: tensorflow/serving/predict\r\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {MODEL_PATH} --tag_set serve --signature_def tensorflow/serving/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\r\n",
      "  inputs['serialized_example'] tensor_info:\r\n",
      "      dtype: DT_STRING\r\n",
      "      shape: ()\r\n",
      "      name: tf_example:0\r\n",
      "The given SavedModel SignatureDef contains the following output(s):\r\n",
      "  outputs['detection_boxes'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (1, 100, 4)\r\n",
      "      name: detection_boxes:0\r\n",
      "  outputs['detection_classes'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (1, 100)\r\n",
      "      name: detection_classes:0\r\n",
      "  outputs['detection_multiclass_scores'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (1, 100, 12)\r\n",
      "      name: detection_multiclass_scores:0\r\n",
      "  outputs['detection_scores'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (1, 100)\r\n",
      "      name: detection_scores:0\r\n",
      "  outputs['num_detections'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (1)\r\n",
      "      name: num_detections:0\r\n",
      "  outputs['raw_detection_boxes'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (1, 1917, 4)\r\n",
      "      name: raw_detection_boxes:0\r\n",
      "  outputs['raw_detection_scores'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (1, 1917, 12)\r\n",
      "      name: raw_detection_scores:0\r\n",
      "Method name is: tensorflow/serving/predict\r\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {MODEL_PATH} --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently the Exporter uses something called a SavedModel.simple  \n",
    "\n",
    "You need the tags, ref:  https://github.com/tensorflow/tensorflow/issues/25288  \n",
    "The stackoverflow reference above had [].  Without them - i.e. [] - you'll get an error \n",
    "  \n",
    "#### Here you need a saved model Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative 1\n",
    "loaded_model = tf.saved_model.load(sess=tf.Session(), tags=[tf.saved_model.tag_constants.SERVING], export_dir=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_model.pb\tvariables\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0824 13:42:22.694341 140046763558720 deprecation.py:323] From <ipython-input-8-b36c4d5cb2ce>:6: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "W0824 13:42:23.811750 140046763558720 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1282: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "# alternative 2\n",
    "# this is what is in the tutorial:\n",
    "# https://www.tensorflow.org/guide/saved_model\n",
    "! ls {MODEL_PATH}\n",
    "with tf.Session() as sess:\n",
    "    detection_graph = tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-08352006d59a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature_def\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tf_example\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loaded_model' is not defined"
     ]
    }
   ],
   "source": [
    "predict = loaded_model.signature_def[\"tf_example\"]\n",
    "print (type(predict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SavedModel\n",
    "\n",
    "### Understanding the MetaGraphDef\n",
    "\n",
    "https://www.tensorflow.org/tfx/serving/signature_defs\n",
    "\n",
    "Pay attention to versions.   Old: Signatures New: SignatureDefs   Much of the documentation I'm reading is out of date (and frustrating because it won't work!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the model signature keys?\n",
    "print (\"loaded model:\", type(loaded_model))   # don't attempt to print loaded_model -- too big?\n",
    "# print (\"Signature Keys:\", list(loaded_model.signatures.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.core.protobuf.meta_graph_pb2\n",
    "\n",
    "sde = loaded_model.SignatureDefEntry\n",
    "print (type(sde))\n",
    "#sde['tensorflow/serving/predict']\n",
    "print (sde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = loaded_model.SignatureDefEntry\n",
    "print (type(l), l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Label Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(LABEL_MAP, use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreive Data\n",
    "Copy the test images locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp {S3_TEST_IMAGES} {IMAGE_DIR} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "# load an image and resturn a numpy array\n",
    "\n",
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(IMAGE_DIR)\n",
    "image_list = list()\n",
    "for f in dir_list:\n",
    "    full_path = os.path.join(IMAGE_DIR, f)\n",
    "    if os.path.isfile(full_path) and os.path.splitext(f)[1].lower() == '.jpg':\n",
    "        image_list.append(full_path)\n",
    "\n",
    "# limitations with the way we are displaying\n",
    "image_list = image_list[:1]\n",
    "print (\"Image Count:\", len(image_list))\n",
    "print (\"Sample:\",image_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (6, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Model\n",
    "\n",
    "TensorFlow provides the framework for the model - but you really don't know how the model was programmed for input & output.    You could dissect the MobileNet source code but the easier method in this case is to examine a sample (that came from tensorflow/models/object_detection)\n",
    "\n",
    "#### How do you pass data into the Model?\n",
    "The model has 3400+ operations and a lot of tensors.  \n",
    "Image Tensor: Tensor(\"image_tensor:0\", shape=(?, ?, ?, 3), dtype=uint8)  \n",
    "\n",
    "##### Input Tensor\n",
    "\n",
    "##### Output Tensors\n",
    "We can are looking for 4 tensors (and they are float32)  \n",
    "What is a detection mask?  \n",
    "\n",
    "#### How do you process the inference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME - get this from the console\n",
    "ENDPOINT_NAME = \"ep-mobilenet-ssd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "predictor=sagemaker.tensorflow.model.TensorFlowPredictor(ENDPOINT_NAME, sagemaker_session)\n",
    "\n",
    "print (type(predictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get attributes from the SAVED GRAPH  \n",
    "# - not the frozen graph.pb\n",
    "def run_inference_for_single_image(image, graph):\n",
    "  with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.get_default_graph().get_operations()\n",
    "      print (\"ALL model operations:\", type(ops), len(ops), ops)\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      print (\"tensor names:\", type(all_tensor_names), len(all_tensor_names))\n",
    "      tensor_dict = {}\n",
    "      for key in [\n",
    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
    "          'detection_classes', 'detection_masks'\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_value = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "          print (tensor_name, tensor_value)\n",
    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "      if 'detection_masks' in tensor_dict:\n",
    "        print (\"*** detection mask in the tensor dict ***\")\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[1], image.shape[2])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "      print (\"image_tensor:\", image_tensor)\n",
    "        \n",
    "      # Run inference\n",
    "      print (\"tensor dict:\", tensor_dict)\n",
    "      output_dict = sess.run(tensor_dict,feed_dict={image_tensor: image})\n",
    "      # output_dict = predictor.predict({'image_tensor': image})\n",
    "\n",
    "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "      output_dict['detection_classes'] = output_dict[\n",
    "          'detection_classes'][0].astype(np.int64)\n",
    "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "      if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in image_list:\n",
    "  image = Image.open(image_path)\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = load_image_into_numpy_array(image)\n",
    "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "  # Actual detection.\n",
    "  output_dict = run_inference_for_single_image(image_np_expanded, detection_graph)\n",
    "  # Visualization of the results of a detection.\n",
    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks'),\n",
    "      use_normalized_coordinates=True,\n",
    "      line_thickness=8)\n",
    "  plt.figure(figsize=IMAGE_SIZE)\n",
    "  plt.imshow(image_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model CLI - run\n",
    "\n",
    "```\n",
    "saved_model_cli run [-h] --dir DIR --tag_set TAG_SET --signature_def\n",
    "                           SIGNATURE_DEF_KEY [--inputs INPUTS]\n",
    "                           [--input_exprs INPUT_EXPRS]\n",
    "                           [--input_examples INPUT_EXAMPLES] [--outdir OUTDIR]\n",
    "                           [--overwrite] [--tf_debug]\n",
    "                           ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_cli run [-h] --dir DIR --tag_set TAG_SET --signature_def\n",
    "                           SIGNATURE_DEF_KEY [--inputs INPUTS]\n",
    "                           [--input_exprs INPUT_EXPRS]\n",
    "                           [--input_examples INPUT_EXAMPLES] [--outdir OUTDIR]\n",
    "                           [--overwrite] [--tf_debug]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Examples \n",
    "Not an example of input - input = tf.train.Example  \n",
    "\n",
    "So, how to create Example from a tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serialized_example_tensor Tensor(\"ReaderReadV2_2:1\", shape=(), dtype=string)\n",
      "image_tensor (1, ?, ?, 3)\n"
     ]
    }
   ],
   "source": [
    "# This is needed since we cloned tensorflow/models under code.\n",
    "cwd = os.getcwd()\n",
    "models = os.path.join(cwd, 'code/models/research/')\n",
    "slim = os.path.join(cwd, 'code/models/research/slim')\n",
    "sys.path.append(models)\n",
    "sys.path.append(slim)\n",
    "\n",
    "from object_detection.inference import detection_inference\n",
    "\n",
    "# cfa_utils\n",
    "#  - some feature helper functions\n",
    "#  - a Feature - (common format) for object detection\n",
    "from code.cfa_utils.example_utils import feature_obj_detect\n",
    "from code.cfa_utils.example_utils import int64_feature, int64_list_feature, bytes_feature, bytes_list_feature, float_list_feature\n",
    "\n",
    "\n",
    "! ls {TEST_TFRECORDS_PATH}\n",
    "input_tfrecord_paths = [TEST_TFRECORDS_PATH]\n",
    "\n",
    "\n",
    "serialized_example_tensor, image_tensor = detection_inference.build_input( input_tfrecord_paths)\n",
    "\n",
    "print (\"serialized_example_tensor\", serialized_example_tensor)\n",
    "print (\"image_tensor\", image_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.train.Example: <class 'tensorflow.core.example.example_pb2.Example'>\n",
      "<class 'list'> <class 'tensorflow.core.example.example_pb2.Example'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type 'Feature' is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-3f2c908cd821>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# print (type(d), d)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# INPUT_EXAMPLES = 'serialized_example=[' + d + ']'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0md_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m    179\u001b[0m         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n\u001b[0;32m--> 180\u001b[0;31m                         o.__class__.__name__)\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type 'Feature' is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# feature dict was defined on the fly\n",
    "img_path = os.path.join(SAMPLE_IMAGE)\n",
    "with tf.io.gfile.GFile(img_path, 'rb') as fid:\n",
    "   encoded_jpg = fid.read()\n",
    "\n",
    "type(encoded_jpg)\n",
    "\n",
    "tf_example = tf.train.Example(features=tf.train.Features(feature={'image/encoded': bytes_feature(encoded_jpg)}))\n",
    "print (\"tf.train.Example:\", type(tf_example))\n",
    "# print (\"tf.train.Example Value:\", tf_example)\n",
    "ex_list = [tf_example]\n",
    "print (type(ex_list), type(ex_list[0]))\n",
    "# = {'image/encoded': bytes_feature(encoded_jpg)}\n",
    "d = {'image/encoded' : bytes_feature(encoded_jpg)}\n",
    "# print (type(d), d)\n",
    "# INPUT_EXAMPLES = 'serialized_example=[' + d + ']'\n",
    "d_str = json.dumps(d)\n",
    "print (d_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/bin/saved_model_cli\", line 11, in <module>\r\n",
      "    sys.exit(main())\r\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 909, in main\r\n",
      "    args.func(args)\r\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 639, in run\r\n",
      "    args.inputs, args.input_exprs, args.input_examples)\r\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 551, in load_inputs_from_input_arg_string\r\n",
      "    input_examples = preprocess_input_examples_arg_string(input_examples_str)\r\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 464, in preprocess_input_examples_arg_string\r\n",
      "    input_dict = preprocess_input_exprs_arg_string(input_examples_str)\r\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 440, in preprocess_input_exprs_arg_string\r\n",
      "    input_dict[input_key] = eval(expr)  # pylint: disable=eval-used\r\n",
      "  File \"<string>\", line 1, in <module>\r\n",
      "NameError: name 'ex_list' is not defined\r\n"
     ]
    }
   ],
   "source": [
    "! saved_model_cli run --dir {MODEL_PATH} --tag_set serve --signature_def tensorflow/serving/predict --input_examples serialized_example=ex_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "If you want the annotations - you'll find the annotations tarball in data/ directory\n",
    "use the Notebook browser to download it\n",
    "\n",
    "The fastest, easiest way to review (and correct / verify) is to use labelImg program which will merge the image and annotation\n",
    "\n",
    "The main conclusion here is our model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (detection_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(SAMPLE_IMAGE)\n",
    "image_np = load_image_into_numpy_array(image)\n",
    "# Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "print (image_np_expanded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image_np_expanded\n",
    "\n",
    "with detection_graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.get_default_graph().get_operations()\n",
    "      print (\"ALL model operations:\", type(ops), len(ops))\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      print (\"tensor names:\", type(all_tensor_names), len(all_tensor_names))\n",
    "      tensor_dict = {}\n",
    "      for key in [\n",
    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
    "          'detection_classes', 'detection_masks'\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_value = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "          print (tensor_name, tensor_value)\n",
    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "      if 'detection_masks' in tensor_dict:\n",
    "        print (\"*** detection mask in the tensor dict ***\")\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[1], image.shape[2])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "      print (\"image_tensor:\", image_tensor)\n",
    "        \n",
    "      # Run inference\n",
    "      print (\"tensor dict:\", tensor_dict)\n",
    "        \n",
    "      # input into the model must be JSON\n",
    "      output_dict = sess.run(tensor_dict,feed_dict={image_tensor: image})\n",
    "      # output_dict = predictor.predict(tensor_dict,feed_dict={image_tensor: image})  # doesn't like feed_dict\n",
    "      # output_dict = predictor.predict(tensor_dict, {image_tensor: image})\n",
    "      # output_dict = predictor.predict(tensor_dict, feed_dict={\"image_tensor\": image_tensor})\n",
    "\n",
    "      # ops includes:\n",
    "      # <tf.Operation 'image_tensor' type=Placeholder>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(image_np_expanded), image_np_expanded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predict(tf.constant(image_np_expanded))[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img = tf.keras.preprocessing.image.load_img(SAMPLE_IMAGE, target_size=[300, 300])\n",
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "print (type(x), x.shape)\n",
    "\n",
    "x = tf.keras.applications.mobilenet.preprocess_input(x[tf.newaxis,...])\n",
    "print (type(x), x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x):\n",
    "    example = tf.train.Example()\n",
    "    example.features.feature[\"x\"].float_list.value.extend([x])\n",
    "    return loaded_model.signatures[\"predict\"](examples=tf.constant([example.SerializeToString()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
