{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using SageMaker Endpoint\n",
    "\n",
    "## Model:  MobileNet (v1) SSD  300x300\n",
    "## Trained For:  CFA Product Images\n",
    "\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/io/encode_jpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, sys\n",
    "import json\n",
    "import base64\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is needed since we cloned tensorflow/models under code.\n",
    "# - if you don't know what this means\n",
    "#   Look at the notebook TrainModel_Step1_Local\n",
    "#      in this notebook, you basically set up the project with includes cloning \n",
    "#      and compiling the tensorflow/models repo\n",
    "#   we are using the utilities found in that repo\n",
    "\n",
    "cwd = os.getcwd()\n",
    "models = os.path.join(cwd, 'code/models/research/')\n",
    "slim = os.path.join(cwd, 'code/models/research/slim')\n",
    "sys.path.append(models)\n",
    "sys.path.append(slim)\n",
    "\n",
    "# visualization\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils.visualization_utils import STANDARD_COLORS\n",
    "from object_detection.utils.visualization_utils import draw_bounding_box_on_image\n",
    "from object_detection.utils.label_map_util import get_label_map_dict\n",
    "\n",
    "from code.cfa_utils.example_utils import feature_obj_detect\n",
    "from code.cfa_utils.example_utils import bytes_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = os.getcwd()\n",
    "IMAGE_DIR = os.path.join(PROJECT_DIR, \"data/jpeg_images\")\n",
    "\n",
    "# you may have to update your model version number\n",
    "MODEL_PATH = os.path.join(PROJECT_DIR, \"trained_model/export/Servo/1570217480\")\n",
    "LABEL_MAP = os.path.join(PROJECT_DIR, \"code/cfa_prod_label_map.pbtxt\")\n",
    "\n",
    "# you can get data using the TrainModel_Step1_Local notebook\n",
    "# when writing - write to code/tfrecords\n",
    "# when you pull from S3 and your processing, read from data/tfrecords\n",
    "TFRECORDS_PATH =  os.path.join(PROJECT_DIR, \"tmp\")\n",
    "                                    \n",
    "SAMPLE_IMAGE = \"/home/ec2-user/SageMaker/ssd-dag/data/jpeg_images/20190710_variety_1562781002.jpg\"\n",
    "\n",
    "# NAME - get this from the console\n",
    "ENDPOINT_NAME = \"ep-mobilenet-ssd\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Model\n",
    "\n",
    "Local Model was pulled from a successful SageMaker training job (S3 -> local) and extracted.   This verifies the training job:\n",
    "- created a saved_model.pb\n",
    "- in export/Servo/\n",
    "\n",
    "And, we can read the Signature Defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Loading saved_model.py from:\", MODEL_PATH)\n",
    "loaded_model = tf.saved_model.load(sess=tf.Session(), \n",
    "                                   tags=[tf.saved_model.tag_constants.SERVING], \n",
    "                                   export_dir=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model complies to serving framework and can read signature defs\n",
    "!saved_model_cli show --dir {MODEL_PATH} --tag_set serve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signatures for:\n",
    "# - serving_default\n",
    "# - tensorflow/serving/predict\n",
    "# appear to be the same\n",
    "!saved_model_cli show --dir {MODEL_PATH} --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Endpoint\n",
    "create the endpoint assuming it doesn't already exist.  \n",
    "\n",
    "you go to the SageMaker console\n",
    "- Endpoints:  Create\n",
    "- on Create & Configure\n",
    "  - name:   ep-mobilenet-ssd  (whatever you want but the global name is in this code - above)\n",
    "  - endpoint configuration:   use the epc-mobilenet-ssd (this specifies p2.xlarge)\n",
    "  \n",
    "This will take 5-10 minutes\n",
    "\n",
    "THERE ARE MORE NOTES in the TrainModel_Step3_TrainingJob.  Creating an endpoint configuration requires knowing the inference code image (Docker?) - and I haven't figured out how to get that from the training job.\n",
    "\n",
    "if it fails...\n",
    "- retrain a model just to make sure you have a good one\n",
    "- recreate the endpoint config - this seems to be the most important artifact\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "from sagemaker.predictor import json_serializer, json_deserializer\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "predictor=sagemaker.tensorflow.model.TensorFlowPredictor(ENDPOINT_NAME, sagemaker_session)\n",
    "\n",
    "print (type(predictor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Image Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eager execution IS turned on\n",
    "image_tensor = tf.io.read_file(SAMPLE_IMAGE)   # EagerTensor\n",
    "image_bytes = image_tensor.numpy()              # bytes\n",
    "print (type(image_bytes))\n",
    "\n",
    "# create a feature\n",
    "feature = {}\n",
    "feature['image/encoded'] = bytes_feature(image_bytes)\n",
    "print (type(feature))\n",
    "\n",
    "# create a tf.train.Features\n",
    "# - because you'll need this when you create tf.train.Example\n",
    "features = tf.train.Features(feature=feature)\n",
    "print (type(features))\n",
    "\n",
    "# now create the tf.train.Example\n",
    "ex = tf.train.Example(features=features)\n",
    "print (type(ex))\n",
    "\n",
    "# serialize the Example to a string\n",
    "ex_str = ex.SerializePartialToString()\n",
    "print (type(ex_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the request\n",
    "# specify the signature name\n",
    "# instances must be a list\n",
    "d = {'signature_name': 'serving_default', 'instances': [{'b64': base64.standard_b64encode(ex_str).decode('ascii')}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model\n",
    "model_response = predictor.predict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(model_response))\n",
    "print (\"model response keys:\", model_response.keys())\n",
    "\n",
    "# predictios is a list\n",
    "predictions = model_response['predictions']\n",
    "predictions_count = len(predictions)\n",
    "print (\"predictions returned\", predictions_count)\n",
    "print (\"predictions keys:\", predictions[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the model response\n",
    "\n",
    "#### The response structure was documented in the model query (cli) you ran above\n",
    "\n",
    "- you see raw_* responses\n",
    "- you see post processed responses - we are using only post processed response data;  note that this is controlled by the pipeline config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 12 object classes defined in code/cfa_prod_label_map.pbtxt\n",
    "print (\"number of predictions - per image:\", len(predictions[0]['raw_detection_scores']))\n",
    "print (\"number of classes per prediction:\", len(predictions[0]['raw_detection_scores'][0]))\n",
    "\n",
    "# raw detection scores\n",
    "# - this is probably related to anchor boxes\n",
    "#   so 1917 is based on the dimensions and the model defsl\n",
    "raw_scores = np.asarray(predictions[0]['raw_detection_scores'], dtype=np.float32)\n",
    "print (type(raw_scores), raw_scores.shape)\n",
    "\n",
    "# detection scores\n",
    "# in the config:  max_number_of_boxes: 100\n",
    "# - so, you have 100 top scores\n",
    "detect_scores = np.asarray(predictions[0]['detection_scores'], dtype=np.float32) \n",
    "print (\"detect scores:\", type(detect_scores), detect_scores.shape, detect_scores[:10])\n",
    "\n",
    "# detection classes\n",
    "detect_classes = np.asarray(predictions[0]['detection_classes'], dtype=np.int8)\n",
    "#np.reshape(detect_classes, (?,))\n",
    "print (\"detect class ids:\", type(detect_classes), detect_classes.shape, detect_classes[:10])\n",
    "\n",
    "# detection boxes\n",
    "# - note they are normalized\n",
    "detect_boxes = np.asarray(predictions[0]['detection_boxes'], dtype=np.float32) \n",
    "print (\"bounding boxes:\", type(detect_boxes), detect_boxes.shape, '\\n', detect_boxes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_class_names(label_map):\n",
    "    \"\"\"\n",
    "    get the class names - as a dict of byte arrays\n",
    "    input:  full absolute path o the label map protobuf\n",
    "    output: dict of byte arrays {class id:  b'name'}\n",
    "    \"\"\"\n",
    "    label_map = get_label_map_dict(label_map)\n",
    "    label_map_reverse = {}\n",
    "    for (k,v) in label_map.items():\n",
    "        label_map_reverse[v] = k\n",
    "    for (k,v) in label_map_reverse.items():\n",
    "        label_map_reverse[k] = str.encode(v)\n",
    "    return label_map_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_length(ds):\n",
    "    \"\"\"\n",
    "    get dataset size\n",
    "      brute force iteration to get record count\n",
    "    \"\"\"\n",
    "    num_elements = 0\n",
    "    for element in ds:\n",
    "        num_elements += 1\n",
    "    return num_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image - numpy jpeg\n",
    "# threshold - float 0.0 - 1.0\n",
    "# detect scores - numpy.shape = (n,)\n",
    "# detect classes - numpy.shape = (n,)\n",
    "# detect boxes - numpy.shape = (n, 4)\n",
    "def display_detection(image_tensor, class_names, threshold, prediction):\n",
    "    \"\"\"\n",
    "    utilility to display image & prediction\n",
    "    input:  image - EagerTensor\n",
    "            class names - dict {class id: b'name'}\n",
    "            threshold - 0.0 - 1.0  (keep if score > threshold)\n",
    "            prediction - model response for this instance\n",
    "    \"\"\"\n",
    "    # post processed prediction\n",
    "    # - without the threshold applied\n",
    "    detect_scores = np.asarray(prediction['detection_scores'], dtype=np.float32)\n",
    "    detect_classes = np.asarray(prediction['detection_classes'], dtype=np.int8)\n",
    "    detect_boxes = np.asarray(prediction['detection_boxes'], dtype=np.float32)\n",
    "    \n",
    "    # keep == w/ threshold applied\n",
    "    keep_idx = np.argwhere(detect_scores > threshold)\n",
    "    class_ids = detect_classes[keep_idx]\n",
    "    (obj_count, discard) = class_ids.shape\n",
    "    # get the class names you need to keep\n",
    "    obj_class_ids = class_ids.reshape(-1)\n",
    "    obj_class_names = []\n",
    "    for i in obj_class_ids:\n",
    "        obj_class_names.append(class_names[i])\n",
    "    obj_class_names = np.asarray(obj_class_names)\n",
    "    # get the bounding boxes you need to keep\n",
    "    bboxes = detect_boxes[keep_idx]\n",
    "    \n",
    "    # just keep data from here on\n",
    "    image_decoded_tensor = tf.image.decode_jpeg(image_tensor)\n",
    "    image = image_decoded_tensor.numpy()\n",
    "    pil_image = Image.fromarray(image)\n",
    "    bb = bboxes.reshape(-1,4)\n",
    "    ymins = bb[:,0]\n",
    "    xmins = bb[:,1]\n",
    "    ymaxs = bb[:,2]\n",
    "    xmaxs = bb[:,3]\n",
    "    \n",
    "    print (\"detected:\", obj_count, obj_class_names)\n",
    "    \n",
    "    for idx in range(obj_count):\n",
    "        draw_bounding_box_on_image(pil_image,ymins[idx],xmins[idx], ymaxs[idx], xmaxs[idx],\n",
    "                color=STANDARD_COLORS[obj_class_ids[idx]], \n",
    "                thickness=4, display_str_list=[obj_class_names[idx]],\n",
    "                use_normalized_coordinates=True)\n",
    "    display.display(pil_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the class names from the label map\n",
    "class_names = get_class_names(LABEL_MAP)\n",
    "\n",
    "display_detection(image_tensor, class_names, 0.5, predictions[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process from TFRecord file\n",
    "\n",
    "parse_single_example  \n",
    "https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/parsing_ops.py#L1025-L1072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tfrecord_file_list_input = [os.path.join(TFRECORDS_PATH, 'train/train.tfrecord'),\n",
    "                            os.path.join(TFRECORDS_PATH, 'val/val.tfrecord'),\n",
    "                            os.path.join(TFRECORDS_PATH, 'test/test.tfrecord')]\n",
    "print (tfrecord_file_list_input)\n",
    "raw_dataset = tf.data.TFRecordDataset(tfrecord_file_list_input)\n",
    "raw_dataset.shuffle(buffer_size = 5000)\n",
    "print (type(raw_dataset))\n",
    "# brute force utility to get record count\n",
    "# gonna take 10-20 sec\n",
    "dataset_length = get_dataset_length(raw_dataset)\n",
    "print (dataset_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "    # Parse the input using the standard dictionary\n",
    "    feature = tf.io.parse_single_example(example_proto, feature_obj_detect)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dataset = raw_dataset.map(_parse_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_nth = 100\n",
    "# get the class names from the label map\n",
    "class_names = get_class_names(LABEL_MAP)\n",
    "\n",
    "\n",
    "\n",
    "for n,i in enumerate(parsed_dataset.take(dataset_length)):\n",
    "    # timing gate\n",
    "    start_time = time.time()\n",
    "    # print (\"start time: {:.4f}\".format(start_time))\n",
    "    \n",
    "    image_bytes = i['image/encoded'].numpy()  # bytes\n",
    "    # create a feature\n",
    "    feature = {}\n",
    "    feature['image/encoded'] = bytes_feature(image_bytes)\n",
    "    # create a tf.train.Features\n",
    "    # - because you'll need this when you create tf.train.Example\n",
    "    features = tf.train.Features(feature=feature)\n",
    "    # now create the tf.train.Example\n",
    "    ex = tf.train.Example(features=features)\n",
    "    # serialize the Example to a string\n",
    "    ex_str = ex.SerializePartialToString()\n",
    "\n",
    "    d = {'signature_name': 'serving_default', 'instances': [{'b64': base64.standard_b64encode(ex_str).decode('ascii')}]}\n",
    "    \n",
    "    model_response = predictor.predict(d)\n",
    "    predictions = model_response['predictions']\n",
    "    if n % display_nth == 0:\n",
    "        display_detection(i['image/encoded'], class_names, 0.5, predictions[0])\n",
    "    \n",
    "    # show the time\n",
    "    finish_time = time.time()\n",
    "    minutes = (finish_time - start_time) / 60\n",
    "    print(n, \"time spent: {:.4f}\".format(finish_time - start_time), \" in minutes: {:.4f}\".format(minutes))\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
