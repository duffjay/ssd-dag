{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRT - DO NOT USE\n",
    "\n",
    "Start with an understanding of your model and the TensorFlow versions - which leads to the CUDA versions.   If on SageMaker - this will not be super flexible.   AWS may be lagging on TF version.\n",
    "\n",
    "IMPORTANT NOTE \n",
    "- use deb (dpkg) on Ubuntu\n",
    "- use rpm (yum) on AWS Linux\n",
    "\n",
    "I learned the hardway - if you use the *.run files, which looks easier - it may not register the packages correctly\n",
    "\n",
    "### READ THE DOCUMENTATION CAREFULLY\n",
    "https://docs.nvidia.com/deeplearning/sdk/tensorrt-install-guide/index.html\n",
    "TensorFlow 1.14 required  \n",
    "CUDA 10.0 or 10.2 required -- 10.1 NOT supported (and just guess what version we have!)\n",
    "So problem - how do you get SageMaker to use CUDA 10.0 an the 410 driver?  \n",
    "\n",
    "\n",
    "### TensorRT version 7.0 \n",
    "### CUDA 10.0 w/ driver 410 -OR- CUDA 10.1 & 418\n",
    "The point here is, you must have this consistent.   I think TensorFlow 1.14 will work fine with CUDA 10.0 or 10.1 and TensorFlow 1.15 will wants CUDA 10.1.     On SageMaker, you have less flexibility because AWS will typically be a TF version behind.   On your physical machine, you may be on the current version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will show you your CUDA version and driver version\n",
    "# note that nvcc is NOT installed\n",
    "\n",
    "# the symbol, cuda, is pointing to cuda-10.0\n",
    "! ls /usr/local -la\n",
    "\n",
    "# BUT, nvidia-smi says 10.1\n",
    "! nvidia-smi\n",
    "\n",
    "# I learned the hardway, that means CUDA 10.1 is installed - so install TensorRT for 10.1\n",
    "# you can ignore the symbol (cuda) for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify cuBLAS (part of the CUDA toolkit) is installed\n",
    "! cat /usr/local/cuda/include/cublas.h | grep CUBLAS\n",
    "! nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20200113 - on SageMaker p2\n",
    "Tensorflow 1.14.0  \n",
    "CUDA 10.1  \n",
    "NVIDIA Driver 418.87  \n",
    "\n",
    "So we \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "PROJECT_DIR = os.getcwd()\n",
    "BUCKET = 's3://cfaanalyticsresearch-sagemaker'\n",
    "\n",
    "SAGEMAKER_DIR = os.path.join(PROJECT_DIR, \"..\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the TensorRT software\n",
    "\n",
    "https://developer.nvidia.com/nvidia-tensorrt-7x-download\n",
    "\n",
    "928 MB This will not go well if you are on CFA network !!!!\n",
    "  - leverage a Workspace if possible (super fast)\n",
    "  - use a curl command (I didn't figure this out) - downloading directly to SageMaker should be fast\n",
    "  - use S3 (I put current packages on the cfaanalyticsresearch-sagemaker/software\n",
    "  - Don't download over CFA/COWS to your computer then upload!! it will fail\n",
    "For SageMaker:  get a RPM package (20200113) CentOS/RedHat 7 CUDA 10.0  \n",
    "For Ubuntu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker\n",
    "# - upload the RPM package to the SageMaker directory; \n",
    "\n",
    "# look in S3\n",
    "! aws s3 ls {BUCKET}/software/\n",
    "\n",
    "# download to SageMaker directory\n",
    "# - you only have to do this once\n",
    "# - but you'll need to install every time you reboot\n",
    "\n",
    "\n",
    "# TensorRT\n",
    "! aws s3 cp {BUCKET}/software/ {SAGEMAKER_DIR} --exclude \"*.*\" --include \"nv-tensorrt-repo-rhel7-cuda10.0-trt7*.rpm\" --recursive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Instructions\n",
    "https://docs.nvidia.com/deeplearning/sdk/tensorrt-archived/tensorrt-700/tensorrt-install-guide/index.html  \n",
    "    \n",
    "Read the instructions carefully  \n",
    "Pre-requesites - regardless of your Linux \n",
    "- CUDA obviously - 10.1 is the target version at this time (driven by your TensorFlow version)\n",
    "- cuBLAS - doesn't look like cuBLAS is on SageMaker (might be on your local machine)  cuBLAS is included in the CUDA Toolkit\n",
    "\n",
    "Check your environment - this is based on using TensorFlow 1.14 - which wants CUDA 10.1  \n",
    "\n",
    "cuBLAS\n",
    "driver - 410.48\n",
    "CUDA version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Installation\n",
    "Read 4.2 for RPM installation  \n",
    "Jupyter won't keep the variable names correctly so,  cut paste this into the terminal.  \n",
    "- open a Jupyter terminal\n",
    "- go to SageMaker directory\n",
    "\n",
    "c\n",
    "Pre-requisites\n",
    "Error: Package: libnvinfer7-7.0.0-1.cuda10.0.x86_64 (nv-tensorrt-cuda10.0-trt7.0.0.11-ga-20191216)\n",
    "           Requires: cuda-cublas-10-0\n",
    "Error: Package: libnvinfer-devel-7.0.0-1.cuda10.0.x86_64 (nv-tensorrt-cuda10.0-trt7.0.0.11-ga-20191216)\n",
    "           Requires: cuda-cublas-dev-10-0\n",
    "Error: Package: libnvinfer-devel-7.0.0-1.cuda10.0.x86_64 (nv-tensorrt-cuda10.0-trt7.0.0.11-ga-20191216)\n",
    "           Requires: cuda-cudart-dev-10-0\n",
    "Error: Package: libnvinfer7-7.0.0-1.cuda10.0.x86_64 (nv-tensorrt-cuda10.0-trt7.0.0.11-ga-20191216)\n",
    "           Requires: cuda-cudart-10-0\n",
    "           \n",
    "`wget http://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda_10.2.89_440.33.01_linux.run`\n",
    "`sudo sh cuda_10.2.89_440.33.01_linux.run`         \n",
    "\n",
    "\n",
    "CUDA Toolkit (to get cuBLAS-10.0)\n",
    "https://developer.nvidia.com/cuda-10.0-download-archive  \n",
    "I put 10.1 on S3 and you downloaded it above - so you just have to install.  \n",
    "`sudo rpm -i cuda-repo-rhel7-10-1-local-10.1.105-418.39-1.0-1.x86_64.rpm`  \n",
    "`sudo yum clean all`  \n",
    "`sudo yum install cuda`  \n",
    "\n",
    "Get the tag value from the current version (file) you downloaded \n",
    "\n",
    "`tag=\"cuda10.0-trt7.0.0.11-ga-20191216\"`  \n",
    "`sudo rpm -Uvh nv-tensorrt-repo-rhel7-${tag}-1-1.x86_64.rpm`  \n",
    "`sudo yum clean expire-cache`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ubuntu Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the Model\n",
    "### Step 1 - convert the checkpoint to TensorFlow Lite\n",
    "\n",
    "See TrainModel_Step3_TrainingJob.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert (Trained) Model Checkpoint to a tflite Model\n",
    "\n",
    "\n",
    "ALSO NOTE:  the script is named convert_checkpoint_to_edge_tflite.sh  \n",
    "Well... the name is no longer totally accurate\n",
    "- I took this from the original Coral TPU tutorial\n",
    "- Another step is required for compiling for the EdgeTPU (not really relevant here since we are confined to AWS where there is no TPU -- so we skip that stuff)\n",
    "- And, I added a step that converts the checkpoint to a TENSORFLOW frozen graph \n",
    "  - note that this generates a frozen_inference_graph.pb\n",
    "  - it ALSO generates a saved model graph.pb  \n",
    "  THESE (frozen graph & saved model) are NOT the SAME!!  \n",
    "  \n",
    "https://stackoverflow.com/questions/46547319/error-when-parsing-graph-def-from-string\n",
    "\n",
    "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/exporting_models.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls code/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAKE UP - make sure the checkpoint num == hyperparameters/num_train_steps\n",
    "# convert checkpoint is a task script - located in the tasks/ directory\n",
    "os.chdir(\"tasks\")  \n",
    "! ./convert_checkpoint_to_edgetpu_tflite.sh --checkpoint_num {NUM_TRAINING_STEPS} --pipeline_config {PIPELINE_CONFIG}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/lite/convert/python_api#using_the_interpreter_from_a_model_file_\n",
    "\n",
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('code/model')\n",
    "tflite_model = converter.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorRT)",
   "language": "python",
   "name": "tensorrt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
